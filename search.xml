<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title></title>
    <url>%2F2021%2F06%2F14%2FSIMCLR%E8%87%AA%E7%9B%91%E7%9D%A3%E7%AE%97%E6%B3%95%E8%AE%BA%E6%96%87%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[SIMCLR自监督算法论文读书笔记原文：https://link.zhihu.com/?target=https%3A//amitness.com/2020/03/illustrated-simclr/ GitHub:https://link.zhihu.com/?target=https%3A//github.com/google-research/simclr 引言过去的自监督算法存在诸多问题，本文截止2020.02是SOTA，具有思想简单，效果好的优点 实现方法方法的整体框架图 整体的思路是对图像做数据增强，让图像对通过两个分支，通过对比损失，拉开同源图片，推开不同源的图片，具体步骤如下 做数据增强，如一张图片增强出两个图片，他们其实是同一张图片的数据增强 引入一个可学的非线性变换函数，即上图的g(.)函数，上图中f(.)函数是特征抽取函数 使用对比交叉熵损失函数训练网络 加大batch size 损失函数 针对一对positive的图片(i, j)，sim函数是计算计算激活特征的余弦相似度，这个公式实际是一个softmax函数，分子是某张图片的特征$z_i$和他的增强图片特征$z_j$的余弦相似度，分母是这张图片与所有不包含自己的余弦相似度之和，可以看见，如果图i和图j的相似度越高，和其他图的相似度越低，这个函数的值越低，N是一个batch数量的图片 推理用于其他任务的时候拿h(i)作为特征，需要做finetunning，自监督学习的特征作为其他任务的特征提取层，其他任务可以是分类，检测，分割等等，就像在imagenet上pretrain出来的参数一样 参考写的比较通俗易懂的知乎：https://zhuanlan.zhihu.com/p/142951091]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2021%2F06%2F14%2F%E5%9B%BE%E7%89%87%E8%87%AA%E5%8A%A8%E8%A3%81%E5%89%AA%E8%B0%83%E7%A0%94%2F</url>
    <content type="text"><![CDATA[图片自动裁剪调研一，引言图片自动裁剪用于从一张图片中裁剪出最合适的子图，子图具有更合理的布局，更美观的效果等。图片自动裁剪在飞猪潜在的应用场景包括自动裁剪从外源爬取过来的图片等。与自动裁剪相关的任务名称还包括图片智能裁剪，image retarget 二，方法2.1 暴力搜索的方法暴力搜索法[3]是比较早的研究思路，它通过滑动窗口的方式获取一系列的候选裁剪框，然后从中选择美学分数最高的，滑动窗口，暴力搜索所有的子图，耗时大。 2.2 基于显著图预处理基于显著图的自动构图方法是最早期用于自动构图的方法，被称为 Attention-Based的方法。它基于一个假设，图像中最显著的区域是照片中最重要的部分，我们应该保留这个最重要的部分而裁剪其他部分。基于显著图的自动构图相关方法的差异主要在于如何获得最小外接矩形，典型的见文[5]。 这类方法的目标就是研究如何用最小的剪裁窗口使得注意力(图像显著特性)总和最大化[4]，注意力总和可以简单定义为图像所有像素值的和，它就是图中的有效信息。 [2]在检测出显著区域以后作为初始化的显著框，然后根据显著区域初始化框调整比例等得到一系列子图，用美学分给子图打分并得到得分最高的子图。 [9]是目前显著物体检测领域的SOTA， 2.3 基于强化学习的搜索法候选框的选择本质上是一个搜索问题，除了减小搜索空间，所以也可以使用更加高效的搜索方法，比如A2RL框架[1]，使用强化学习更高效地搜索裁剪框。 2.4 其他方法除此之外，还有一些别的方法，比如[6]提出了子图裁剪和子图美学评估网络，结合之后用于挑选出最佳子图。[8]使用弱监督和自监督的方式做图片的自动裁剪，论文中将裁剪的问题归为image retarget问题，像素通过映射，将图片映射为一个裁剪后的图像。 三，总结暴力搜索可以搜索全部子图，但是耗时太大几乎无法使用，基于显著图只能框出显著区域的外接区域，对于飞猪的应用场景，使用基于强化学习的方法，外加一些规则应该是最好的解决方案。 参考[1] Debang Li,[Huikai Wu, Junge Zhang, A2-RL: Aesthetics Aware Reinforcement Learning for Image Cropping, CVPR, 2017 [2] Wenguan Wang, Jianbing Shen, Deep Cropping via Attention Box Prediction and Aesthetics Assessment, ICCV, 2016 [3] Yi-Ling Chen, Jan Klopp, Min Sun, Learning to Compose with Professional Photographs on the Web [4] Ardizzone E, Bruno A, Mazzola G, et al. Saliency Based Image Cropping[C]. international conference on image analysis and processing, 2013 [5] Chen J, Bai G, Liang S, et al. Automatic Image Cropping: A Computational Complexity Study[C] Computer Vision and Pattern Recognition [6] Zijun Wei1, Jianming Zhang2, Xiaohui Shen2, Zhe Lin2, Radomır Mec, Good View Hunting: Learning Photo Composition from Dense View Pairs, CVPR, 2018 [8] Donghyeon Cho, Jinsun Park, Tae-Hyun Oh, Weakly- and Self-Supervised Learning for Content-Aware Deep Image Retargeting, ICCV, 2017 [9] Zhe Wu, Li Su, Qingming Huang, Cascaded Partial Decoder for Fast and Accurate Salient Object Detection, CVPR, 2019]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2021%2F06%2F14%2F%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%E7%9A%84%E6%95%B0%E6%8D%AE%E4%B8%8D%E5%9D%87%E8%A1%A1%2F</url>
    <content type="text"><![CDATA[回归问题数据不平衡一，背景机器学习中数据不平衡是一种常见问题，数据集不平衡分为分类(离散)数据的失衡和回归(连续)数据的失衡，目前绝大部分的解决方案都是针对分类失衡而提出的，回归问题的失衡指的是连续数据比例分布不均匀，如在图片美学数据集的场景下，AADB的数据统计分布为正太分布，两端得分分布较少，用不均衡的数据拟合训练得到的模型，会导致很多意想不到的问题。 二，方法数据不平衡问题的解决方法基本上可以分为基于数据层面的解决方案，基于算法层面的解决方案，以及一些其他比较小众的方案。 2.1 分类问题的数据不平衡2.1.1 基于数据的解决方案 下采样，对样本数量比较多的类别数据进行下采样[17]。 上采样，对样本数量比较少的类别数据进行上采样[16]。 生成新的数据样本，[2][3]对数量少的类别进行线性插值的操作，使数量少的类别的数据变多，缓解数据不平衡。 2.2.2 基于算法层面的解决方案 修改模型损失函数，如对样本数量比较少的类别赋予更重的损失权重，focal loss[9]同时调整正负样本权重和难易分类样本的权重来提升模型的性能。对于分类问题，很多算法努力扩大类之间的margin，各个类别设置的margin通常是一样的，[5]设计了一种能使各个类别使用不同margin的loss。[6][7][8]基于数据，设计不同的损失函数，提升失衡类别或整体类别的性能。 使用学习技巧，使用决策树，boost, bagging，transfer learning, meta learning, two stage learning，[10]通过调整UR(Under Representation,即失衡类别)类的特征分布，使UR类特征分布像常规类特征分布，从而提升整体模型性能，[1]中作者使用了元学习来解决类别失衡问题，除了抽取图片特征，作者使用了一个额外的memory模块来表示类内与类间距离，对于长尾类别，模型从memory模块获取有效信息来提升模型性能。[11]通过度量学些，将长尾数据映射到更高维空间，扩大类间距离，防止样本少的类别被样本多的类别所淹没，提升模型性能。[12]将特征表达和分类解耦，通过两阶段学习来提升长尾问题的性能。 自监督/半监督学习提升性能表现，[4]使用自监督与半监督学习提升模型在长尾数据的性能，作者通过理论分析了自监督与半监督可以提升长尾数据的性能，随后做了实验证明了在标准训练方法之前插入自监督和半监督的预训练，可以提升模型的性能。 2.2.3 改变模型评价 选择合适的评价方式[18]，如不使用准确率，如1）ROC曲线下面积，2）mean Average Precesion，指的是在不同召回下的最大精确度的平均值，3）假设共有n个点，假设其中k个点是少数样本时的Precision，这个评估方法在推荐系统中也常常会用。 调整阈值，大部分默认阈值为输出值的中位数，如逻辑回归输出0-1，正常阈值取0.5，解决办法之一是提高阈值。 转换问题定义，将少分类的问题定义为异常检测，使用无监督学习检测异常类别。 2.2 回归问题的数据不平衡回归问题的数据不平衡目前还研究的比较少，主要是直接将部分分类问题的数据不平衡解决办法迁移到回归问题。 2.2.1 基于数据的解决方案 下采样，对样本数量比较多的区间进行下采样，评判的标准比较模糊。 上采样，对样本数量比较少的区间进行上采样，评判的标准比较模糊。 生成新的数据样本，[14]对样本数量比较少的区间进行插值生成新的样本， 该方法的缺点是比较难生成高维的数据，如图片。 2.2.2 基于算法层面的解决方案 修改损失函数，对不同区间样本re-weight，修改特征等。[13]通过损失函数对不同的区间样本赋予不同的损失权重，这篇论文利用连续数据之间的相关性，对数据的统计值进行平滑，然后对数据的损失函数赋予统计函数的倒数权重。[13]调整高维数据特征，该方法是针对图片的方法，原理是基于一个假设，即数据的输入特征分布应该和输出特征分布类似，通过调整输出特征分布来提高模型的表现性能 使用学习技巧，boost, bagging，[15]采用集成学习的方式，多次采样，使用多个模型，最终取各个模型得分的平均值。 三， 总结目前大部分的研究都集中于分类问题的数据不均衡，回归问题的数据不均衡被研究的比较少，同时相应的解决方案也比较少。这样的原因一方面是分类问题比较常见，是各种其他任务如检测的基础模块，其被研究的时间更长，另一方面的原因是回归的长尾部分可以转化为其他问题，如异常检测等。 对于分类问题，通常我们应该根据不同的具体情况采取不同的解决放啊： 1、在正负样本都非常之少的情况下，应该采用数据合成的方式。 2、在负样本足够多，正样本非常之少且比例及其悬殊的情况下，应该考虑一分类方法。 3、在正负样本都足够多且比例不是特别悬殊的情况下，应该考虑采样或者加权的方法。 4、采样和加权在数学上是等价的，但实际应用中效果却有差别。尤其是采样了诸如Random Forest等分类方法，训练过程会对训练集进行随机采样。在这种情况下，如果计算资源允许过采样往往要比加权好一些。 5、另外，虽然过采样和欠采样都可以使数据集变得平衡，并且在数据足够多的情况下等价，但两者也是有区别的。实际应用中，我的经验是如果计算资源足够且小众类样本足够多的情况下使用过采样，否则使用欠采样，因为过采样会增加训练集的大小进而增加训练时间，同时小的训练集非常容易产生过拟合。 6、对于欠采样，如果计算资源相对较多且有良好的并行环境，应该选择Ensemble方法。 对于回归问题，大部分分类问题的解决办法都可以迁移到回归问题上，回归问题的数据不平衡由分类数据不均衡发展而来，单目前解决方案并不充分。 参考文献[1] Ziwei Liu, Zhongqi Miao, Xiaohang, Large-Scale Long-Tailed Recognition in an Open World, CVPR, 2019 [2] Chawla, N. V., Bowyer, K. W., Hall, L. O., and Kegelmeyer, W. P. Smote: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16:321–357, 2002 [3] He, H., Bai, Y., Garcia, E. A., and Li, S. Adasyn: Adaptive synthetic sampling approach for imbalanced learning. In IEEE international joint conference on neural networks, pp. 1322–1328, 2008 [4] Yuzhe Yang, Zhi Xu,Rethinking the Value of Labels for Improving Class-Imbalanced Learning, CVPR, 2020 [5] Kaidi Cao, Colin Wei, Adrien Gaidon, Learning Imbalanced Datasets with Label-Distribution-Aware Margin Loss, NeurIPS, 2019 [6] Yin Cui, Menglin Jia, Class-Balanced Loss Based on Effective Number of Samples, CVPR, 2019 [7] Dong, Q., Gong, S., and Zhu, X. Imbalanced deep learning by minority class incremental rectification. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2019 [8] Huang, C., Li, Y., Chen, C. L., and Tang, X. Deep imbal- anced learning for face recognition and attribute predic- tion. IEEE transactions on pattern analysis and machine intelligence, 2019 [9] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Focal Loss for Dense Object Detection, CVPR, 2017 [10] Xi Yin, Xiang Yu, Kihyuk Sohn, Feature Transfer Learning for Face Recognition with Under-Represented Data, CVPR, 2019 [11] Zhang, X., Fang, Z., Wen, Y., Li, Z., and Qiao, Y. Range loss for deep face recognition with long-tailed training data. In Proceedings of the IEEE International Conference on Computer Vision, 2017 [12] Kang, B., Xie, S., Rohrbach, M., Yan, Z., Gordo, A., Feng, J., and Kalantidis, Y. Decoupling representation and classifier for long-tailed recognition. ICLR, 2020 [13] Yuzhe Yang Kaiwen Zha Ying-Cong Chen, Delving into Deep Imbalanced Regression, arxiv, 2021 [14] Paula Branco, Luıs Torgo, Rita P. Ribeiro, SMOGN: a Pre-processing Approach for Imbalanced Regression, PMLR, 2017 [15] Branco, P., Torgo, L., and Ribeiro, R. P. Rebagg: Resampled bagging for imbalanced regression.PMLR, 2018 [16] Samira Pouyanfar, et al. Dynamic sampling in convolutional neural networks for imbalanced data classification [17] He, H. and Garcia, E. A. Learning from imbalanced data. TKDE, 2008. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks [18] Jason Brownlee, https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/, blog, 2019]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2021%2F06%2F14%2Fnms%E5%8F%8A%E5%85%B6%E5%8F%98%E7%A7%8D%2F</url>
    <content type="text"><![CDATA[nms及其各种变体summary 普通nms, ICCV, 2017 加权平均nms, (Weighted NMS), ICME Workshop, 2017 定位优先nms, ECCV, 2018 自适应阈值，CVPR, 2019 中心点距离，DIoU, AAAI, 2020 一，普通nms传统nms的逻辑是先对预测框分类别，针对具体某一个类别的框，先按照该框内是否有物体的置信度分排序，然后取得分最高的和他之后的框做iou，将iou大于阈值的框剔除。依次重复这个过程，直到所有框都被处理完毕，代码见nms.py，该方法的优点是简单容易理解且容易实现，缺点是效率低，刚性阈值，在稠密物体之间容易误杀真实的物体框。 二，加权平均nms普通nms基于假设置信度最高的框它的框的定位也是最准确的，但是其实真实的结果并不是这样的，所以这里作者提出了加权平均nms，找出iou比较高的那一堆框，置信度取值最大的那个，定位取这一堆框的平均值。具体的过程是如同普通nms，先按照不同类别对框排序，然后取置信度最高的那个框，并计算剩下框与之iou大于阈值的那些框的集合，最后将这些框的集合位置取平均值，得到这个框的位置。 三，定位优先nms作者在预测bounding box的时候，对网络多加了一个分支，用于表示该框定位的置信度，然后在做nms的时候，流程和普通的nms一样，但是选取的是位置的置信度，最后类别概率的置信度选取和最大位置置信度框有交叠的所有框中最大的那一个。 四， 自适应阈值前面所有的nms都是设置固定的剔除阈值，但是在物体密度比较高的时候其实应该降低剔除的阈值，否则会误伤交叠物体，所以这篇文章作者提出了自适应阈值设置，即在不同物体密度的地方设置不同的阈值。具体的步骤是在网络中添加一个预测物体密度的分支，通过得到的密度值设置不同的iou剔除阈值 五， 中心点距离前面所有的iou都是基于面积的交并比得出剔除阈值的，本问作者认为这并不是最好的办法，所以提出了新的衡量框是否是重叠的评估方法，通过两个框的中心点来衡量，设d为两个框的几何中心点的距离, c为两个框最远的角点点距离，然后用公式$$DIoU=IoU-\frac {d^2}{c^2} $$ 简版nms实现对所有框按分数从大到小排序，每一轮取分数最大的框，和剩下所有的框做iou，将大于阈值的去掉，去掉的办法是保留需要的框的下标，下一轮迭代从分数次高的开始，知道下标列表为空 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import numpy as npimport matplotlib.pyplot as pltboxes = np.array([[100, 100, 210, 210, 0.72], [250, 250, 420, 420, 0.8], [220, 220, 320, 330, 0.92], [100, 100, 210, 210, 0.72], [230, 240, 325, 330, 0.81], [220, 230, 315, 340, 0.9]])def py_cpu_nms(dets, thresh): x1 = dets[:, 0] y1 = dets[:, 1] x2 = dets[:, 2] y2 = dets[:, 3] areas = (y2 - y1 + 1) * (x2 - x1 + 1) scores = dets[:, 4] keep = [] index = scores.argsort()[::-1] while index.size &gt; 0: i = index[0] keep.append(i) # 计算交叠面积，取两个框左上角的最大值，两个框右下角的最小值，这两个点构成重叠矩形，矩形有可能不存在(两个框无重叠) x11 = np.maximum(x1[i], x1[index[1:]]) # 当前框和剩下所有的框取最大 y11 = np.maximum(y1[i], y1[index[1:]]) x22 = np.minimum(x2[i], x2[index[1:]]) y22 = np.minimum(y2[i], y2[index[1:]]) w = np.maximum(0, x22 - x11 + 1) # the with of overlap h = np.maximum(0, y22 - y11 + 1) # the height of overlap overlaps = w * h ious = overlaps / (areas[i] + areas[index[1:]] - overlaps) idx = np.where(ious &lt;= thresh)[0] # 根据iou &gt; thr取出没被过滤的框 index = index[idx + 1] # 过滤掉所有重叠框，保留剩下框的下标 return keepdef plot_bbox(dets, c=&apos;k&apos;): x1 = dets[:, 0] y1 = dets[:, 1] x2 = dets[:, 2] y2 = dets[:, 3] plt.plot([x1, x2], [y1, y1], c) plt.plot([x1, x1], [y1, y2], c) plt.plot([x1, x2], [y2, y2], c) plt.plot([x2, x2], [y1, y2], c) plt.title(&quot; nms&quot;)plt.figure(1)ax1 = plt.subplot(1, 2, 1)ax2 = plt.subplot(1, 2, 2)plt.sca(ax1)plot_bbox(boxes, &apos;k&apos;) # before nmskeep = py_cpu_nms(boxes, thresh=0.7)plt.sca(ax2)plot_bbox(boxes[keep], &apos;r&apos;) # after nmsprint(&quot;ttt&quot;)]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2021%2F06%2F14%2F%E7%BE%8E%E5%AD%A6%E7%BB%BC%E8%BF%B0ATA20201110%2F</url>
    <content type="text"><![CDATA[美学综述1 引言图片美学(Image Aesthetic Assessment)评价是一个重要的课题，评价图片的质量美学在很多方面有广泛的应用，如在视频网站视频封面筛选，图片检索网站图片排序，视频失真等等。图片美学的主要目标是要从感官上区分好看与不好看，转化为计算机可表达的量则为美学分，美学级别分类等等。 图片美学的研究任务可以分为美学图片分类，美学评分，美学分布，美学因素，美学描述；美学分类是对图片质量高低做分类，如质量高，质量中，质量低；美学评分为给图片打分，美感分布则给图片一个美学分的分布；美学因素是评价图片某个具体因素的得分，如图片色彩的评分；美感描述为用语言评价该图的美学情况。 图片美学主要由模糊，噪声，色彩，失焦，曝光，清晰度，EXIF(Exchangeable Information File)参数，以及属性和内容决定，属性如管线，色彩，主题突出三分法则等，内容则为是否自然景观，是否包含人物，动物，天空，拥挤与否。 美学评价可以分为主观评价和客观评价，目前该方向的研究主要集中于客观评价。实现方法可以三类：手工特征方法，通用图像特征方法和基于学习的特征三种方法，但是由于美学更偏主观评价，且内容更丰富，基于特征学习的方法效果具有绝对优势。 2 方法图片美学被广泛研究，前人的研究方法主要包括手工提取图片美学特征，使用图片通用特征，以及使用DNN提取图片美学特征的三大类方法。手工提取的特征主要包括色彩，构图，清晰度等通常意义的特征，通用特征指fisher vector,SIFT等特征。 2.1 hand-carfted feature based Naila Murray等人在[2]中提出了一个广泛引用且非常重要的美学数据集:AVA,随后它使用了基于SVM的方法针对AVA的众多标注进行美学分类实验；Luca Marchesotti在[6]中仔细分析了AVA数据集，通过手工提取图片特征，然后通过传统机器学习方法构造了3个应用：图片分类，回归美学分，图片检索；Jun-Tae Lee等人在[12]中通过规则检测图片几何特征(如是否对称，是否水平等)和内容(如是否有天空等)来判断图片是否优美；[1],[21],[22],[23], [24], [25]通过手工提取图片属性，内容等特征，通过规则或者接传统机器学习方法来判断美学-接SVM进行美学分类，接SVR美学回归。 [3]通过手工提取图片特征，然后将得到的特征输入SVM进行分类，这个应该是首次使用rank的思路来训练美学分，作者在文中做了很多数据均衡的工作；AADB应该借鉴的这篇文章的思路，该文结合了图片美学标签和图片内容标签，使用了rank的损失函数，训练出分类的网络，质量高，质量一般，质量低。 2.2 Generic image feature based使用通用的图片特征，如将fisher vector, SIFT等特征应用于图片美学评分[26][27][35]，以及将bag-of-visual-words[28]应用于图片美学评分，图片通用特征(如SIFT,Fisher Vector等)本来是用于图像识别的，但是这些特征也能帮助美学打分。 2.3 Learning feature based基于深度学习的众多方法已经应用于解决该问题，他们可以归纳为如下维度：从网络结构的设计角度，从不同美学任务角度，从不同网络训练方式角度，此外还有一些方法针对特定的条件，如针对有属性/语义的数据而提出有效的解决办法。 2.3.1 网络结构针对该问题，目前很多工作聚焦在修改网络上以获得更好的结果，他们可以分为4类，第一种是应用空间池化，这么做的好处是可以解决任意大小的输入，第二种是在网络中使用注意力机制，第三种是使用多输入，多子网络，第四种是使用孪生网络，通过排序解决美学分回归排序等问题。 2.3.1.1 spacial pool [9][30]神经网络一般采用固定尺寸输入，为了适应这种需求，输入图像需要通过裁剪，缩放或填充进行转换，这往往会损坏图像的构图，降低图像分辨率，或导致图像失真，从而损害原始图像的美感。[9]提出了一个composition-preserving的方法，它直接从原始输入图像中学习美学特征，而不需要任何图像转换。具体来说，该方法在常规的卷积层和池化层之上增加了一个自适应的空间池化层来直接处理原始大小和长宽比的输入图像，类似faster-rcnn中的roi-pooling。为了能够进行多尺度的特征提取，提出了Multi-Net Adaptive Spatial Pooling ConvNet架构，该架构由多个具有不同自适应空间池化大小的子网络组成，另外，还利用基于场景的聚合层来有效地结合多个子网络的预测结果。 [30]针对网络通常需要将输入图片处理后在输入，这通常会有信息损失，本文通过提出的的MLSP模块，将不同尺寸，不同感受野的特征结合在一起，从而适应高分辨率输入，解决不固定输入的问题，提升准确率的同时降低参数数量，计算耗时。具体来说为将中间特征层resize到固定的尺寸。 2.3.1.2 attention 机制 [20][18][33][20]针对 patch 的融合方法进行改进。之前的 patch 融合操作都是采用了 max、min 或者 sort 等操作进行融合，而这篇论文在融合多patch基础上加入了 空间注意力机制(Spatial Attention Meachine) 来提升算法性能。Shaolin Su等人在[18]提出了一个新的网络来感知图片的局部信息和内容，其中包含了注意力机制，来提升图片的质量分数，通过深浅层次网络感知整体与局部，通过额外模块学习图片的内容，最后将他们综合起来。 [33]在网络结构中也使用了空间注意力模块来提升性能。 2.3.1.3 Multi column &amp; Multi patch [5][7][9][20][11]为了能同时获得全局美学特征和局部美学特征，多路网络的方法应运而生。[7][9]等文章通过随机裁剪多个patch，以及resize全图一并输入到网络中，从而学习到局部与全局美学。[5]通过尝试多种输入，最后得出结论，随机裁剪，resize以及结合风格的多coloum的效果最好。 2.3.1.4 孪生网络[8][13][16]Kong Su等人在[8]中提出了一个双路输入的美学回归/分类的孪生网络结构，双路输出到一个对比损失函数，通过对比损失函数控制模型对图片打分，最后单路模型获取美学综合分，美学属性分。 Keunsoo Ko等人在[13]提出两两对比的方法来对测试数据集图片排序，能对数据集的所有数据排序，缺点为不能给出美学分，且计算量庞大。 Jun-Tae Lee等人在[16]中提出了通过孪生网络来解决三个问题：美学分类，美学综合分，个性化的美学分，亮点在于通过结构化矩阵统一网络输出和最终的任务输出。 2.3.2 美学任务按照美学研究任务，研究可以细分为美学质量级别分类，美学分数回归，美学分数分布，美学个性化，针对不同的研究，有相应的解决方法 2.3.2.1 美学质量级别分类/回归 [2][3][8][8][9][16][30]美学质量级别分类是研究得相对最为透彻也是最多的，[2][3]将美学质量定义高，低两类从而输出二分类结果。[8]等文章用连续的美学分设置阈值将分数转化为类别，也获得的当时最好的分类结果。 将图片美学定义为0-1之间的小数，从而把美学转化为回归问题，[8][9][16][30]都输出了图片美学分结果。 2.3.2.2 美学分布 [14][29]不同于美学质量级别分类/回归，美学分布尝试输出美学分布，这样更接近于ground-truth, 因为标注的时候通常是很多人标注同一张图片，美学分也是一个分布。 [14]通过神经网络的方法对图片质量进行评级，对神经网络最后一层修改为评级为10个等级的输出，针对一张图片，它的输出是1-10的十个质量分的概率分布，而不是质量高低的分类或者一个浮点型的回归的美学分。 训练的时候同多分类一样，损失函数使用交叉熵损失，但是针对所有类别都做$p_ilog{\hat p_i}$,针对一个样本，10分类的损失函数为$\sum_{i=1}^N -p_i log{\hat p_i}$。[29]通过jensen-shannon散度计算美学分布。 2.3.2.3 美学个性化 [10][16]审美是个主观问题，因而每个人都会有不同的看法。 [10]通过在通用数据基础上，加入个人偏好的美学数据集通过微调的方式得到个人倾向的美学分。[16]通过对比的方式用ground-truth来获得测试图片的得分，加入个人的偏好数据集调整综合分。 2.3.3 训练方式从网络训练方式上，可以分为全监督的学习方法，无监督的学习方法，以及基于对抗网络的方法。 2.3.3.1 全监督 [8][30][17]基于CNN的大部分实现方法都是基于全监督，如[8][30]。全监督是目前主流的计算机视觉任务的实现方法，模型通过数据驱动的方式，学习数据中的特征。 2.3.3.2 对抗网络 [31]生成网络是深度学习中一个新兴且重要的分支，最近也有人把生成网络的思想用于美学分相关任务。[31]设计了一个对抗网络，美学分回归网络作为生成网络(G)，区分网络(D)用来区分是label和ground-truth。 2.3.3.3 无监督 [19]美学由于其主观性，通常需要多人标注取平均，这更加加大了数据获取的难度，因而无监督学习应用前景可观。[19]通过无监督的方式学习美学分数，减少了数据标注的工作，不过该方法的效果仍然比不上有监督学习的效果，且该论文基于分类比较结果。 2.3.4 其他此外还有一些方法针对特定的条件，如针对有属性/语义的数据而提出有效的解决办法。 2.3.4.1 attribute/semantics-aware model [11][31][10][15][17][18][8]高层语义能用于提高美学评分，通常我们认为好的属性(如色彩鲜艳)和美学分高保持一致。 [11]提出了一种A-lamp CNN的架构的计算美学评估模型，其亮点在于同时兼顾了图片的细节评估和整体的评估，本文有两个亮点，其核心思想是更有效地最大化输入信息。通过专门挑选对图像美学影响较大的patch来实现这一目标，1 细节美学部分提出了自适应的多patch的选择模块，而非随机裁剪；2 另外通过属性图的构建整合了图像整体信息，综合整体与局部，得出美学分 [10]分析用户图片美学排序和图片属性/内容的关系，并提出个性化美学架构 训练如下： 先用提出的数据集训练一个通用美学分预测网络 用1训练出来的网络推理个人的小数据集，将推理结果和用户的自己标注的ground-truth做对比得到差值 训练一个专门预测2中差值的网络 训练好1和3中的网络以后，针对一张图片，先用1模型得到一个通用的得分，第三个模型预测出差值，然后相加得到个性化的美学分值，[17]探索了不同属性对美学分的影响。 3 美学数据集美学数据集按照标注维度可以分为质量级别分类，美学分，美学属性/内容三类，质量级别具体指按照美学质量高，中，低等，包含这类标注的数据集如3.1所述；美学分是给一张图片客观评价，包含美学分的数据集有如3.2；美学属性/内容除了在标注美学分之外，还标注了一些附加信息如色彩，清晰度等属性得分，内容是否包含人脸等，此外还有些数据集会包含额外的信息，如拍摄参数。 3.1 美学等级分类 The CUHK 数据来源：https://www.dpchallenge.com/ Ke et al 于2006年从上述官网收集了数120000张图片，该数据集的标签是二分类：质量高和质量低，2011年就有人将分类准确率提高到90%，现在应该更高；图片的质量分没有形成一个均匀分布的形态，而是大部分的图片集中在质量高或者质量低的图片上，中间分数的图片比较少；从图片上看，图片更接近于ImageNet, PascalVOC的风格，图片包含的内容主体少，和飞猪的数据集比，内容没有那么丰富，但是图片质量高 The CUHK-Photo Quality (CUHK-PQ) CUHK的升级， Luo et al 2011年提出，包含17690张图片，图片质量分为高质量分和低质量分，并被分成7个场景，动物，植物，静物，建筑，风景，人物，夜景， 3.2 美学分 Photo.net 官网：https://www.photo.net/ 2006年提出，该数据集包含3581张图片，每张图片包含两个分数，美学分和创意性，美学分取值范围为[0-10]，这两个变量高度相关；每张图片至少由两个人标注，然后取平均分；该数据集有很多图片是放在框里面的图，该数据集的数据大部分还是比较好看的，和我们自己的数据集比较起来，PN是专业的，而飞猪的数据是业余人员拍摄的图片 AVA 下载地址：http://research.google.com/ava/ 2012年，从www.dpchallenge .com上下载，包含255000张图片，从该网站963个摄影比赛主题捞取，数据集包含3种标注 Aesthetic annotations，标注人员组成：专业的图像工作者，摄影师，也包括了摄影爱好者；每张图的投票数量为78～549，平均为210个；投票分值为[0, 10]，值越高则图片质量越高；数据集中有很多非真实的摄影图，以及后期处理过的图，AVA中分数超过5分占多数。 Semantic annotations，数据集包含了66个类别语义标签，最常用的标签是自然，风景等。 Photographic style annotations，包含14个标签(互补色，双色调，高动态范围，Image_Grain，Light_On_White，长曝光，微距摄影，运动模糊，Negative_Image，三分法则，浅景深，Silhouettes，柔焦，消失点)，可以认为是图片美学的属性 具体的见AVA下载下来的README FLICKR-AES 2017, Jian Ren,Xiaohui Shen,Zhe Lin, 罗格斯特大学， Adobe实验室 包含40000张图片，每张图片评分从1-5，每个图片由5个AMT人员打分，然后求平均 AADB Shu Kong， Adobe 包括了10000张图片，training:val:test-8500:500:1000,属性分为11个维度，数据集更接近自然，普通用户拍摄的图片，具体如下图 标注包括3个维度 得分，[1,2,3,4,5]离散值，多人打分，训练的时候会在相同打分者，不同打分者等情况采样。 属性，[-1,0,1]得分，11个属性，每个属性都打分。 语义标签，语义标签是通过k-means聚类得到。 每个维度分别打分，张图片由5个人标注，总分由人主观给定，正向属性超过3个给5分，2个给4分，以此类崔，负向属性超过3个个1分。属性的分数默认0分，正向1分，负向0分，5个人加起来求平均，所以大部分值都是0，0.2，0.4之类的，因为他们都是被5除的结果，个别出现0.5的情况，应该是这张图片标注的人数是4，而不是5，属性分是根据论文的附录以及打分结果推断出来的。内容标签由k-menas得到 AROD 2018年，38万个样本，德国蒂宾根大学的Schwarz等，基于图片网站Flickr收集了38万幅图像 使用被喜欢量的对数与访问量对数的比值评估美学程度 3.3 其他 MIRFLICKR Muller et al 2010年提出，包含100万图片，数据集比较接近AVA，但是相比较缺乏美学分的标注，包含了图片文字描述，纹理, EXIF（Exchange Information File,焦距，曝光时间…）等 Reference Sagnik Dhar, Vicente Ordonez, and Tamara L. Berg. High level describable attributes for predicting aesthetics and interesting, CVPR, 2011 Naila Murray， Luca Marchesotti， Florent Perronnin，AVA: A Large-Scale Database for Aesthetic Visual Analysis, CVPR, 2012 Naila Murray， Luca Marchesotti，Florent Perronnin, Learning to rank images using semantic and aesthetic labels, CVPR, 2012 Le Kang1, Peng Ye1, Yi Li, Convolutional Neural Networks for No-Reference Image Quality Assessment, CVPR, 2014 Lu, Xin, et al. “RAPID: Rating Pictorial Aesthetics using Deep Learning.” acm multimedia (2014): 457-466. ACM, 2014 Luca Marchesotti · Naila Murray, Discovering beautiful attributes for aesthetic image analysis，IJCV, 2014 Lu, Xin, et al. “Deep Multi-patch Aggregation Network for Image Style, Aesthetics, and Quality Estimation.” international conference on computer vision (2015): 990-998, ICCV, 2015 Shu Kong, Xiaohui Shen,Photo Aesthetics Ranking Network with Attributes and Content Adaptation，ECCV, 2016 L. Mai, H. Jin, and F. Liu. “Composition-preserving deep photo aesthetics assessment. “ Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition(CVPR). 2016 Jian Ren, Xiaohui Shen, Zhe Lin, Radomir Mech, and David J. Foran. Personalized image aesthetics. In ICCV, 2017 Ma, S.; Liu, J.; and Chen, C. W. 2017. A-Lamp: Adaptive layout-aware multi-patch deep convolutional neural network for photo aesthetic assessment. In CVPR, 722–73, CVPR, 2017 Jun-Tae Lee，Han-Ul Kim, Photographic composition classification and dominant geometric element detection for outdoor scenes, Journal of Visual Communication and Image Representation, 2018 Keunsoo Ko, Jun-Tae Lee, Pairwise aesthetic comparison network for image aesthetic assessment,ICIP, 2018 Talebi, H., and Milanfar, P. 2018. NIMA: Neural image assessment. TIP 27(8):3998–4011，Google, 2018 杨文雅, 宋广乐, 崔超然, 基于语义感知的图像美学质量评估方法，计算机应用2018年第11期 Jun-Tae Lee， Chang-Su Kim，Image Aesthetic Assessment Based on Pairwise Comparison – A Unified Approach to Score Regression, Binary Classification, and Personalization, ICCV,2019 Yuming Fang, Hanwei Zhu, Yan Zeng, Kede Ma, Zhou Wang，Perceptual Quality Assessment of Smartphone Photography，CVPR, 2020 Shaolin Su, Qingsen Yan, Yu Zhu，Blindly Assess Image Quality in the Wild Guided by A Self-Adaptive Hyper Network，CVPR, 2020 Kekai Sheng1,2, Weiming Dong，Revisiting Image Aesthetic Assessment via Self-supervised Feature Learning，AAAI, 2020 Sheng, K.; Dong, W.; Ma, C.; Mei, X.; Huang, F.; and Hu, B.-G. 2018b. Attention-based multi-patch aggregation for image aesthetic assessment. In ACM MM, 879–886, 2018 Yiwen Luo and Xiaoou Tang. Photo and video quality evaluation: Focusing on the subject. In ECCV, 2008 Xiaoou Tang, Wei Luo, and Xiaogang Wang. Content- based photo quality assessment. IEEE Trans. Multimedia, 2013 Wei Luo, Xiaogang Wang, and Xiaoou Tang. Content-based photo quality assessment. In ICCV, 2011 Yiwen Luo and Xiaoou Tang. Photo and video quality eval- uation: Focusing on the subject. In ECCV, 2008 Xiaoou Tang, Wei Luo, and Xiaogang Wang. Content- based photo quality assessment. IEEE Trans. Multimedia, 15(8):1930–1943, Dec. 2013 Luca Marchesotti, Florent Perronnin, Diane Larlus, and Gabriela Csurka. Assessing the aesthetic quality of photographs using generic image descriptors. In ICCV, 2011 Florent Perronnin and Christopher Dance. Fisher kernels on visual vocabularies for image categorization. In CVPR, 2007 Hsiao-Hang Su, Tse-Wei Chen, Chieh-Chi Kao, Winston H. Hsu, and Shao-Yi Chien. Scenic photo quality assessment with bag of aesthetics-preserving features. In ACM Multime- dia, 2011 Jin, X.; Wu, L.; Li, X.; Chen, S.; Peng, S.; Chi, J.; Ge, S.; Song, C.; and Zhao, G. 2018. Predicting aesthetic score distribution through cumulative jensen-shannon divergence. In AAAI, 77–84, 2018 Hosu, V.; Goldlucke, B.; and Saupe, D. 2019. Effective aesthetics prediction with multi-level spatially pooled features. In CVPR,2019 Pan, B.; Wang, S.; and Jiang, Q. 2019. Image aesthetic assessment assisted by attributes through adversarial learning. In AAAI, 679–686，2019 Z. Wang and A. C. Bovik. Modern Image Quality Assessment. San Rafael, CA, USA: Morgan Claypool Publishers, 2006. Kuang-Yu Chang, Kung-Hung Lu, Aesthetic Critiques Generation for Photos, ICCV, 2017 Kaiming He, Xiangyu Zhang, Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition, CVPR, 2015 F.Perronnin,J.Sa ́nchez,andT.Mensink.Improvingthe fisher kernel for large-scale image classification. In Pro- ceedings of the 11th European Conference on Computer Vi- sion: Part IV, ECCV, 2010]]></content>
  </entry>
  <entry>
    <title><![CDATA[conda&virtual对比]]></title>
    <url>%2F2020%2F11%2F28%2Fconda%2F</url>
    <content type="text"><![CDATA[conda相关 创建环境 create -n pytorch3 python1234 - 删除环境 conda删除环境 ````conda remove -n name --all` 配置国内源,conda源 123conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/conda config --set show_channel_urls yes 或者直接修改~/.condarc 12345channels:- https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/- https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/- defaultsshow_channel_urls: true virtualenv相关 安装, 这里是用virtualenvwrapper的形式，virtualenvwrapper是对virtualenv的包装 12pip install virtualenv --userpip install virtualenvwrapper --user 查看 virtualenvwrapper.sh 路径 which virtualenvwrapper.sh source上一步得到的路径 source /usr/local/bin/virtualenvwrapper.sh 创建环境 aesthetic -python1234 - 删除环境 ````rmvirtualenv aesthetic` 激活虚拟环境 xxx```1234 - 退出虚拟环境 ```deactivate 相关问题 pip insall安装包的时候会找不到包，其实是有的，比如默认python2.7.7下安装tensorflow pip install tensorflow==1.14.0报错如 1Could not find a version that satisfies the requirement tensorflow (from versions: ) 这个问题的本质原因是python的版本，如果将python版本升级到python2.6.16就不会有问题解决办法之一是conda create -n python=2.7virtualenv貌似不能控制到2.7.x中x的版本号，conda略胜一筹]]></content>
  </entry>
  <entry>
    <title><![CDATA[行车记录仪]]></title>
    <url>%2F2020%2F11%2F01%2F%E8%A1%8C%E8%BD%A6%E8%AE%B0%E5%BD%95%E4%BB%AA-%E6%A0%91%E8%8E%93%E6%B4%BE%2F</url>
    <content type="text"><![CDATA[摄像头树莓派外接摄像头有两种一般法，一种是通过USB，这种比较常用，另一种是通过CIS，这种事树莓派官方自带的方法，我就是通过这种接口外接摄像头获取图片的 注意方向别插反了,摄像头排线金属的朝金属的一方，插好硬件后 raspi-config```12选5```interfacing options 接着选1 Camera选择enable,系统会提示重启，重启后就可以了 获取图片这部分采用最简单的raspistill工具获取 获取的shell命令为-o image.jpg -t 1000```12程序设置每秒取一张图片，代码如下 import osimport shutilimport timefrom datetime import datetime,timedelta if name == ‘main‘: TIME_DALTA = 1 while True: save_root = “/home/pi/Videos/save_images/“ time_now = datetime.now() N_days_ago = time_now + timedelta(days=-TIME_DALTA) year_month_day = str(time_now.year) + “-“ + str(time_now.month) + “-“ + str(time_now.day) year_month_day_N = str(N_days_ago.year) + “-“ + str(N_days_ago.month) + “-“ + str(N_days_ago.day) hour_min_sec = str(time_now.hour) + “-“ + str(time_now.minute) + “-“ + str(time_now.second) root_dir = os.path.join(save_root, year_month_day) root_dir_N = os.path.join(save_root, year_month_day_N) if os.path.isdir(root_dir_N): shutil.rmtree(root_dir_N,True) save_path = os.path.join(root_dir, hour_min_sec) + “.jpg” if not os.path.exists(root_dir): os.mkdir(root_dir) try: time_start = datetime.now() print(“++++++++++++++++++++++save path: “, save_path) print(“start time: “, time_start) cmd = “raspistill -t 1000 -o {}”.format(save_path) time_end = datetime.now() print(“end time: “, time_end) os.system(cmd) # time.sleep(1) except: print(&quot;get image failed&quot;) 123456### 开机自动运行因为没接口控制什么时候开始，什么时候结束，就设置为开机程序自动运行```sudo vim /etc/rc.local 添加启动命令 python save_video_from_rtsp.py > save_log.txt 2>&1 &```123456### 环境问题- pip安装包的时候报memoryerror,原因是树莓派3b的内存只有1G，解决办法是增加SWAP空间 ```cd /var 先关闭 swapofflink1234重新设置swap大小```sudo dd if=/dev/zero of=swap bs=1M count=1024 格式化 mkswaplink1234开启```sudo swapon /var/swap 通过free -m查看 参考 opencv+picamera 本打算用opencv+picamera来执行的，但是装不上opencv，装不上cv2所需要的numpy,报错ModuleNotFoundError: No module named &#39;numpy.core._multiarray_umath 一直没有解决,缺失libcblas时参考了：https://blog.csdn.net/kevindree/article/details/88772691 想参考未参考上的：http://www.voidcn.com/article/p-gcmfibth-bmp.html https://blog.csdn.net/qq_35987777/article/details/97919963 raspistill 参考了：https://www.cnblogs.com/jikexianfeng/p/7130843.html， 也可以raspistill –help 获取图片：https://www.cnblogs.com/uestc-mm/p/7587783.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[摄像头监控存储]]></title>
    <url>%2F2020%2F10%2F24%2F%E6%91%84%E5%83%8F%E5%A4%B4%E7%9B%91%E6%8E%A7%E5%AD%98%E5%82%A8%2F</url>
    <content type="text"><![CDATA[背景很多人都会买家用监控摄像头，比如小米摄像头。一般费用由两方面，一个是买摄像头的硬件成本，比如小米的也就100-200左右；另一方面是存储的服务费，如果你想看7天前的录像，或者看一个月前的录像，这个云服务是收费的，我刚好有一台台式机，我就把摄像头的视频取出来存储在本地 环境摄像头：水星 操作系统：ubuntu18.04 过程参考水星官方获取流地址的指导教程：https://security.tp-link.com.cn/service/detail_article_4432.html 代码 123456789101112131415161718192021222324252627282930313233import cv2import osimport shutilfrom datetime import datetime,timedeltaif __name__ == &apos;__main__&apos;: save_root = &quot;your path&quot; rtsp = &quot;rtsp://admin:123456@xxx.xxx.xxx.xxx:554/stream1&quot; vid = cv2.VideoCapture(rtsp) fps = vid.get(5) flag = 0 TIME_DALTA = 7 while True: time_now = datetime.now() N_days_ago = time_now + timedelta(days=-TIME_DALTA) year_month_day = str(time_now.year) + &quot;-&quot; + str(time_now.month) + &quot;-&quot; + str(time_now.day) year_month_day_N = str(N_days_ago.year) + &quot;-&quot; + str(N_days_ago.month) + &quot;-&quot; + str(N_days_ago.day) hour_min_sec = str(time_now.hour) + &quot;-&quot; + str(time_now.minute) + &quot;-&quot; + str(time_now.second) root_dir = os.path.join(save_root, year_month_day) root_dir_N = os.path.join(save_root, year_month_day_N) if os.path.isdir(root_dir_N): shutil.rmtree(root_dir_N,True) save_path = os.path.join(root_dir, hour_min_sec) + &quot;.jpg&quot; if not os.path.exists(root_dir): os.mkdir(root_dir) try: ret, frame = vid.read() if flag == fps: cv2.imwrite(save_path, frame) flag = 0 flag += 1 except: print(&quot;无法读取图像&quot;) https://github.com/qingzhouzhen/video_save/blob/master/save_video_from_rtsp.py 启动 1nohup python save_video_from_rtsp.py &gt; save_log.txt 2&gt;&amp;1 &amp;]]></content>
  </entry>
  <entry>
    <title><![CDATA[caffe-classify]]></title>
    <url>%2F2020%2F08%2F22%2Fcaffe-classify%2F</url>
    <content type="text"><![CDATA[环境caffe, Ubuntu16.04, python2.7 编译caffe这里需要注意一点，读取数据用python读取图片，不用官方推荐的lmdb数据格式，这样更容易定位问题 定义类名为1classifyInput 的python类文件：1classify-input.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118import caffeimport numpy as npimport yamlimport osimport cPickleimport cv2class classifyInput(caffe.Layer): def load_data_annotations(self, index): label, img_path = index.split(&apos; &apos;) return &#123;&apos;label&apos; : label, &apos;image&apos; : img_path, &apos;flipped&apos; : False&#125; def get_rand_idx(self): inds = np.arange(len(self._train_roidb)) inds = np.reshape(inds, (-1, 2)) row_perm = np.random.permutation(np.arange(inds.shape[0])) inds = np.reshape(inds[row_perm, :], (-1,)) self._perm = inds self._cur = 0 def setup(self, bottom, top): layer_params = yaml.load(self.param_str_) self._cfg_path = layer_params[&apos;cfg_path&apos;] cfg_dict = &#123;&#125; with open(self._cfg_path) as fp: for line in fp: data = line.strip().split(&apos;:&apos;) cfg_dict[data[0]] = data[1] self._batch_size = int(cfg_dict[&apos;batch_size&apos;]) self._num_classes = int(cfg_dict[&apos;num_classes&apos;]) self._train_path = cfg_dict[&apos;train_path&apos;] self._test_path = cfg_dict[&apos;test_path&apos;] self._size_w = int(cfg_dict[&apos;size_w&apos;]) self._size_h = int(cfg_dict[&apos;size_h&apos;]) self._flip = int(cfg_dict[&apos;flip&apos;]) self._mean_value = int(cfg_dict[&apos;mean_value&apos;]) self._train_image_index = [] with open(self._train_path) as fp: for line in fp: data_path = line.strip() self._train_image_index.append(data_path) self._test_image_index = [] with open(self._test_path) as fp: for line in fp: data_path = line.strip() self._test_image_index.append(data_path) self._test_roidb = [self.load_data_annotations(index) for index in self._test_image_index] self._train_roidb = [self.load_data_annotations(index) for index in self._train_image_index] if(self._flip == 1): num_images = len(self._train_roidb) for i in xrange(num_images): self._train_roidb.append(&#123;&apos;label&apos; : self._train_roidb[i][&apos;label&apos;], &apos;image&apos; : self._train_roidb[i][&apos;image&apos;], &apos;flipped&apos; : True&#125;) self._train_image_index = self._train_image_index * 2 self._name_to_top_map = &#123;&#125; self._name_to_top_map[&apos;data&apos;] = 0 self._name_to_top_map[&apos;label&apos;] = 1 top[0].reshape(1, 3, self._size_h, self._size_w) top[1].reshape(1,) self.get_rand_idx() self._test_cur = 0 def forward(self, bottom, top): if (self.phase == caffe.TRAIN): image_num = len(self._train_roidb) if(self._cur + self._batch_size &gt; image_num): self.get_rand_idx() db_inds = self._perm[self._cur:self._cur + self._batch_size] self._cur += self._batch_size minibatch_db = [self._train_roidb[i] for i in db_inds] elif (self.phase == caffe.TEST): test_image_num = len(self._test_roidb) if(self._test_cur + self._batch_size &gt; test_image_num): minibatch_db = self._test_roidb[self._test_cur:] + self._test_roidb[:self._batch_size - test_image_num + self._test_cur] self._test_cur = self._batch_size - test_image_num + self._test_cur else: minibatch_db = self._test_roidb[self._test_cur : self._test_cur + self._batch_size] self._test_cur += self._batch_size im_blob = np.zeros((len(minibatch_db), 3, self._size_h, self._size_w), dtype=np.float32) im_labels = np.zeros((len(minibatch_db),), dtype=np.float32) for i in xrange(len(minibatch_db)): im = cv2.imread(minibatch_db[i][&apos;image&apos;]) im = cv2.resize(im, (self._size_w, self._size_h), interpolation=cv2.INTER_LINEAR) if(minibatch_db[i][&apos;flipped&apos;]): im = im[:, ::-1, :] im = im.astype(np.float32, copy=False) if(self._mean_value == 1): pixel_means = np.array([[[103.52, 116.28, 123.675]]]) im -= pixel_means else: im = im / 255.0 im_blob[i, :] = im[:,:,::-1].transpose([2,0,1]) im_labels[i] = minibatch_db[i][&apos;label&apos;] blobs = &#123;&apos;data&apos;: im_blob, &apos;label&apos;: im_labels&#125; for blob_name, blob in blobs.iteritems(): top_ind = self._name_to_top_map[blob_name] # Reshape net&apos;s input blobs top[top_ind].reshape(*(blob.shape)) # Copy data into net&apos;s input blobs top[top_ind].data[...] = blob.astype(np.float32, copy=False) def backward(self, top, propagate_down, bottom): &quot;&quot;&quot;This layer does not propagate gradients.&quot;&quot;&quot; pass def reshape(self, bottom, top): &quot;&quot;&quot;Reshaping happens during the call to forward.&quot;&quot;&quot; pass 继承caffe.Layer，复写setup()方法和forward()方法，定义完后将文件放在1path/caffe-master/python 文件夹下 添加环境变量1export LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64:/usr/local/lib:/xxx/software/anaconda2/lib/ 制作数据将数据标注成pascal voc的格式，再使用如下脚本随机切取目标 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169import osimport cv2import numpy.random as nprimport numpy as npimport xml.dom.minidomdef IoU(box, boxes): box_area = box[2] * box[3] area = boxes[:, 3] * boxes[:, 4] xx1 = np.maximum(box[0]-box[2]/2, boxes[:, 1]-boxes[:, 3]/2) yy1 = np.maximum(box[1]-box[3]/2, boxes[:, 2]-boxes[:, 4]/2) xx2 = np.minimum(box[0]+box[2]/2, boxes[:, 1]+boxes[:, 3]/2) yy2 = np.minimum(box[1]+box[3]/2, boxes[:, 2]+boxes[:, 4]/2) w = np.maximum(0, xx2 - xx1) h = np.maximum(0, yy2 - yy1) inter = w * h ovr = inter / (box_area + area - inter) return ovrdef iou(box1, box2): s1x = box1[2] - box1[0] s1y = box1[3] - box1[1] s1 = s1x * s1y s2x = box2[2] - box2[0] s2y = box2[3] - box2[1] s2 = s2x * s2y x_left = max(box1[0], box2[0]) y_left = max(box1[1], box2[1]) x_right = min(box1[2], box2[2]) y_right = min(box1[3], box2[3]) if x_left &gt; x_right or y_left &gt; y_right: return 0 s_delta = (x_right - x_left) * (y_right - y_left) iou = s_delta / (s1 + s2 - s_delta) return ioufilepath = &quot;F:/data/classify/晚上数据/image/&quot;green_light_path_train = &quot;F:/data/VOCdevkit/light_9_3/train/green/&quot;red_light_path_train = &quot;F:/data/VOCdevkit/light_9_3/train/red/&quot;yellow_light_path_train = &quot;F:/data/VOCdevkit/light_9_3/train/yellow/&quot;black_light_path_train = &quot;F:/data/VOCdevkit/light_9_3/train/black/&quot;negative_path_train = &quot;F:/data/VOCdevkit/light_9_3/train/neg/&quot;green_light_path_valid = &quot;F:/data/VOCdevkit/light_9_3/valid/green/&quot;red_light_path_valid = &quot;F:/data/VOCdevkit/light_9_3/valid/red/&quot;yellow_light_path_valid = &quot;F:/data/VOCdevkit/light_9_3/valid/yellow/&quot;black_light_path_valid = &quot;F:/data/VOCdevkit/light_9_3/valid/black/&quot;negative_path_valid = &quot;F:/data/VOCdevkit/light_9_3/valid/neg/&quot;#label:4(open) label:5(close)min_w = 20min_h = 20n_idx = 0p_idx = 0img_num = 0for root,dirs,files in os.walk(filepath): valid_count = 0 for file in files: valid_count += 1 postfix = os.path.splitext(file)[1].lower() if (postfix == &quot;.jpg&quot;): moveFlag = False imgpath = os.path.join(root, file) annopath = imgpath.replace(&apos;image&apos;, &apos;label&apos;) annopath = annopath.replace(&apos;jpg&apos;, &apos;xml&apos;) door_list = [] airplane = [] dom_tree = xml.dom.minidom.parse(annopath) annotation = dom_tree.documentElement objects = annotation.getElementsByTagName(&quot;object&quot;) for object in objects: name = object.getElementsByTagName(&quot;name&quot;)[0] name_data = name.childNodes[0].data bndbox = object.getElementsByTagName(&quot;bndbox&quot;)[0] xmin = bndbox.getElementsByTagName(&quot;xmin&quot;)[0] xmin_data = xmin.childNodes[0].data ymin = bndbox.getElementsByTagName(&quot;ymin&quot;)[0] ymin_data = ymin.childNodes[0].data xmax = bndbox.getElementsByTagName(&quot;xmax&quot;)[0] xmax_data = xmax.childNodes[0].data ymax = bndbox.getElementsByTagName(&quot;ymax&quot;)[0] ymax_data = ymax.childNodes[0].data bbox = [xmin_data, ymin_data, xmax_data, ymax_data, name_data] door_list.append(bbox) img_num += 1 if (len(door_list) != 0): img = cv2.imdecode(np.fromfile(imgpath,dtype=np.uint8),-1) w, h = img.shape[1], img.shape[0] # gt_bboxes = np.array(door_list, dtype=np.float32).reshape(-1, 5) for i in range(len(door_list)): x_left = int(door_list[i][0]) y_top = int(door_list[i][1]) x_right = int(door_list[i][2]) y_bottom = int(door_list[i][3]) crop_w = x_right - x_left + 1 crop_h = y_bottom - y_top + 1 neg_num = 0 end_flag = False while (neg_num &lt; 3): neg_w = npr.randint(int(crop_w * 0.8), np.ceil(1.25 * crop_w)) neg_h = npr.randint(int(crop_h * 0.8), np.ceil(1.25 * crop_h)) neg_x = npr.randint(0, w) neg_y = npr.randint(0, h) neg_x = min(w - neg_w - 1, neg_x) neg_y = min(h - neg_h - 1, neg_y) crop_box = np.array([neg_x, neg_y, neg_x+neg_w, neg_y+neg_h]) label_box = list(map(float, door_list[i][0:-1])) Iou = iou(crop_box, label_box) if (np.max(Iou) &lt; 0.3): if valid_count%10 != 0: save_file = os.path.join(negative_path_train, &quot;%s.jpg&quot;%n_idx) else: save_file = os.path.join(negative_path_valid, &quot;%s.jpg&quot; % n_idx) cropped_im = img[neg_y : neg_y + neg_h, neg_x : neg_x + neg_w, :] cv2.imwrite(save_file, cropped_im) n_idx += 1 neg_num += 1 if (end_flag): break pos_num = 0 while (pos_num &lt; 3): pos_w = npr.randint(int(crop_w * 0.8), np.ceil(1.25 * crop_w)) pos_h = npr.randint(int(crop_h * 0.8), np.ceil(1.25 * crop_h)) delta_x = npr.randint(-crop_w * 0.2, crop_w * 0.2) delta_y = npr.randint(-crop_h * 0.2, crop_h * 0.2) pos_x = max(x_left + crop_w / 2 + delta_x - pos_w / 2, 0) pos_y = max(y_top + crop_h / 2 + delta_y - pos_h / 2, 0) if pos_x + pos_w &gt; w or pos_y + pos_h &gt; h: continue crop_box = np.array([pos_x, pos_y, pos_x+pos_w, pos_y+pos_h]) label_box = list(map(float, door_list[i][0:-1])) Iou = iou(crop_box, label_box) if (Iou &gt;= 0.65): cropped_im = img[int(pos_y) : int(pos_y+pos_h), int(pos_x) : int(pos_x+pos_w), :] if (door_list[i][-1] == &quot;red_light&quot;): if valid_count % 10 != 0: save_file = os.path.join(red_light_path_train, &quot;%s.jpg&quot; % p_idx) else: save_file = os.path.join(red_light_path_valid, &quot;%s.jpg&quot; % p_idx) elif (door_list[i][-1] == &quot;yellow_light&quot;): if valid_count % 10 != 0: save_file = os.path.join(yellow_light_path_train, &quot;%s.jpg&quot; % p_idx) else: save_file = os.path.join(yellow_light_path_valid, &quot;%s.jpg&quot; % p_idx) elif (door_list[i][-1] == &quot;green_light&quot;): if valid_count % 10 != 0: save_file = os.path.join(green_light_path_train, &quot;%s.jpg&quot; % p_idx) else: save_file = os.path.join(green_light_path_valid, &quot;%s.jpg&quot; % p_idx) elif (door_list[i][-1] == &quot;black_light&quot;): if valid_count % 10 != 0: save_file = os.path.join(black_light_path_train, &quot;%s.jpg&quot; % p_idx) else: save_file = os.path.join(black_light_path_valid, &quot;%s.jpg&quot; % p_idx) cv2.imwrite(save_file, cropped_im) p_idx += 1 pos_num += 1 print (&quot;%s images done, pos: %s neg: %s&quot;%(img_num, p_idx, n_idx)) 每个目标在扩大20%的范围内随机裁剪3个正样本，三个负样本 cfg文件定义配置文件，描述数据路径，训练的batch，类别数量，图片resize高宽 12345678batch_size:256num_classes:5train_path:/home/xxx/data/light_9_3/light_9_3_train.txttest_path:/home/xxx/data/light_9_3/light_9_3_valid.txtsize_w:96size_h:96flip:0mean_value:0 solver文件12345678910111213141516171819202122232425net: &quot;light-classify-train.prototxt&quot;test_iter: 13test_interval: 20#test_initialization: falsedisplay: 20average_loss: 20lr_policy: &quot;multifixed&quot;#stepsize: 1000#gamma: 0.1#base_lr: 0.0001stagelr: 0.0001stagelr: 0.001stagelr: 0.0001stagelr: 0.00001stageiter: 500stageiter: 1500stageiter: 3000stageiter: 4500max_iter: 5000iter_size: 2momentum: 0.9weight_decay: 0.0005snapshot: 1000snapshot_prefix: &quot;../output/light_9_3/door-classifyxx&quot;solver_mode: GPU 模型文件自定义的数据读取层如下： 123456789101112131415161718192021222324252627282930name: &quot;YOLOV3-TINY&quot;layer &#123; name: &apos;input-data&apos; type: &apos;Python&apos; top: &apos;data&apos; top: &apos;label&apos; include &#123; phase:TRAIN &#125; python_param &#123; module: &apos;classify-input&apos; layer: &apos;classifyInput&apos; param_str: &quot;&apos;cfg_path&apos;: ./cfg.txt&quot; &#125;&#125;layer &#123; name: &apos;input-data&apos; type: &apos;Python&apos; top: &apos;data&apos; top: &apos;label&apos; include &#123; phase:TEST &#125; python_param &#123; module: &apos;classify-input&apos; layer: &apos;classifyInput&apos; param_str: &quot;&apos;cfg_path&apos;: ./cfg.txt&quot; &#125;&#125; 除了最后一层loss，剩下的和推理的一样 训练12cd path/light_classify/cfgsh train-door-classify.sh 测试python测试： 12cd path/light_classifypython test-classify-light.py TensorRT使用docker，运行TensorRT-caffe项目]]></content>
  </entry>
  <entry>
    <title><![CDATA[Faster-rcnn]]></title>
    <url>%2F2020%2F08%2F22%2Ffaster-rcnn%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[原理 conver layer原图resize到800x600， Conv layer出来(50x38x256)的输出 RPN 从conv layer出来后每一个像素点配9个anchor；上面的分之经过1x1卷积形成50x38x18的张量，channel维度上每两个点代表一个anchor，分表表示前景与背景； 下一个分支经过1x1卷积形成50x38x36的张量，channel维度上每四个点代表一个anchor，用来回归位置的准确性，输出的anchor是(x,y,w,h), GT是($x_a,y_a,w_a,h_a$)，通过线性映射使得两者比较接近，需要训练如下参数 $$t_x=(x-x_a)/w_a, ty=(y-y_a)/h_a, t_w=log(w/w_a), th=log(h/h_a)$$, 每一个anchor的位置都有这样的一个线性回归的参数需要学习 Proposal选择softmax出来分数比较高的，处理位置回归，得到比较靠谱的框 Roi Pooling 从rpn选出来一些框，该框对应到共享卷积输出，将对应的位置分成若干等分(文中是7x7)，每一个等分中取max，得到一个channel切片的输出，将所有切片堆叠起来输入到下一个卷积输入 faster-rcnn bound-box回归 结合前面rpn推荐出来的框，再次预测推荐框和GT的偏移量 classification 将共享卷积中对应rpn输出的7x7的特征输入fc得到softmax后的特征进行分类，分真实目标类别(voc20类) anchor从conv layer出来后每一个像素点配9个anchor，三个尺度(0.5, 1, 2)，三个大小(128, 256, 512)，rpn阶段和faste-rcnn阶段预测的值作用于这些anchor得到真正的完整的预测值，具体的是在rpn阶段通过缩放尺度和平移将anchor移动到和GT差不多的位置，faste-rcnn阶段再做一次 训练rpn网络 $p_i$是rpn阶段为每个anchor预测的概率值，$p_i^$是该anchor是GT的概率，计算的方法是如果该anchor和GT交并比大于0.7则是1，小于0.3则是0，介于两者之间的anchor不参与计算loss；$t_i$是rpn阶段从anchor到GT的尺度变换的预测值，$t_$是从anchor到GT的标准值。分类loss使用交叉熵损失函数，bbox回归使用smooth_l1 loss。 训练fast-rcnn roi_pooling后接fc，一路通过softmax做类别分类，另一路对anchor回归，将anchor平移缩放到GT的位置]]></content>
  </entry>
  <entry>
    <title><![CDATA[仿射变换]]></title>
    <url>%2F2020%2F08%2F22%2F%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[getAffineTransform(src, dst) 该函数接受两个3x2的矩阵，表示源图片的三个点和对应目标图片的三个点 用$X = \begin{bmatrix} x_0 \ y_0\ \end{bmatrix} $表示原始图片的一个点，$T = \begin{bmatrix} x \ y\ \end{bmatrix}$表示转换后的图片上对应的点，$M= \begin{bmatrix} a_{00} &amp;&amp; a_{01} &amp;&amp; b_{00} \ a_{10} &amp;&amp; a_{11} &amp;&amp; b_{10}\ \end{bmatrix}$表示转换矩阵，转换后的点由下式计算得到： $$x=a_{00}x_0+a_{01}y_0+b_{00}, y=a_{10}x_0+a_{11}y_0+b_{10}$$ —–公式1 代入三个点，6个公式，6个未知数，解6元一次方程可以得到M cv2.warpAffine(image, trans_input, (inp_width, inp_height), flags=cv2.INTER_LINEAR), 该方法需要输入目标图像的大小 由1得到M，针对原始图像上每一个像素点，都是用公式1映射到目标图像上，目标图像连续区域采用插值的方式解决小数问题，边缘外部的用黑色填充]]></content>
  </entry>
  <entry>
    <title><![CDATA[读书笔记3.11]]></title>
    <url>%2F2020%2F08%2F22%2F%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B03.4%2F</url>
    <content type="text"><![CDATA[##从0到1 彼得·蒂尔 ###内容简介 #####第0章 前言 从0到1是什么–从无创造出有，不是从竞争的零和中获利 #####第1章 未来的挑战 解释从0到1，水平进步与垂直进步，从0到1是垂直进步 #####第2章 像1999年那样狂欢 描述了1999年互联网泡沫前后的情景以及总结出了经验教训 第3章 所有成功的企业都是不同的成功的企业都是解决了问题，失败的企业都是无法逃避竞争，描述了垄断与非垄断的相关问题 第4章 竞争意识不要去追求竞争，随大流的去竞争，要放弃跟风的竞争，专注于自己 第5章 后发优势还是在讲垄断与创新 第6章 成功不是中彩票成功前需要准备，愿景，规划等等 第7章 向钱看齐幂次法则 第8章 秘密讲述商业机会，怎么发现商业机会与如何对待商业机会 第9章 基础决定命运创业公司的基础，合伙人，股权，董事会等等，不要先天残疾 第10章 打造帮派文化员工招聘与文化培养的一些问题 第11章 顾客不会自动上门写了产品销售吗，营销人员等地问题 第12章 人类和机器论文人类和机器的关系，机器就是机器，不能替代人 第13章 绿色能源与特斯拉论述公司发展壮大几个必要而非充分的条件，以及举特斯拉的例子 第14章 创始人悖论讲述创始人对于公司的重大意义 个人感悟最近看的一堆烂书中比较好的一本，没有创业过，也没有在硅谷呆过，纯粗BB 关于竞争，说中提到的是多数人会选择竞争比较激烈的地方吗，因为那意味着市场大，收获多，对于我来说我从来没有主动选择过，但是不能不说没有竞争，从初中起座位就是按考试排名来安排的，所以按照作者的思路，我和我初中高中同学(大学也差不多，我们大学和高中几乎没有区别)大部分人都是选择了最激烈的竞争领域，所以20几年来拿的出来的东西少之又少，如果学点其他东西，肯定会更有意义。我是比较赞同作者的这个观点的，不要随大流的去参与竞争，盲目的去竞争，那往往意味着激烈的竞争，而应该从自身的特点出发，选择别人不容易复制，能形成垄断壁垒的地方，不一定是自身的能力，比如钢琴十级，可以是自身坚定的信仰，甚至是自身一段念念不忘的痛苦的遭遇，竞争于垄断，一定要着眼于垄断。 关于paypal黑帮，必须有才华，并且由衷的热爱与现在同事共事，把同事当作一生都可以合作与联系的伙伴，而不是冷漠的为了工资勉强凑一起的人。最好是团队目标一致，齐心协力，把事业当作一身追求的东西，而身边的伙伴都是可以共同实现这个目的的人。 关于合适的机会与合适的人，书中举过很多例子吗，比如招聘时应聘的人会说喜欢公司文化，公司的氛围，名气，有才华的同事等等一堆，很不幸，这些好像我都有前科，但是最合适的公司提供独一无二的岗位，比如解决某个问题，与团队一起完成公司的使命，但问题是，在中国，同一个行业，哪怕是细分行业，能做到独一无二的，可能就只有造原子弹这种事情了。 从较小的市场切入，很多公司都是从很小的，在当时被人所看不起或不理解的场景做起来的，最后逐渐通过扩大领域占领其他市场，活着开创一片市场，按照这个思想，农村包围城市，星星之火可以燎原，只要抓住身边一小点的机会吗，能让它活下来，在慢慢的扩大，这个应该就是作者的思路了。 ##番茄工作法图解 书比较简单，主要包括一些时间管理学习效率的理论和实验结果，诸如脑补结构，生物节律，记忆原理的描述，还花了很多篇幅描述如何使用番茄时钟，比如如何处理外部中断，内部中断，团体番茄钟等，看完就忘记了，不过番茄钟这个思想还是比较有意思的，最核心的部分大概就是：开启一个番茄时钟，时间是25分钟，集中精力的工作25分钟，然后休息3-5分钟，期间会有杂七杂八的一大堆事情会发生，作者都做了详细的描述，这种我觉得得根据个人情况做调整，没必要按照它这个生搬硬套 ##活出生命的意义 维克多·佛兰尔克 ####作者背景介绍 作者维克多·佛兰尔克有纳粹集中营的经历，他的家人全部被毒气室毒死，只有他和妹妹幸存，他一生对生命充满了热情，67岁学习驾驶飞机，80岁登上阿尔卑斯山。他本人是个医学博士，著名心理医生，将自己的人生经历和理论结合起来开创了意义疗法，找到了自己生命的意义。 ####本书内容梗概 书分为两部分，前半部分讲述作者在集中营的生活，后半部分讲述意义疗法 前半部分讲述了作者如何被送进奥斯维辛，集中营中的组织结构，囚头对待犯人的态度与手段，犯人进去了以后的生理与心理的变化，集中营生活的劳动与饮食的艰辛，最后作者是怎么活着出来的。 后半部分结合作者的具体案例描述意义疗法，如果承受苦难是有意义的，那苦难就不在时苦难，描述了爱对于个人的意义，生命的本质等问题，阐述了找到人生的意义的方法：1通过创立某项工作或事业 2通过体验（爱）3通过忍受苦难，在与苦难对抗中产生意义 ####个人总结与感悟 全书看的时候是非常轻松的，流畅连贯，但是总感觉缺少理工科要的层次与逻辑，包括第一部分的经历描述，似乎既不是按照时间的逻辑，也不是按照具体人物场景的逻辑，导致看完没办法塞到一个框架里面去做总结；第二部分也是看的云里雾里，心理学，医学的概念完全没有什么概念，看完不知所云。 弗洛依德认为人追求快乐，阿德勒认为人追求优越，维克多认为人追求意义。每个人生来不一样，心理长期不平衡或者扭曲，必然影响身体健康。读完全书，作者的找到人生意义的方法给了我很大的启发同时我也非常认可这种观点。创立某项事业，人活着的意义很多时候是通过工作体现出来的，职业使命让人活的更充实而有目的；人一辈子的时间大多时候都是在解决爱与恨，工作两个问题，如果爱与恨能导致进步，那爱是在前面牵引带力量，恨是在后面驱赶的力量，如果可以隔断(比如血缘的割不断)，爱就爱的彻底，恨的恨的彻底，爱用爱来回报，仇恨用仇恨来回报，其他人这一辈子都不会有交集；如果忍受困难是有意义的，那苦难就是有意义的，作者忍受苦难是为了思念妻子家人，期待将来能重逢，没有办法改变的事情就选择忍受，因为忍受的目的是清楚的，为了爱，为了生活，为了将来大富大贵都行，为了达成某个目的而与命运对抗，达到自己能力范围内的胜利 柳井正全传李鑫 故事梗概家中独子(日本人的思维估计是长子要继承家业，小的，女的可以随便想干啥就干啥)，富二代，父亲与伯父起初都是经营西装，后来扩展到建筑等行业，从小就一直不服输，受严厉的管教，毕业后去工作不久就辞职经营家族生意，恋爱，找到了一生的归宿，后来自己创业，贩卖衣服，从西装转型到休闲服装，创立优衣库，靠低价以及自助等经营方式，以及出色的宣传手段，在日本迅速走红，之后在香港，美国相继开店，描述了中间的曲折与故事，扩张到其他行业比如食品，最终成了现在优衣库王朝，期间讲述了不少优衣库的理念与手段，比如质量控制，广告策略，用人策略，权利分配，公司应该维持稳定还是激进 个人感悟柳井正是松下幸之助，稻盛和夫之后新一代经营之神，但是我总感觉学这些东西用在科技行业是行不通的，目前我还没有到达’道’的精神层面，所以看的飞快。但凡牛逼的人不一定是家境怎么样，但是大多从小都有个性，有抱负，心中有一颗不甘平凡的愿望，活着有一个强烈的理由让其孜孜不倦的坚持，柳井正大概就是想超越他父亲，等到他真正超越他父亲，他在事业上已经已经进入指数增长的阶段，找到了通往高速路店路口 关于优衣库的高科技保暖内衣，我也是不明觉厉，所以亲自试了一下，然后并没什么*用，轻薄倒是事实，价格相对其他品牌的来说也确实是挺便宜的，确实是打价格战出身的 顺便吐槽下这本书，看看目录，懵懂的青春期有1，2，3，4，5，你怎么不叫人生1，2，3，4，5到结束呢，不能概括一下吗，非要用这么抽象的标题，我喜欢逻辑清晰的东西，一眼看过去就知道框架是什么，再读详细内容的时候去补充血与肉，这么个标题，太不符合我的口味了 区块链：定义未来金融与经济新格局火币网 张健 ####前言 之所读这本书，是特意找的，最近区块链太火，也想了解一下，虽然比特币早就出了，10年要是拿着手机台式机去搞两个现在就发了，当前比特币的单价大概8000多吗，火币网是中国最大的数字资产交易平台与数字资产金融服务商，据说这是具有颠覆效果的技术，看了一下，也是云里雾里 内容简介第0章讲区块链的背景，文字与货币的诞生，货币的运行机制，有中央信息维护中心或者第三方信用认证，再描述了互联网加速信息的传播，有一大段的内容应用了kk失控的关于密码朋克的论述，也就是说很早就有人想做去中心化的事情 第一章区块链是什么，描述了大概是个什么，发明人中本聪与他发明该技术的来由，将区块链解释为一本公共的账本，网络中所有的节点都存有一份改账本，一次写权限需要修改所有账本，去中心化的优点以及区块链的未来展望的描述 第二章描写区块链带来的机遇，描述区块链对数字货币诸如比特币，以太币的相互关系，区块链对金融将会产生什么样的影响，去中心化对金融场景是最诱人的，区块链对物联网与共享经济带来的新机遇，原先所有的公司都是一个第三方机构或者认证机构，比如在淘宝买东西，其实是我和卖家的关系，区块链可以把淘宝撇开，省去中间的这些麻烦，同理似乎银行也不是必要的在某些场景下，这样推广下去，央行好像也是可有可无的 第三章讲区块链的应用场景，包括已经比较成熟的数字货币比特币，众筹，合同合约，征信等等，按照目前很多公司在孵化的项目，太多东西能和区块链沾点边 第四章讲述区块链的原理，讲述了密码学，侧链等一些技术，也没怎么看太懂 个人总结区块链在18年太火了，突然之间觉得进入18年没有人再说人工智能等事，一夜之间17年被炒的火热的人工智能就没什么人提了，转而铺天盖地区块链，打开朋友圈公众号的都是区块链，这节奏比衣服的流行趋势还要换的快，挖矿造成的结果之一就是英伟达的gpu卖脱销，100强的It企业买不到gpu，要排队，价格还涨了很多，当然这个也不光是挖矿造成的，深度学习本身就是计算密集型的东西。 虽然书中总结了很多技术，也赋予了很多新的名词，但是计算机的本质就那么些东西，大概就是一个去中心化的账本，每个节点都有机会去写这个账本，获取写的机会的过程就叫挖矿，挖矿比的就是机器的计算能力，获取写的权利后再去更新所有的分布式的账本，最后拿到一个串数字，以后发红包就一串数字发来发去，这样发展下去是不是有可能造钱的机构会有危险比如美帝，大家都不用他印的美元了。能解决的问题包括一些电子货币比如比特币什么的，这种事没有使用价值就像纸币一样，另外作为技术，能够用于去中心化，很多的金融机构都会用这个来提升效率，或者但凡有网络，且有一个中枢，且效率比使用分布式的更低都有可能被区块链技术优化 跟钱钱学理财水湄物语 花了两个小时，理论读了一堆，实践没有，因为没钱，所以收获几乎为0，倒是被作者字里行间的秀恩爱的狗粮伤的扶不起，等我以后有了我的’暖手’我也去对这个世界一报还一报。 套路和穷爸爸富爸爸差不多，就是用钱生钱，让钱自动为你工作，养一只为你生蛋的鹅，我也对无人看管而自动增值的东西比较感兴趣，机器可以替代人，程序可以替代人，嗯，无人概念会替代掉很多人的工作，无人超市，无人驾驶以及一切]]></content>
  </entry>
  <entry>
    <title><![CDATA[人脸识别算法]]></title>
    <url>%2F2020%2F08%2F22%2F%E4%BA%BA%E8%84%B8%E8%AF%86%E5%88%AB%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[问题人脸识别算法包括两种类别，一种是给定一张图片，再给一个id，输出该图片是否是该id匹配的对；另一种是给一个数据库，再给一张图片，如果这个图片在数据库中有匹配，找出该匹配，如果没有输出不存在 siamese网络siamese通过神经网络对图片进行编码，编码函数描述为$f(x^i)$， 输出一个特征向量，比如128维的张量 有两种办法判断两幅图片是否是同一张图片 每张图片分别通过siamese网络，分别得到特征向量$f(x^1)$和$f(x^2)$, 计算两者之间的欧式距离$d=||f(x^1)-f(x^2)||^2$，如果欧氏距离超过$\tau$,则判断为他们不是同一张图片，如果欧式距离小于$\tau$, 则判断为同一张图片； 还有一种办法是一步完成，在网络里对两者的特征向量做逻辑回归，即做二分类，如果两幅图为相同人则为1，不相同则为0 triplet loss从数据集中选三个图片A(anchor), P(positive), N(negative), 其中anchor和positive是同一张图片，anchor和negative是不同的图片，计算anchor和positive的距离$d(A, P)$, negative和positive的距离$d(d(A, N))$, 我们的目的是使$d(A, P)$尽可能的小，$d(A, N)$尽可能大，用如下公式衡量loss： $L=max \lbrace( d(A, P) -d(A, N) + \alpha), 0 \rbrace$ 一次学习在人脸识别问题中，如果数据库中有人员新增或者离开，就必须重新训练数据，这就比较麻烦，必须学习一次，siamese就是一次学习，即one-shot问题]]></content>
  </entry>
  <entry>
    <title><![CDATA[calibrate]]></title>
    <url>%2F2020%2F08%2F22%2Fcalibrate%E4%B8%93%E9%A2%98%2F</url>
    <content type="text"><![CDATA[训练策略 输入832x640, 5500个迭代，lr=0.001, 1000,3000,4500迭代处各降低10倍学习率 输入416x416，4000个迭代，lr=0.001,1000,2500,3500迭代出各降低10倍学习率 输入416x416, 5500个迭代，lr=0.001, 1000,3000,4500迭代处各降低10倍学习率 与3同一个模型，区别在于做calib的数据不一样 输入832x640,5500个迭代，lr=0.001,1000,3000,4500迭代处各降低10倍学习率 输入832x640，6500个迭代，lr=0.001,1500,3000,5000个迭代处各降低10倍学习率 输入832x640，7500个迭代，lr=0.001,2000，4000，6000个迭代处降低10倍学习率 fp32与int8对比， IOU阈值0.4， 置信度阈值：0.25 序号 训练数据集 验证集 验证集描述 输入大小 calib fp32灯组数量 int8灯组数量 错误预测 准确率 没有检测到 recall 平均IOU 平均置信度下降 1 VOC2031 2031_trainval shenzhen 185张图片357个灯组 832x640 全部训练数据96_96 358 358 0 100% 0 1 0.806 2.31 2 VOC2025 2031_trainval shenzhen 185张图片357个灯组 416x416 全部训练数据64x64 355 350 0 100% 7 0.98 0.86 2.45 3 VOC2031 2031_trainval shenzhen 185张图片357个灯组 416x416 全部训练数据64x64 366 363 6 98.35% 11 97% 0.76 2.77 4 VOC2031 2031_trainval shenzhen 185张图片357个灯组 416x416 crop_fix jiali 366 357 7 98.03% 17 95.35% 76.54% 2.67 5 VOC2031 2031_trainval tianjin 203个图片 775个灯组 832x640 全部训练数据96_96 783 798 1 99.87% 5 99.36% 0.7713 5.99 6 VOC2031 2031_trainval tianjin 203个图片 775个灯组 832x640 全部训练数据 96_96 781 782 1 99.87% 1 99.87% 0.83 1.98 7 VOC2031 2031_trainval tianjin 203个图片 775个灯组 832x640 全部训练数据 96_96 784 780 1 99.87% 5 99.36% 0.85 0.81 8 VOC2031 2031_trainval tianjin 203个图片 775个灯组 832x640 全部训练数据 96_96 780 783 1 99.87% 0 1 0.86 fp32与ground truth对比，IOU阈值0.4， 置信度阈值：0.25 序号 训练数据集 验证集 验证集描述 输入大小 fp32灯组数量 错误预测 准确率 没有检测到 recall 平均IOU 1 VOC2031 2031_trainval shenzhen 185张图片357个灯组 832x640 358 3 99.16% 2 99.43% 0.8166 2 VOC2025 2031_trainval shenzhen 185张图片357个灯组 416X416 355 9 97.46% 12 96.64% 0.7142 3 VOC2031 2031_trainval shenzhen 185张图片357个灯组 416x416 366 18 95.08% 9 97.47% 0.726 4 – – – – – – – – 5 VOC2031 2031_trainval tianjin 203个图片 775个灯组 832x640 全部训练数据96_96 6 99.23 0 1 0.8394 6 VOC2031 2031_trainval tianjin 203个图片 775个灯组 832x640 全部训练数据96_96 6 99.23% 0 1 0.85 7 VOC2031 2031_trainval tianjin 203个图片 775个灯组 832x640 全部训练数据96_96 9 98.85% 0 1 0.852 8 VOC2031 2031_trainval tianjin 203个图片 775个灯组 832x640 全部训练数据96_96 5 99.35% 0 1 0.855 int8与ground truth对比，IOU阈值0.4， 置信度阈值：0.25 序号 训练数据集 验证集 验证集描述 输入大小 int8灯组数量 错误预测 准确率 没有检测到 recall 平均IOU 1 VOC2031 2031_trainval shenzhen 185张图片357个灯组 832x640 358 3 99.16% 2 99.43% 0.8166 2 VOC2025 2031_trainval shenzhen 185张图片357个灯组 416X416 350 7 98% 15 95.80% 0.7042 3 VOC2031 2031_trainval shenzhen 185张图片357个灯组 416x416 363 26 92.84% 22 93.83 0.64 4 VOC2031 2031_trainval shenzhen 185张图片357个灯组 416x416 357 18 94.5% 20 94.40% 65.12% 5 VOC2031 2031_trainval tianjin 203个图片 775个灯组 832x640 798 3 99.62% 1 99.87% 73.38% 6 VOC2031 2031_trainval tianjin 203个图片 775个灯组 832X640 782 2 99.70% 0 1 80.52 7 VOC2031 2031_trainval tianjin 203个图片 775个灯组 832X640 780 5 99.35% 0 1 0.813 8 VOC2031 2031_trainval tianjin 203个图片 775个灯组 832X640 783 6 99.23% 0 1 0.8082]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hexo-next添加搜索]]></title>
    <url>%2F2020%2F08%2F22%2FHexo-next%E6%B7%BB%E5%8A%A0%E6%90%9C%E7%B4%A2%2F</url>
    <content type="text"><![CDATA[简单实用hexo自带搜索功能 安装插件1npm install hexo-generator-searchdb --save 配置博客打开博客根目录的_config文件，添加 12345search: path: search.xml field: post format: html limit: 10000 配置主题打开当前主题下的_config文件，将Local search 的相关配置，设为 true 123local_search: enable: true # if auto, trigger search by changing in 重新部署 参考：https://yashuning.github.io/2018/06/29/hexo-Next-%E4%B8%BB%E9%A2%98%E6%B7%BB%E5%8A%A0%E6%90%9C%E7%B4%A2%E5%8A%9F%E8%83%BD/]]></content>
  </entry>
  <entry>
    <title><![CDATA[TensorRT打印网络推理时间]]></title>
    <url>%2F2020%2F08%2F22%2FTensorRT%E6%89%93%E5%8D%B0%E7%BD%91%E7%BB%9C%E6%8E%A8%E7%90%86%E6%97%B6%E9%97%B4%2F</url>
    <content type="text"><![CDATA[实现profiler类 123456789101112131415161718192021222324252627class Profiler : public nvinfer1::IProfiler&#123;public: void printLayerTimes(int itrationsTimes) &#123; float totalTime = 0; for (size_t i = 0; i &lt; mProfile.size(); i++) &#123; printf(&quot;%-40.40s %4.3fms\n&quot;, mProfile[i].first.c_str(), mProfile[i].second / itrationsTimes); totalTime += mProfile[i].second; &#125; printf(&quot;+++++++++++++iteration: %d&quot;, itrationsTimes); printf(&quot;Time over all layers: %4.3f\n&quot;, totalTime / itrationsTimes); &#125;private: typedef std::pair&lt;std::string, float&gt; Record; std::vector&lt;Record&gt; mProfile; virtual void reportLayerTime(const char* layerName, float ms) &#123; auto record = std::find_if(mProfile.begin(), mProfile.end(), [&amp;](const Record&amp; r)&#123; return r.first == layerName; &#125;); if (record == mProfile.end()) mProfile.push_back(std::make_pair(layerName, ms)); else record-&gt;second += ms; &#125;&#125;; 注册profiler类 实例化1中的类，并将其注册到context中 123IExecutionContext* context = engine-&gt;createExecutionContext();cuda_context_ = context;cuda_context_-&gt;setProfiler(&amp;mTrtProfiler); 调用打印时间 12p_Net-&gt;run(image);p_Net-&gt;printTime();]]></content>
  </entry>
  <entry>
    <title><![CDATA[kmeans]]></title>
    <url>%2F2020%2F08%2F22%2Fkmeans%2F</url>
    <content type="text"><![CDATA[玩一下kmeans，调戏以下国足，顺便预测一下18世界杯冠军，18-7-15 23:00世界杯 物以类聚，人以群分，选亚洲15支球队 123456789101112131415data = &#123;&quot;zhongguo&quot;:[50,50,50,40], &quot;riben&quot;:[28,9,29,12], &quot;hanguo&quot;:[17,15,27,26], &quot;yilang&quot;:[25,40,28,18], &quot;shate&quot;:[28,40,50,25], &quot;yilake&quot;:[50,50,40,40], &quot;kataer&quot;:[50,40,40,40], &quot;alianqiu&quot;:[50,40,50,40], &quot;wuzibiekesitan&quot;:[40,40,40,40], &quot;taiguo&quot;:[50,50,50,40], &quot;yuenan&quot;:[50,50,50,50], &quot;aman&quot;:[50,50,40,50], &quot;balin&quot;:[40,40,50,50], &quot;chaoxian&quot;:[40,32,50,50], &quot;yinni&quot;:[50,50,50,50]&#125; 依次选2006年，2010年，2014年，2018年世界杯的数据作为聚类样本，打进世界杯的得分用排名衡量，预选赛小组未出线的给50，预选赛十强的给40，澳大利亚没统计，18年的排名是估计的，虽然11点是冠亚军决赛，理论上其他队伍排名已经定了，但是我不会，这样算，得分越多的越low。 k选3，初始中心选中国，日本，沙特，先计算每一条数据到三个中心点的欧氏距离，并将其归为最近点那一类，处理完所有数据后，计算每个类的中心点，更新聚类中心，重新以上步骤，知道聚类中心不再变化，代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import numpy as npdef name2indexf(names): name2index = &#123;&#125; for index, name in enumerate(names): name2index[name] = index return name2indexdef cacul_eudist(vec1, vec2): assert len(vec1) == len(vec2) dist = np.linalg.norm(vec1 - vec2) return distif __name__ == &quot;__main__&quot;: data = &#123;&quot;zhongguo&quot;:[50,50,50,40], &quot;riben&quot;:[28,9,29,12], &quot;hanguo&quot;:[17,15,27,26], &quot;yilang&quot;:[25,40,28,18], &quot;shate&quot;:[28,40,50,25], &quot;yilake&quot;:[50,50,40,40], &quot;kataer&quot;:[50,40,40,40], &quot;alianqiu&quot;:[50,40,50,40], &quot;wuzibiekesitan&quot;:[40,40,40,40], &quot;taiguo&quot;:[50,50,50,40], &quot;yuenan&quot;:[50,50,50,50], &quot;aman&quot;:[50,50,40,50], &quot;balin&quot;:[40,40,50,50], &quot;chaoxian&quot;:[40,32,50,50], &quot;yinni&quot;:[50,50,50,50]&#125; data_array = np.zeros(shape=(len(data.keys()),4)) name2index = name2indexf(data.keys()) for name in data.keys(): index = name2index[name] data_array[index] = np.array(data[name]) k_center = np.array([data_array[0],data_array[1],data_array[4]]) k_center_with_near = &#123;&#125; # ------cacul center and it near------- epoch = 0 while True: for index, item in enumerate(k_center): k_center_with_near[index] = [] # ----------choice nearest for each data--------- for index in range(len(data.keys())): data_item = data_array[index] near = 0 dist_min = 100000 for i in range(len(k_center)): dist = cacul_eudist(data_item, k_center[i]) if dist_min &gt; dist: dist_min = dist near = i k_center_with_near[near].append(index) # ------recacul center--------------- end_tag = True for center_near_index in k_center_with_near.keys(): contry_index = k_center_with_near[center_near_index] center_near_data = [] for item in contry_index: center_near_data.append(data_array[item]) center_near_data = np.array(center_near_data) new_center = np.mean(center_near_data, axis=0) if not (k_center[center_near_index] == new_center).all(): end_tag = False k_center[center_near_index] = new_center print(&quot;epoch:&quot;,epoch, &quot;center:&quot;, k_center) epoch += 1 test = data.keys() if end_tag: for item in k_center_with_near.keys(): print(&quot;classv&#123;&#125; include&quot;.format(item), [list(data.keys())[n] for n in (k_center_with_near[item])]) break 结果： 123classv0 include [&apos;zhongguo&apos;, &apos;yilake&apos;, &apos;kataer&apos;, &apos;alianqiu&apos;, &apos;wuzibiekesitan&apos;, &apos;taiguo&apos;, &apos;yuenan&apos;, &apos;aman&apos;, &apos;balin&apos;, &apos;chaoxian&apos;, &apos;yinni&apos;]classv1 include [&apos;riben&apos;, &apos;hanguo&apos;]classv2 include [&apos;yilang&apos;, &apos;shate&apos;] 这样算，中国队在亚洲只能算3流球队 预测克罗地亚冠军，虽然实力比法国弱一些，但是不要低估对冠军渴望的心]]></content>
  </entry>
  <entry>
    <title><![CDATA[线性回归从0开始]]></title>
    <url>%2F2020%2F08%2F22%2F%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BB%8E0%E5%BC%80%E5%A7%8B%2F</url>
    <content type="text"><![CDATA[习题 为什么squared_loss函数中需要使用reshape? 因为y_hat是二维的，第二个维度的个数为1，而label值y是一维的，y_hat之所以是二维的是因为X是二维的，dot出来的值一定是二维的 如果样本个数不能被批量大小整除，data_iter函数有什么变化？ data_iter函数最后会一个batch返回的样本个数不是batch_size个，而是不足batch_size个，如1000个样本，batch_size=9,最后一个batch样本个数是9个]]></content>
  </entry>
  <entry>
    <title><![CDATA[读书笔记3.25]]></title>
    <url>%2F2020%2F08%2F22%2F%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B03.22%2F</url>
    <content type="text"><![CDATA[##人类简史-从动物到上帝 尤瓦尔·赫拉利 全书结构逻辑清晰，比较容易读懂与理解，也举了不少生动的例子，好书 ###第一部分 认知革命 第一章讲述人类起源，一开始和其他物种没什么区别，但是渐渐的人类会合作，会使用火，使得效率变高，食物更容易被消化，从而将其他动物与人类区分开；第二章知善恶树，人类开始会讲故事，把一堆人凑一起合作，故事可以多半是假的，但是只要人人都信就成，比如基督教聚集一伙人做礼拜，现在创始人给员工画大饼讲故事，让大家聚集在一起。第三章亚当和夏娃的一天，原始社会很富足的，饿了去找吃的，困了就睡，大概一周工作2天就行了，前提是那时候到处都是吃的，大家都相信神灵，不管有没有，反正大家相信就行，相信皇帝是上帝的儿子，大家要服从他。第四章毁灭地球的人类，书中通过翔实证据证明人类就是毁灭因素，导致大型哺乳动物灭绝。 第二部分农业革命农业革命使人类过的更富足，这是史上最大骗局，人类因为农业革命而变的更凄惨的多，以前采集度日的时候三天只要工作一天就行了，但是农业时代却要天天面朝黄土背朝，所以农业革命使人更悲剧，那这样算的话信息社会简直不如死掉去算了，因为几乎996，这个怎么也是农业时代翻一倍多工作量，所以这个是显然的。而且更为悲剧的是农业时代90%的是农民，但是历史书写的却是剥削农民的人。第六章盖起的金字塔，讲述讲故事的力量，大家都相信一个故事，相信神话，相信汉谟拉比法典；第七章记忆过载描述文字与数字系统对人类进步的作用(这个似乎和前面重合)；第八章历史从无正义，历史上出现的不公正现象，等级歧视，性别歧视，种姓歧视等等，详细分析了各种歧视的原因等等。 第三部分 人类的融合统一第九章 历史的方向，全球统一，方方面面的统一；第十章 金钱的味道，金钱让大家信仰一致，介绍了金钱货币发展的来由；第十一章帝国的愿景，帝国是打着诸如为了你好的名义为了私利而前进；第十二章，宗教的法则，宗教发展及解释相关，比如佛教教人放下欲望，训练心智 第四部分 科学革命第十四章发现自己的无知，人类进入近代，飞速的发展，数学飞速发展吗，军事科技的进步，原子弹的使用，人们使用科技摆脱贫穷等；第十五章科学与帝国的联姻，这一章讲欧洲掌握科技之后对全球的殖民以及造成的结果；第十六章资本主义教条，分析欧洲成功的原因，资本的作用，信贷，杠杆等创新，而中国皇帝还使用赋税获取资金，没人愿意赋税，但是人人都愿意投资，这就是欧洲成功的一个原因，另一原因就是欧洲分散的权利和庞大的野心，比如比利时国王用慈善的名义压迫刚果盆地；第十七章工业的巨轮，工业革命的原因和对近代的促成作用，造成翻天覆地的变化；第十八章一场永远的革命，现代社会的改革，我们再也不会贫穷，不会战争，也没有帝国了，进化在加速；十九二十章，展望人类未来，快乐是什么，从生物学角度快乐是短暂的，只有这样人类才会孜孜不倦的去繁衍后代，追求财富，从宗教角度，人应该放下欲望，平静的过下去，并且体会到快乐，没有对比就没有伤害，大部分情况下人会无意识的去对比，而同处身边的人大概都差不多，所以痛苦，哪怕你心态调整的好，但爸妈不会不去比，他们会用血脉的名义把你和兄弟姐妹之间对比，和亲戚朋友对比，从而得出一个结论。智人末日，语言人类将消失，更高级的人类会取代我们，我们正在临近寄点 非暴力沟通第一章 让爱融入生活，非暴力沟通的4要素，观察，感受，需要，请求，非暴力沟通的过程： 什么是我们的观察 我的感受如何 哪些需要导致那样的感受 为了改善生活，我的请求是什么 第二章 是什么蒙蔽了爱，什么东西使我们难以体会到心中的爱？道德评判，进行比较，回避责任，强人所难，站在道德制高点批评他人，人比人气死人，强迫他人干某些东西等等 第三章 区分观察和评论，观察使客观事实，评论是带有主观感情的，比如书中举的例子，xx的工作时间很长，这个就是评论，而xx一周工作70小时以上就是观察，非常客观，尽量客观，因为评论使他人倾向于认为我们在批评 第四章 体会和表达感受，建立感受词汇表，表达自己的感受，有时候示弱有助于解决冲突，把感受和想法区分开来 第五章 感受到根源，听到不中听话的四种选择，1责备自己，2指责他人，3体会自己的感受和需要，4体会他人的感受和需要，有心中的感受需要说出来，书中举的他自己的妈妈为了得到一个钱包而假装生病最后被护士当作礼物拿走了的难过，说出来，不能因为害怕出头鸟之类的想法而不作为不声张，中国教育的弊端，个人成长的三个经历，第一个阶段是感情的奴隶，总是事事考虑他人的感受，牺牲自己的感受为了迎合他人而牺牲自己，尤其是默默无闻的牺牲自己，第二阶段面目可憎，第一个阶段的另外一种极端，只考虑自己的感受，不顾别人的想法，第三阶段，乐于互助，帮助他人时出于爱，既表达自己又关心他人 第六章 请求帮助，请求帮助的时候要提出具体的，可操作的请求，不要模棱两可，明确谈话的目的，这个比较经常，谈着谈着跑题，请求反馈，我们说的和别人理解的经常时不一样的，要求别人重复一下请求的内容可以确保别人明白了，在集体讨论时提出请求，否则就是一群人嗑瓜子聊天，提出请求要明确可操作，不要模棱两可 第七章 用全身心倾听，体会他人的感受，并给予他人反馈，给予他人反馈的具体的操作方法是将他人的一想法准确的复述一遍，需要与安慰等分开区别 第八章 倾听的力量，永远不要在别人愤怒的时候说但是，而应该把他的观点重复一次，猜测一次用于倾听，书中描述了一个不听话的小男孩调皮，这位妈妈正要发飙，而小男孩的妹妹只是说‘’妈妈你好像很生气‘’，她就不想发飙了，倾听有助于谈话变得有意思，如果没人听，演讲的人也觉得没意思 第九章 爱自己，是人就有故事有缺点，但是椅子就没有，自责使人更像一张椅子而不是一个人；书中作者举了一个例子，为了客户，匆忙签字，最后忘了戴笔帽，把自己的外衣弄坏了，从而自责，如果一个人爱他人，他会这么拼命的去责备他人吗，出于爱他人，不会责备他人，那可以这么责备自己，这样推断出不爱自己，所以要爱自己，也要爱自己服务他人的渴望吗，这样既能爱自己也能爱他人 第十章 充分表达愤怒，书中描述了表达愤怒的四个步骤，我愣是没看懂，从举例看，我感觉就是：在不骂娘的情况下和对方啰嗦撕逼，直到对方服了为止，有的人特别适合说脏话表达愤怒，我和一个高中校友在买火车票的时候曾经破口大说脏话，我失去了一个校友，我知道我不适合说脏话表达愤怒 第十一章 运用强制力避免伤害，讨论了暴力惩罚的坏处，以及怎么样使用非暴力沟通的方法使用暴力，比如举例中设置一个空的教室，让那些不想学习的学生有个去处，但是我小时候中午的时候无论你想不想睡觉，都必须睡觉，而且必须以规定的睡的姿势，还有一个监督的学生，简直就是模拟奥斯维辛集中营的做法，也不知道我是怎么过来的，现在的小孩子实在是太幸福了。 第十三章 表达感激，非暴力沟通鼓励表达感激，在表达感激是说出(1)对我们有益的行为(2)我们的那些需求得到了满足(3)我们的需求得到满足后我们的心情是什么样的 最后书籍以一个故事结尾讲他一个善良的舅舅，永远对他人提供帮助，而他外祖母也是一个善良的人，救助过自称是主耶稣的人，大萧条期间救助过一个四口之家的裁缝，当时她自己还有9个孩子，善良的心地，开阔的胸襟都是难得的品质 通读全书，对作者最认同的一个观点，他应该把书买到中东去，让那里的人好好沟通一下 ##失控 凯文·凯利 这是一本比较难看懂，尤其是不能短时间像看故事一样看懂的书，看之前最好看一下网上的读书笔记，这样能更清晰的理解内容，是一部值得看的书，文中旁征博引，感慨作者竟然知道这么多知识，书中讨论了生物来源，人造与天生，像好奇宝宝一样一本正经的去看待自身与周边；2，3从蜂群思维讨论群体智慧，数量多引起的质变；4-6章生物学地阐述生命，草原与火的关系，构成生命系统的元素组装，生命的进化，最终构成我们现在看到的一个多样性的世界，还在不停的进化中；第7章控制的兴起，精确控制与进化的漫无目的的前进，让物体可以自己进化，脱离控制，赋予自发控制的力量，催生自动进化，书中描述了很多大型生态模拟，真实版的楚门的世界。接着讨论人工进化，作者从当时的角度阐述互联网，物联网的概念，人工进化，让机器像人一样思考的进化，人工智能的畅想，失控，人没有办法对这个世界进行控制，朝着不可预测的方向进化。 书涉及的内容非常多，详细列举了故事的来源出处，旁征博引，看完的时间比较短，套路和尤瓦尔赫拉里写的时间简史未来简史比较像，具体的比较详细的参考http://blog.sina.cn/dpool/blog/s/blog_53cebf1f01010yex.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[深入浅出强化学习原理入门]]></title>
    <url>%2F2020%2F08%2F22%2F%E6%B7%B1%E5%85%A5%E6%B5%85%E5%87%BA%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%8E%9F%E7%90%86%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[第2章 马尔可夫决策过程 马尔可夫性， 当前系统的下一个状态仅与当前状态有关，而与以往状态无关 马尔可夫过程，以一定的概率在有限状态下转换，吃饭睡觉打豆豆。。。 马尔可夫决策过程，马尔可夫决策过程(S,A,P,R,$\gamma$)，S是有限状态集合，A是有限动作集合，P为状态转移，R是回报函数，$\gamma$是折扣因子，马尔可夫决策过程与马尔可夫过程不同的是它的动作是带转移概率的 强化学习的目标是在给定的马尔可夫决策过程中寻找最优策略，这个策略指的是状态到动作的映射，在q-learning中，这个策略就是q表，第一维是状态，第二维是动作 策略，所谓策略是指状态到动作的映射，在q-learning中指的是从一个状态转移到下一个状态到概率 状态值函数V，智能体在状态s时采用策略$\pi$采取一系列连续动作得到的累积回报的期望，详细一点就是智能体处于状态s，采用策略pi后所有一系列动作得到的回报累积，这里之所以要加期望，是因为有些策略采取的动作是概率，比如书中的例子一个人在某个状态睡觉打游戏是有一定概率大，都有可能发生—挖坑，书中图2.4圆圈中的状态值函数是怎么计算出来的？ 状态行为值函数Q，智能体在状态s时采取一步动作后进入下一个状态得到的状态值函数 挖坑—什么是贝尔曼方程？###第3章 基于模型的动态规划方法 理解公式$v_{k+1}(s) = \sum_{a\in A}\pi(a|s)(R_s^a+\gamma\sum_{s^\in S}P_{ss}v_k(s`))$就行 强化学习中主要使用值函数进行策略评估和策略改善，即训练与推理。上面的公式就是值函数迭代的公式（高斯塞德尔迭代法），从图3.7理解，当前状态的值函数等于采取所有动作得到的回报并加上所到达状态的值函数的期望，具体计算方法见图3.7下面的那个计算例子。 利用值函数进行推理采用贪婪策略，即从当前状态出发，把所有可以采取的动作都做一遍，哪个回报大就采用哪个动作 第4章 基于蒙特卡罗的强化学习方法 蒙特卡罗积分，求某个函数在某个区间的定积分，将函数先乘以某个分布已知的函数$\pi(x)$,然后在这个已知的分布上采样，最后求和的均值。 原理，和动态规划的一样，有衡量在某个状态s采取一些列动作获得回报和的状态值函数V，有Q函数，不同的是动态规划的值函数是迭代出来的，蒙特卡洛方法的V函数是采样然后计算出来的 同策略，产生数据的策略和要评估改善的策略是同一个策略，即训练的时候在状态s采取的动作和在推理的时候在状态s采取的动作是否一致，dqn中就是异策略，训练的时候为了尽可能走过所有的状态，状态到动作映射会覆盖所有的状态，但是推理的时候只会选取最有的策略 第5章 基于时间差分的强化学习方法TD方法：$V(S_t)&lt;-V(S_t)+\alpha(R_{t+1}+\gamma V(S_{t+1})-V(S_t))$ 值函数等于上一步的值函数的一个比例加上当前动作后的reward和下一个状态的值函数 时序差分方法，蒙特卡罗方法，动态规划方法的区别与联系，动态规划方法用空间换时间，把每个动作后的Q表都记录下来，在求某个状态的值函数时，在采取了某个动作走到下一个状态时，下一个状态的Q值已经记录下来了，由上一次记录并存储下来，举例见书中图3.7，这个方法需要知道状态的转移概率，在每个状态，采取哪个动作的概率都是需要已知，这个现实中是 不好弄的；蒙特卡罗可以解决动态规划需要状态转移矩阵的问题，它从随机初始状态开始，通过多次实验，统计从状态s到最终状态得到的奖励，缺点是效率低，每次都需要等到实验做完，且需要多次实验；时间差分结合了两者优势，用Q表存储记录每次实验后的状态值函数，但是像蒙特卡洛方法一样不停的与周围环境交互得到新数据，不要先验概率 TD($\lambda$):TD方法使用了下一个状态的值函数，TD($\lambda$)使用多个 动态规划，蒙特卡洛，时间差分的对比蒙特卡洛：$Q(s,a)\leftarrow Q(s,a) + \alpha(G_t - Q(s,a))$ 在状态s处的状态行为值函数为状态行为值函数+随机试验到状态s时的累积回报（挖坑，Q不是不记忆的吗？） 时间差分：$Q(s,a)\leftarrow Q(s,a)+\alpha(r + \gamma Q(s,a)- Q(s,a))$ 时间差分用空间换时间，用一张Q表记录以前做过的试验，更新的时候通过走一步进入状态s`，并加上以前记录的状态s·共同得到状态s处的状态行为值函数， ###第6章 基于值函数逼近的强化学习方法 基于值函数逼近理论，在Q-learning，时间差分，蒙特卡洛等方法中，使用Q表记录在什么样的状态采取什么动作会得到什么回报值，基于值函数逼近的就是使用参数$\theta$表示值函数，输入状态和动作，得到回报的值函数 dqn， $Q(s,a)\leftarrow Q(s,a)+\alpha(r + \gamma Q(s,a)- Q(s,a))$ ​ $\theta_{t+1}=\theta + \alpha[r + \gamma max_{a}Q(s,a`;\theta)- Q(s,a;\theta)] \Delta (Q(s,a;\theta))$ 计算TD目标网络的参数$r + \gamma max_{a}Q(s,a`;\theta)$的参数为$\theta^-$，而计算值函数的网络参数为$\theta$，一条数据包括当前状态$s_1$，采取动作a，立即回报r，下一个动作$s_2$，取出数据训练时，$\theta$每一步都更新，$\theta^-$每隔一定步数才更新；计算的方法为用贪婪策略在某些状态选取动作，存储一下数据，然后采样通过上式子更新参数$\theta$，目标网络参数只计算，等到若干步以后，才将Q函数的$\theta$更新到目标函数的Q函数中 第7章 基于策略梯度的强化学习方法直接搜索策略 ###第8章 基于置信域策略优化的强化学习方法 TRPO Trust Region Policy Optimization基于置信域策略优化 $\eta(\tilde\pi)=\eta(\pi) + E_{s_0,a_0…\tilde\pi}[\sum_{t=0}^{\infty}\gamma^tA_{\pi}(s_t,a_t)]$，为了搜索好的策略，即关于的策略函数每前进一步，回报函数都会比以前的好，需要更好的$\theta$的步长，或者把回报函数更改为旧回报函数加上一项不小于0的新值，以保证递增，$A_{\pi}(s,a)=Q_{\pi}(s,a)-V_{\pi}(s)$ 信息论，$H(P,Q)=-\int P(x)logQ(x){\rm d}x$,交叉熵常用来作为机器学习的损失函数，真是样本分布是$P(x)$，模型概率分布是$Q(x)$，两者相等时最小 优化方法，最速下降法，就是朝着导数方向前进，应该就是梯度下降法；牛顿法，使用二阶导数性质的梯度下降法，$x_{k+1}=x_k + d_k$, $G_kd_k=g_k$，$g_k$是一阶导数，$G_k$是二阶导数，反正就是迭代更新参数x 第9章 基于确定性策略搜索的强化学习方法随机策略指在状态s确定时，智能体采取的动作不一定是一样的，但是确定性策略则不一样，在状态s一定是，它所采取的动作一定是一样的。 Actor-Critic- Algorithm，行动与评估策略，行动策略是随机的以保证可以探索环境，评估策略是确定性的 ddpg，$r_t+\gamma Q^w(s_{t+1},u_{\theta}(s_{t+1})-Q^w(s_t, a_t))$,行动策略网络是$u$，参数为$\theta$,评估网络$Q$，参数是$w$，训练的时候采用贪婪策略探索环境，使用确定性策略更新Q，评估的时候使用Q函数 DQN和DDPG的区别，DQN时离散的，DDPG时连续的；DQN只有一个网络，DDPG有两个网络； ###第10章 基于引导策略搜索的强化学习方法 无模型时，智能体通过向环境试错得到策略网络，gps算法先通过控制相从数据中获取好的数据，监督相从控制相产生的数据学习模型；之所以要这么干的原因是有些网路有成千上万个参数，用常见的环境试错，无法从环境中学习到好的策略 第11章 逆向强化学习通常强化学习的回报函数都是人为经验给出的，但是这个是很主观的，逆向强化学习就是为了解决如何学习强化学习回报而来的。 学徒学习方法，智能体从专家示例中学习回报函数 最大边际规划方法，先建模$D={(x_i,A_i,p_i,F_i,y_i,L_i)}$从左到右依次为状态空间，动作空间，状态转移概率，回报函数的特征向量，专家轨迹，策略损失函数，学习者要照一个特征到回报的现行映射]]></content>
  </entry>
  <entry>
    <title><![CDATA[linux ssr客户端安装]]></title>
    <url>%2F2020%2F08%2F22%2Flinux-ssr%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[安装ssr客户端git clonelink12```cd path/shadowsocksr-linux-client-CLI/ cp ssrlink12```sudo chmod 777 /usr/local/bin/ssr install```12```ssr config```等同于``` vi /usr/local/share/shadowsocksr/config.json 123456789101112131415161718192021222324&#123; &quot;server&quot;: &quot;69.194.9.xxx&quot;, &quot;server_ipv6&quot;: &quot;::&quot;, &quot;server_port&quot;: 16675, &quot;local_address&quot;: &quot;127.0.0.1&quot;, &quot;local_port&quot;: 1080, &quot;password&quot;: &quot;xxx&quot;, &quot;method&quot;: &quot;aes-256-cfb&quot;, &quot;protocol&quot;: &quot;origin&quot;, &quot;protocol_param&quot;: &quot;&quot;, &quot;obfs&quot;: &quot;plain&quot;, &quot;obfs_param&quot;: &quot;&quot;, &quot;speed_limit_per_con&quot;: 0, &quot;speed_limit_per_user&quot;: 0, &quot;additional_ports&quot; : &#123;&#125;, // only works under multi-user mode &quot;additional_ports_only&quot; : false, // only works under multi-user mode &quot;timeout&quot;: 120, &quot;udp_timeout&quot;: 60, &quot;dns_ipv6&quot;: false, &quot;connect_verbose_info&quot;: 0, &quot;redirect&quot;: &quot;&quot;, &quot;fast_open&quot;: false 配置看自己vps启动ssr服务的时候端配置 让命令行走代理 安装proxychains apt install proxychains```1234- 配置proxychains```sudo vim /etc/proxychains.conf 将最后一行改成127.0.0.1 1080```1234- 测试```proxychains wget https://www.google.com 让所有的命令都走代理 bash```123456#### 注意每次重启了机器需要重启ssr服务```ssr start 参考： https://www.jianshu.com/p/4763b23aafb9 https://mikoto10032.github.io/post/%E7%A8%8B%E5%BA%8F%E5%91%98%E9%82%A3%E4%BA%9B%E4%BA%8B/linux%E4%BD%BF%E7%94%A8ssr%E5%AE%A2%E6%88%B7%E7%AB%AF/]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2020%2F08%2F22%2Fsamba%2F</url>
    <content type="text"><![CDATA[linux安装samba ubuntu16.04安装samba,windows访问 安装samba sudo apt-get install samba 修改配置项 vimlink12在末尾添加如下，记得/home/hhq/ 路径下有share这个文件夹 [share] path = /home/hhq/share public = yes writable = yes valid users = hhq create mask = 0644 force create mode = 0644 directory mask = 0755 force directory mode = 0755 available = yes12343. 设置登陆密码```sudo touch /etc/samba/smbpasswd smbpasswd -a huawei```12344. 启动samba服务```sudo /etc/init.d/samba start /etc/init.d/samba stop ```12 ```sudo /etc/init.d/samba restart /etc/init.d/samba reload```1234565.在windows文件浏览器上浏览文件```\\\10.154.236.xxx\share``` 参考： ```https://www.cnblogs.com/gzdaijie/p/5194033.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[mxnet工具类]]></title>
    <url>%2F2020%2F08%2F22%2Fmxnet%E5%B7%A5%E5%85%B7%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[profiler官方http://mxnet.incubator.apache.org/architecture/release_note_0_9.html?highlight=profiler profiler可以用来分析mxnet的性能，比如计算中每一个算子前向所花的时间 主要步骤包括设置profiler: 1234profiler开始：* ```mx.profiler.profiler_set_state(&apos;run&apos;) profiler结束： 1234### 网络可视化```mx.viz.plot_network(net)```或者```mx.viz.plot_network(sym).view() 可视化之前记得在系统上安装graphviz, apt-get install graphviz```12- 使用gluon可视化网络 block = NASNetALarge(1000) mx.viz.plot_network(block(mx.sym.var(“x”))) 12- notebook保存图片 net_dot = mx.viz.plot_network(net, save_format=’png’) net_dot.save(‘test.dot’) 12### 显示网络的每一层参数 mx.viz.print_summary(net,{“data”:(32,3,192,192),})`]]></content>
  </entry>
  <entry>
    <title><![CDATA[opencv编译]]></title>
    <url>%2F2020%2F08%2F22%2Fopencv%E7%BC%96%E8%AF%91%2F</url>
    <content type="text"><![CDATA[opencv编译12345678910111213141516171819202122下载opencv:https://opencv.org/releases.htmlunzip opencv-3.4.0.zipcd opencv-3.4.0mkdir build &amp;&amp; cd buildcmake -D CMAKE_BUILD_TYPE=RELEASE \-D CMAKE_INSTALL_PREFIX=/usr/local \-D WITH_CUDA=on -D ENABLE_FAST_MATH=1 \-D CUDA_FAST_MATH=1 -D WITH_CUBLAS=1 \-D WITH_NVCUVID=on -D CUDA_GENERATION=Auto \-D WITH_OPENCL=off -D WITH_OPENCL_SVM=off \-D WITH_OPENCLAMDFFT=off \-D WITH_OPENCLAMDBLAS=off ..make -j 24sudo make install卸载: cd build sudo make uninstall make clean如果要重新编译，记得删除掉build文件夹，否则不生效 第一次成功编译执行的时候报错找不到libgtk2.0, sudo apt-get install libgtk2.0-dev 报错nvcc fatal : Unsupported gpu architecture &#39;compute_75&#39;,将CUDA_GENERATION=Auto去掉，删除build文件夹重新编译]]></content>
  </entry>
  <entry>
    <title><![CDATA[深度学习(花书)]]></title>
    <url>%2F2020%2F08%2F22%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%8A%B1%E4%B9%A6%2F</url>
    <content type="text"><![CDATA[第一章 前言本章节描述了深度学习的发展历史，应用前景，发展趋势，粗略的介绍机器学习如何有别于软件编写的方法-机器学习是机器自己可以获取知识，有别于软件中的硬编码，深度学习的发展历史，那些导致它发展迟缓对手技术，核方法与图模型等。 第二章 线性代数本章描述线性代数技术，和本科的代数，考研的代数没有太大的区别， 除了一些新的概念以往没有接触过的，比如 张量，高纬矩阵 线性子空间，这个以前可能听过，原始向量线性组合后所能到达的点的集合，$Ax = b$, b就是A向量的线性子空间 伪逆，解方程$Ax = B$,无解的情况下求的A的左逆使得$Ax$与B的欧几里德距离最小 ###第三章 概率与信息论 第四章 数值计算通过迭代更新解的估计值，而非解析过程推倒得到解，比如函数$y=f(x)$,可以通过数学算法准确地解出最小值，也可以通过梯度下降的方法近似得到最小值，前者是解析解，后者是数值计算 什么是多项分布？ 上溢，下益，接近0的数四舍五入为0为下益，接近无穷则为上益，数值稳定的例子：softmax 病态条件，条件数表征函数相对输入的微小变化而变化的快慢程度，$f(x)=A^{-1}x$的解通过求矩阵A的逆得到，如果矩阵A的特征值最大值与最小值的比例(条件数)特别大，对求解影响特别大 基于梯度的优化方法，优化指的是改变x的值使得函数$f(x)$取得最大或者最小，梯度下降是沿倒数反方向逐步改变输入 jacobian和Hessian矩阵，输入和输出都为向量，这时对变量求到得到的求导矩阵为jacobian矩阵；多维输入输出二阶导数矩阵为Hessian矩阵 约束优化， 变量的范围固定在某一些范围内，比如在$L^2$空间内要求$x^2$=1,转化为($sin\theta, cos\theta$) ##第五章 机器学习基础 5.1 学习算法三要素： 任务T, 性能度量P, 经验E 机器学习的任务定义为系统应该如何处理样本，常见的任务有分类($P(k|x)$)，回归，转录(语音识别)，机器翻译，异常检测，合成采样；性能度量，衡量算法好坏，比如图像识别错误率；经验，从样本中学习规律，比较重要的是那个概念 监督学习，提供样本x与答案y，让系统拟合样本至y，并提供泛化能力，如图片分类，检测 非监督学习，有数据，但是没有标签数据，如聚类 强化学习，数据来自于智能体和环境的交互反馈###5.2 容量，过拟合，欠拟合 过拟合，模型容量太小，不能降低训练集的误差，比如用一次函数拟合二次函数产生的样本 欠拟合，模型容量太大，记住了太多训练集的特征，泛化能力太差即测试集误差太大，比如高次多项式拟合二次函数产生的样本，点与点之间曲线会陡增，但是实际曲线比较平滑 容量，函数逼近样本点能力，比如一次函数只能逼近直线 奥卡姆剃刀原则，在同样能够解释已知观测现象中，应该选择最简单那个(挖坑，不懂) 没有免费午餐定律， 在所有可能的数据生成分布上平均之后，每一个分类算法在未观测过的点上都有相同的错误率，在机器学习算法中，没有哪个算法在各种情况下总是比别的算法更好 正则化(regularizer)，中文翻译规则化，加入正则项的目的是防止过拟合，书中举二次函数的例子，如果用90次多项式拟合二次函数，显然会过拟合，如果江9次多项式中的3-9次的参数衰减到很小几乎没有，那差不多就是正确的，所以加入正则项的目的是抵消那些高次，深度学习中经常加入$L_2$正则项，因为loss函数是二次的###5.3 超参数和验证集 k折交叉验证，把数据集分成k份，一次选取其中的一份作为测试集，其他的作为训练集，然后把针对该模型的k次测试误差求平均。k折交叉用于数据集较小的时候充分利用数据###5.4 估计，偏差和方差 点估计，模型参数估计值为$\theta$,真实值为$\tilde\theta$,数据集是$x_1,x_2,,,$点估计是$\tilde\theta$,它是对$\theta$的近似，它由数据点映射而来 偏差$bias(\tilde{\theta})$=$Ebias(\tilde{\theta})-\theta$ 一致性，$plim_{m\rightarrow\infty}\hat\theta_{m}=\theta$,该式表示当数据点m趋近无穷的时候，点估计会收敛到它的真实值###5.5 最大似然估计 估计拟合函数的好坏 对$\theta$最大似然估计定义:$$arg max \sum_{i=1}^{m}{log*p_{model}(x^{(i)};\theta)}$$ 对$\theta$条件似然估计定义:$$arg max \sum_{i=1}^{m}{log*P(y^{(i)}|x^{(i)};\theta)}=-mlog\sigma-\frac{m}{2}log(2\pi)-\sum_{i=1}^{m}\frac{|||\hat{y}^i-y^i||^2}{2\sigma^2}$$ 在m个样本上的均方误差：$$MSE_{train}=\frac{1}{m}\sum_{i=1}^{m}||\hat{y}^i-y^i||^2$$ $p_{model}$将任意输入映射到一个实数来估计真实值概率，所有的x都是独立同分布的样本点，最大似然的目的在于对所有的输入x，其输出接近真实值的概率之和最大，最大似然估计和均方误差的作用差不多，都是衡量一个拟合函数的拟合效果好不好 5.6贝叶斯统计最大似然估计是预先知道函数点估计，也即知道函数权重$\theta$，然后对样本进行预测，贝叶斯统计中$\theta$也是变化的，是未知的定植，贝叶斯统计使用$\theta$的全估计，训练m个样本后的$\theta$对下一个样本的预测由影响，$\theta$是由前m个样本统计出来的(挖坑，这个地方理解的不是特别明白) 5.7监督学习 监督学习，每一个样本都有一个label,样本经过系统$f(x)$将被用于与label对比 支持向量机，类似于线性回归，支持向量机基于线性函数，分类的时候输出类别而非逻辑回归的概率，支持向量机使用了核技巧，将逻辑回归中的线性函数写成样本点积的形式：$f(x)=b+w^{-1}x=b+\sum_1^mx^{-1}x^{i}$,(-1表示矩阵转置，markdown不知道怎么输入转置)，使用核函数核估计替换点积：$f(x)=b+\sum_1^m\alpha_ik(x,x^{(i)})$,总之支持向量机使用核函数使系统非线性化，核函数有很多，比如高斯核，早起图像检测rcnn分类部分就使用了支持向量机吗，支持向量机好是好，不过已经落时了，核函数$\phi(x)$提供来一组描述x的特征###5.8无监督学习算法 主成分分析，将数据变换为元素之间彼此不相关表示，在输入空间寻找一个旋转，消除数据中未知变化因素，能用来干嘛？怎么-对输入数据作用于旋转W得到一个新的数据？ K-meas，将特征相似的数据分类，所有数据一共聚类成K类 ###5.9 随机提督下降 梯度下降，将所有的数据一口气全部塞入系统，计算慢，内存塞不下 小批量随机梯度下降，一次只使用一个样本训练，不收敛 随机梯度下降，一次计算batch-size个样本 第六章 深度前馈神经网络网络就是由一些计算单元链式的组成，前者输出为后者的输入，最终输出结果，计算单元的参数都可以学习 学习XOR, 异或单元 基于梯度的学习，基于梯度下降方法构建网络，使用概率模型构建代价函数，介绍了输出单元，高斯输出分布的线性单元，sigmod单元，softmax单元 隐藏单元，神经网络的隐藏层，一些计算单元，线性，非线形的，比如卷积，激活函数等 架构设计，万能近似定理高速我们，无论学习什么学习函数，都可以用一个足够大的MLP去逼近。架构设计，层与层之间是直接相连还是像resnet一样跳跃的相连接起来 反向传播算法，核心就是计算导数，即梯度，大部分机器学习的例子是计算代价函数关于参数$\theta$的导数##第七章 深度学习中的正则化正则化的目的在于向深度学习代价函数中添加惩罚，使原来的参数衰减 $L_2$正则，在代价函数中添加各个参数的平方项，线性的缩放每个参数$w_i$，$L_2$正则能使模型感知具有较高方差的输入数据，$L_2$：$\tilde{J}(w;X,y) = \frac{\alpha}{2}w^{\top}w + J(w;X,y)$,与之对应的梯度为$\Delta\tilde{J}(w;X,y)=\alpha w+\Delta \tilde J(w;X,y)$ ,更新梯度的公式为$w\leftarrow (1-\alpha)w+\Delta_{w}\tilde{J}(w;X,y)$,可以看出，正则化把权重参数做了收缩，即每步更新参数之前，先以常数因子收缩参数，再加上之前的导数项 $L_1$正则化，$\sum_i|w_i|$，定义为各个参数绝对值之和 数据增强，图片翻转，平移，随机剪裁，添加噪声等使泛化能力增强的操作 噪声鲁棒性，向模型中添加噪声本质就是向模型添加正则项，添加方差很小的噪声能防止大的权重越学习越大的极端 多任务学习，就是共享参数，参数的一部分公用于多种任务 提前终止，early stop，提前停止训练，书中的第一个粗略是用现在的validation error和之前的对比，如果现在的值比之前反而大，p次出现这种情况就停止，认为是出现过拟合 参数共享，某些特征使用相同的参数，以减少模型的大小 Bagging和其他集成方法，通过综合几个模型得出决策结果，比如求各个模型的平均 Dropout, droupout的具体操作是在一次训练中神经网络每一个单元以一定的概率置为0，在本次训练中这个参数被看作是0，再执行正向计算与反向计算，这种操作等同与有很多模型，然后对所有模型求平均，所有模型共享一份参数 对抗训练，有时候、向数据集中加入微小的扰动，会使得计算结果错误率大大提高，所以对抗训练就是不断向样本加入噪声，让系统认识到这是个错误的样本，基本的做法是对抗样本(加入噪声的样本)生成器G不断生成样本去欺骗判别模型D，D要识别这是假的样本，两者不断交手，最终系统能正确识别对抗样本 切面距离，正切传播，流形正切分类器，即用于监督学习吗，也用在了强化学习中吗，没看懂，挖个坑，日后来填 ##第八章 优化 8.1 学习和纯优化有什么不同 批量梯度下降， 使用整个训练集，一次性计算所有样本中单个样本，并行，然后参数求平均 随机梯度下降，每次只使用一个样本，一次计算一个样本的参数，然后再训练好的系统上训练下一个样本 小批量随机梯度下降，使用样本的数量介于一个与全部之间，一次计算batch-size个样本的参数，求平均然后再计算下一个batch-size###8.2 神经网络中的挑战 病态，随机梯度下降会卡在某些情况下，很小的更新步长会导致代价函数增大，应该是进入局部最小值(不太懂) 凸函数，就是凸函数，直观理解就是中间值大于两边的值 凸优化，在凸集上最小化凸函数 局部极小值，深度学习模型的局部极小值是可以接受的，因为它的代价函数也很小，加入它的代价函数很大，会使模型出问题 鞍点，高原，平坦，神经网络会迅速跳出鞍点，因为梯度下降的目的是不停的寻找梯度更低分方向，而牛顿法则会停在鞍点，因为牛顿法的目的是寻找梯度为0的地方 长期依赖，输入值经过特别深层次的神经网路容易引起梯度消失或者梯度爆炸，原因是梯度多次相乘会越来越小，比如线性函数，参数$\theta$是一个很小的值，经过多次的自己乘自己，差不多就没有来，再对变量求导，就可以忽略不计，resnet 通过旁路连接解决这个问题8.5 基本算法 随机梯度下降,选取m个样本，一次计算m个梯度，求平均，然后更新神经网络参数 动量，将之前一步的梯度作用于当前的梯度，取得加速的效果，比如在某个方向上，上一步与当前梯度该方向相反，那么该方向的梯度将背衰减，而与上一个方向同方向的变量将加速 nesterov动量，与上面动量算法唯一不同的地方在计算梯度的时候，动量法是在上一轮计算中更新过网络参数后直接用于当前批次数据的梯度计算，得到新梯度再与之前的速度矫正，nesterov算法是用之前的速度先更新网络参数，在用于计算当前批次数据 Adagrad, 将计算出来的梯度平方累积后再用于更新网络 RMSProp，累计梯度平方，与上一个算法的不同在于累积的方式8.6 二阶近似方法 牛顿法， 计算梯度，再计算Hessian矩阵(二阶梯度)，用计算的H矩阵的逆乘以梯度作为detle更新参数 共轭梯度法，用于解大型方程组，介于最速下降法和牛顿法之间的一个方法，只需要一阶导数信息， BFGS,在牛顿法的基础上的优化，牛顿法需要计算Hessian的逆矩阵，而BFGS用近似矩阵，省去了大量的计算开销###8.7优化策略和元算法 batch-norm,计算一个批数据的均值，训练的时候样本先减去均值，这样会使得收敛加速 坐标下降，优化函数包含多个要求解的参数，可以固定住一部分，训练另一部分，如此循环，直到所有的都求出最优解，书中举例$f(x)=(x_1-x_2)^2+\alpha(x_1^2+x_2^2)$就不适合用该方法，因为参数$x_1, x_2$之间的关系太密切，固定住一个意味着另一个就定下来了 Polyak算法，对优化算法访问过的参数$\theta$求均值，物理意义是，如果算法在两个山腰或者山顶之间来回走动而跨过山底部，那么使用polyak算法会取得折中的山底结果 设计有助于优化的模型，就是设计神经网络的堆叠次序，采用什么算子等，比如resnet使用旁路连接减少梯度消失，这不是很多研究人员等主要工作吗##第九章 卷积神经网络卷积神经网络启发于哺乳动物神经单元，输入信息经过神经元一直传输到输出，从数学角度解释就是输入经过庞大的网络参数得到输出，深度学习要干的核心的事情就是设计这个网络，求出这一堆参数 卷积，熟悉神经网络计算的很快就能在脑海里构建一副输入被卷积核反复移动得到一组输出，但是书中基于连续再到离散给了一堆的公式会让人摸不着头脑 动机， 之所以对输入做卷积的动机，比如做边缘检测，稀疏等等 池化，具体的操作是将某个点周围的数据结合起来，比如做平均，取最大，一般卷积，激活和池化会连在一起作为一个计算单元 高校卷积算法， 在信号与系统这本书中，两个信号的卷积元算等价于先将信号做傅立叶变换转化到频域，在频域做乘法，然后从频域转回时域，这样做是因为在通信领域系统能处理的是频域的信号，而我们人能处理的是时域的信号##第十章 序列建模:循环和递归网络读到循环网络，我脑海里还是李沐课程周杰伦歌词预测里的那图和代码，对于一个循环神经网络单元，有三个参数(不包括偏置)，目标是输入一个词向量能预测下一个词向量以能够作词，输入与第一个参数作用，上一时刻隐含单元的输出与第二个参数作用，这两者广播相加得到当前时刻的隐含层输出H，当前时刻隐含层输出作用于第三个参数得到结果，贴代码： 123456789101112def rnn(inputs, state, *params): # inputs: num_steps 个尺寸为 batch_size * vocab_size 矩阵。 # H: 尺寸为 batch_size * hidden_dim 矩阵。 # outputs: num_steps 个尺寸为 batch_size * vocab_size 矩阵。 H = state W_xh, W_hh, b_h, W_hy, b_y = params outputs = [] for X in inputs: H = nd.tanh(nd.dot(X, W_xh) + nd.dot(H, W_hh) + b_h) Y = nd.dot(H, W_hy) + b_y outputs.append(Y) return (outputs, H) 双向rnn，单向rnn是根据前面的信息推测后面的信息，而双向rnn还需要根据后面的信息推断前面的信息，比如”今天我肚子疼，想__一天”，根据后面可以缩小前面可以选词的范围，语音识别中，不光前面的声音输入决定输出，后面的信息也会决定前面的一些内容##第十一章 实践方法论 准确率， 正类预测为正类除以正类预测为正类与负类被预测为正类之和 召回率， 被预测为正类的样本除以所有正类样本的数量 网格搜索超参， 就是递归，加入有m个超参，每个参数有n 个选择，那么搜索的复杂度是O$(m^n$) 自动搜索，基于模型的搜索##第十二章 应用 CPU,GPU，后者可以并行计算，比如梯度下降，每个GPU单独计算自己的样本的梯度，然后将梯度取平均进行更新 分布式，数据并行，每个机器训练不同的数据，得到梯度后去更新共同维护的一份参数；模型并行，每个机器使用同一个数据，分别负责模型的不同部分 对比度归一化，对图片取均值等操作，一张图片的平均值被移除会怎么样？ 数据集增强，旋转，裁剪等操作 语音识别，没有接触过，目的是将一段音频与一段文字做映射，就像图像一样输入一副图像，输出一个分类，输入一段语音，通过神经网络输出一段文字，由于语音前后有关联性，需要循环神经网络处理(挖坑：语音识别为什么需要循环神经网络？) 自然语言处理 机器翻译，作诗作词 推荐，用神经网络提取想要的特征，比如提取音乐的特征，该特征和用户喜欢歌曲特征做匹配 强化学习推荐，采取动作a获得奖励，采取动作$a^`$获取另外一个奖励，每次以一定的概率采取动作以获得最大的奖励同时也以一定的概率向周围动作探索以获得更大的奖励##第十三章 线性因子模型 独立成分分析， 信号拆分成独立的进行分析，就像信号分解一样，相同房间用不用语言说话的人可以将内容解析成不同的语言进行分析 慢性特征分析，重要特征随时间变化缓慢 稀疏编码， 推断层h的过程和学习该层的参数##第十四章 自编码器就是压缩，并且是有损压缩，将数据变得更小，降维等等，用深度网络自动学习压缩算法，输入$x$,输出$x^`$使得两者尽可能一样 第十五章 表示学习同样的数据可以有不同的表示，比如数字可以用阿拉伯数字表示也可以用罗马数字表示，但是用罗马数字表示的数字做除法会很蛋疼。它和深度学习的关系是，比如在一个分类器中，数据从输入到softmax层之前，都属于数据的表示，最后才是任务阶段。表示学习的目的就是要对原始数据学习到最好的表示，以方便后期任务的进行。 贪心逐层无监督预训练，（挖坑：不理解，如何一层一层无监督训练，将数据卷积，处理，怎么样处理？） 迁移学习，用一个已经学习好的模型，作用于另外一个任务，具体的操作比如在imagenet使用resnet50训练好的1000类要作用于flower数据集上，那需要把训练好的模型最后一层fc去掉，再添加一个102分类的fc作为最后的分类，这样再在flower有标签数据集上训练##第十六章 深度学习中的结构化概率模型计算图是数据输入到输出由一开始就定义好计算规则，数据就是正常的输入，比如图片，语音等，结构化的概率模型指将计算节点的数据表示称概率，用于处理与概率相关的任务，如接力跑问题，谁依赖谁跑完多少时间的概率受限玻尔兹曼机，将数据替换成二值变量，是0还是1的概率取决于参数w和前面相连接节点的值##第十七章 模特卡罗方法蒙特卡罗方法等思想是用近似替代准确，用概率解代替解析解，比如算圆周率，向二维[0,1]之间均匀采样，最后在采样的点中，落在圆中的点除以总的点就能得到圆周率。离散函数求和，连续函数求积分都可以表示为p*f的形式，求函数f在概率p的情况下的期望，比如f(1)在1处取0的概率0.1，取0.5点概率0.4，取1的概率0.5，那么它的期望就是0.1*0+0.5*0.4+1*0.5,在物理理解上就是大量撒点，并求出小于函数值的那些点挖坑：马尔可夫链蒙特卡洛方法，吉布斯采样方法，看着这一堆公式，和我理解的都不太一样##第十八章 直面配分函数##第十九章 近似推断变分推断，如果隐含层分子相互独立，联合分布可以改成各个分布乘积 $q(h|v)=\prod_i(q_i|v)$ 第二十章 深度生成模型玻尔兹曼机，受限玻尔兹曼机，深度信念网络，深度玻尔兹曼机，时值数据上的玻尔兹曼机，用于结构化或序列输出的玻尔兹曼机 唧唧歪歪一大堆网络，就是输入一些数据，输出一堆数据，并且已经过时了 有向生成网络，应该就是目前比较流行的卷积神经网络什么的了]]></content>
  </entry>
  <entry>
    <title><![CDATA[resnet,densenet的反向传播详解]]></title>
    <url>%2F2020%2F08%2F22%2F%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%2F</url>
    <content type="text"><![CDATA[正常网络的反向传播 $z=x \bigotimes W1$, $h=\phi(z)$, $o = h \bigotimes W2$, $L=(o-y)^2$, 初始化网络参数，给个初始值，经过前向传播，图中x,z,h,o的值都是已知的 $\frac{\partial L}{\partial W2}= \frac{\partial L}{\partial o}· \frac{\partial o}{\partial W2}$, o,h是已知的，$\frac{\partial o}{\partial W2}=h$, $\frac{\partial L}{\partial o}=2(o-y)$,代入公式得$\frac{\partial L}{W2}=2h(o-y)$ $\frac {\partial L}{\partial W1}=\frac{\partial L}{\partial o} \frac{\partial o}{\partial h} \frac{\partial h}{\partial z} \frac{\partial z}{\partial x}=\frac {\partial L}{\partial o} W2 \bigotimes \frac{\partial \phi(z)}{\partial z} x^T=2(o-y) W2 \bigotimes \frac{\partial \phi(z)}{\partial z} x^T$ resnet的反向传播一个正常的两层网络 对于一个正常的block，$\frac{\partial L}{\partial l_1}=\frac{\partial L}{\partial l_2} \frac{\partial l_2}{\partial l_1}=\frac{\partial L}{\partial l_2} \frac{\partial l_2}{\partial o}W$ 一个两层的resent block $\frac{\partial L}{\partial l_1}=\frac{\partial L}{\partial l_2}\frac{\partial l_2}{\partial o}\frac{\partial o}{\partial l_1}=\frac{\partial L}{\partial l_2}\frac{\partial l_2}{\partial o} \frac{\partial(o_1+l_1)}{\partial l_1}=\frac{\partial L}{\partial l_2} \frac{\partial l_2}{\partial o}(W+1)$ 和没有残差的两层block相对比，在W的位置多出了1，即梯度不衰减的传递回去了，避免梯度消失 densenet的反向传播两层的densenet结构如下 从L反向传播到$l_1$梯度： $\frac{\partial L}{\partial l_1}=\frac{\partial L}{\partial l_2}\frac{\partial l_2}{\partial o}\frac{\partial o}{\partial l_1}=\frac{\partial L}{\partial l_2}\frac{\partial l_2}{\partial o} \frac{\partial(o_1 cat\ l_1)}{\partial l_1}=\frac{\partial L}{\partial l_2} \frac{\partial l_2}{\partial o}(W cat\ 1)$ cat为运算算子，cat=concate,将两个张量按照维度拼接起来，$W cat\ 1$是在W上按照$l_1$的shape的1全部concate到W上]]></content>
  </entry>
  <entry>
    <title><![CDATA[darknet转caffe]]></title>
    <url>%2F2020%2F08%2F22%2Fdarknet%E8%BD%ACcaffe%2F</url>
    <content type="text"><![CDATA[参考：https://blog.csdn.net/nodototao/article/details/85711703 https://blog.csdn.net/cgt19910923/article/details/83242079 1 修改caffe源码并编译参考安装caffe 2 模型转换 安装 pytorch 下载github开源工程https://github.com/marvis/pytorch-caffe-darknet-convert 下载https://github.com/ChenYingpeng/caffe-yolov3/blob/master/model_convert/yolov3_darknet2caffe.py，用这个文件替换上一个工程根目录下的darknet2caffe.py]]></content>
  </entry>
  <entry>
    <title><![CDATA[int8量化]]></title>
    <url>%2F2020%2F08%2F22%2Fquantize%2F</url>
    <content type="text"><![CDATA[梯度量化参考： http://on-demand.gputechconf.com/gtc/2017/presentation/s7310-8-bit-inference-with-tensorrt.pdf 目标：将FP32的数据转换为INT8，并且精度不会降低很多 为什么：INT8有更高的吞吐量，更低的内存占用 挑战：INT8精确度低的问题 解决办法：在量化训练好的模型参数和计算的时候最小化信息损失 结果：TensorRT已经搞定了这些 量化 原理一句话原理将fp32的卷积(相乘)转换为int8的卷积(相乘) A = scale_A * QA + bias_A B = scale_B * QB + bias_B AB = scale_A scale_B QA\QB + ​ scale_A *QA+ bias_B + ​ scale_B *QB + bias_A + ​ bias_A * bias_B 英伟达通过实验说不要偏置也没有影响 A = scale_A * QA B = scale_B * QB AB = scale_A QA*scale_B * QB 线性量化将FP32的值线性映射到一个-127~127之间的数 ，具体的做法是将参数从小到大排列，然后平均分成255份，最小的那一份的数对应-127，次小的那一份数对应-126… 饱和量化将fp32过大过小的值去掉，再执行线性映射 如何找到合适的阈值 收集不同样本计算下参数的直方图 ，比如某一层resnet_52_conv 产生很多阈值，将FP32的值映射到int8 让不同阈值的FP32激活值分布和INT8激活值做KL_divergence，选出该最小对应的阈值 如何做KL散度 $$KL = min(H(P)-H(Q))$$，P为参考样本，Q为int8计算之后的样本 LK_divergence具体是怎么做的？ 小目标检测中，转tensorrt精度会有很大的损失，原因是系统把红绿灯那一小块卷积后的结果当成噪点处理了，被截断，这是猜想 不断阶段参考样本，从128开始 将截断外的数值求和 将2中得到的值添加到阶段样本P之后 求样本P的概率分布 创建样本Q，它是从P的INT8量化 将样本Q拉伸到和P一样长 计算Q的概率分布 求P,Q的KL散度，得到T]]></content>
  </entry>
  <entry>
    <title><![CDATA[svm算法原理]]></title>
    <url>%2F2020%2F08%2F22%2Fsvm%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[间隔与支持向量有类别的数据需要被分为两类，通过一个最优超平面将其分为两部分，推理的时候在最优超平面一边的为一类，在另一边的为另一类，最优超平面定义为$w^Tx+b=0$，模型为$f(x)=w^Tx+b$。 现在的问题是如何求得这个最优超平面，根据点到直线的距离，从支持向量到最优超平面的距离为$\frac{wx+b}{|w|}$，求最小距离时，即求支持向量到最优超平面的距离，因为对于支持向量有$wx+b=1$,目标转为$min \ w$ 最优化的目标转化为： $min\frac{1}{2}||w||^2$ $s.t. \ y_i(w^Tx_i+b)&gt;=1, i=0, 1, 2, 3…m$ 式6.6 对偶问题目的：利用式6.6的对偶问题得到模型$f(x)$ 使用朗格朗日乘子法可以解式6.6， 对式6.6点每个约束条件加拉格朗日乘子$\alpha_i$,该问题的拉格朗日式表示为： $L(w, b, \alpha)=\frac{1}{2}||w||^2+\sum_{i=1}^m\alpha_i(1-y_i(w^Tx_i+b))$ 式6.8 令L对$w$和b偏导为0： $w=\sum_{i=1}^m\alpha_i y_ix_i$ 6.9 $\sum_{i=0}^m\alpha_iy_i=0$ 6.10 将这两个偏导得到的条件代入6.8式，可以消去w和b，6.9代入6.8，消去w，b由6.10整体代入消除；得到式6.6的对偶问题： $max \ \sum_i^m\alpha_i-\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_jx^T_ix_j$ 式6.11 $s.t. \ \sum_{i=1}^{m}\alpha_iy_i=0$ $\alpha_i&gt;=0$ 挖坑：为什么这里可以通过式6.6点对偶问题得到6.6的解？ 解出式6.11中的$\alpha_i,\alpha_j$就可以得到w, 从而得到模型： $f(x)=wx+b=\sum_{i=1}^m\alpha_iy_ix_ix+b$ 利用smo算法求解6.11式 smo的思想是选取一对需要更新的参数$\alpha_i, \ a_j$其他参数固定，根据式6.11的约束条件，将一个变量用另外一个变量表示，6.11变成二次凸优化问题，解出$\alpha_i,\alpha_j$，具体的： 式6.11的约束条件重新写为：$\alpha_{i}y_{i}+\alpha_{j}y_{j}=c$ 将上式$\alpha_{j}$用$\alpha_{i}$表示：$\alpha_{j}=\frac{c-\alpha_{i}y_{i}}{y_{j}}$ 式6.11此时变成只有一个变量的二次函数，仅有的约束是$\alpha_{i}$大于0. b的求解可以通过支持向量的$y_sf(x_s)=1$解出 $y_s(wx+b)=1$-&gt; $b=\frac{1}{y_s}-wx$，更鲁棒一点的可以对所有的支持向量求均值 核函数原先的输入空间可能是线性不可分的，将其映射到高维空间后就可以分了 $x-&gt;\phi(x)$ 模型$f(x)=wx+b$变成$f(x)=w^T\phi(x)+b$ 最优化问题 $min\ \frac{1}{2}||w||^2$ $s.t.\ y_i(wx_i+b)&gt;=1$ 用拉格朗日解法，其对偶问题为： $max\ \sum_{i=1}^{m}\alpha_i+\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_j\phi(x_i)^T\phi(x_j)$ $s.t.\ \alpha_iy_i=0$ 由于$\phi(x_i)\phi(x_j)$的计算量可能很大，用核函数$k(x_i, x_j)$替代]]></content>
  </entry>
  <entry>
    <title><![CDATA[caffe安装]]></title>
    <url>%2F2020%2F08%2F22%2Fcaffe%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[安装caffe前提：安装好cuda8, cudnn, opencv3 参考： http://caffe.berkeleyvision.org/install_apt.html https://blog.csdn.net/yhaolpz/article/details/71375762 http://3ms.huawei.com/km/blogs/details/5425467 http://3ms.huawei.com/km/blogs/details/5577119 git clone https://github.com/BVLC/caffe.git（修改过以适配yolov3）-&gt;/data0/hhq/project/caffe-master/ caffe-master, mv Makefile.config.example Makefile.config ```修改12将 #USE_CUDNN := 11修改成： USE_CUDNN := 112将 #OPENCV_VERSION := 31修改为： OPENCV_VERSION := 312将 #WITH_PYTHON_LAYER := 11修改为 WITH_PYTHON_LAYER := 112打开anaconda3下的python并使用boost python ANACONDA_HOME := /data/softinstall/anaconda3PYTHON_INCLUDE := $(ANACONDA_HOME)/include \ $(ANACONDA_HOME)/include/python3.6m \ $(ANACONDA_HOME)/lib/python3.6/site-packages/numpy/core/include Uncomment to use Python 3 (default is Python 2)PYTHON_LIBRARIES := boost_python3 python3.5m12添加include lib USE_PKG_CONFIG := 1INCLUDE_DIRS += /usr/include/hdf5/serial /data/soft/opencv-3.4.0/include/LIBRARY_DIRS += /usr/lib/x86_64-linux-gnu/hdf5/serial/12在文件末尾添加上,添加最后一行是因为编译的时候去conda/lib下去找zlib.so这个包 LINKFLAGS := -Wl,-rpath,/data/softinstall/anaconda3/lib123- make -j24- ```cd caffe-master, make pycaffe -j8 测试， import caffe 使用的时候,先添加bashrc PYTHONPATH12或者在代码中 caffe_root=’/data/project/caffe/python’import syssys.path.insert(0, caffe_root+’python’)1234### 报错- ```/data/softinstall/anaconda3/lib/libpng16.so.16：对‘inflateValidate@ZLIB_1.2.9’未定义的引用```,添加```LINKFLAGS := -Wl,-rpath,/data/softinstall/anaconda3/lib 找不到库 1234/usr/bin/ld: 找不到 -lboost_python3/usr/bin/ld: 找不到 -lpython3.5m/usr/bin/ld: 找不到 -lcblas/usr/bin/ld: 找不到 -latlas 123先查找，再安装apt-cache search atlassudo apt-get install libatlas-base-dev /usr/lib/x86_64-linux-gnu/```12```sudo ln -s libpython3.5m.so.1 libpython3.5m.so sudo ln -s libboost_python-py35.so libboost_python3.so]]></content>
  </entry>
  <entry>
    <title><![CDATA[TensorRT-CenterNet-物体检测]]></title>
    <url>%2F2020%2F08%2F22%2FTensorRT-CenterNet-ObjectDet%2F</url>
    <content type="text"><![CDATA[源码https://github.com/qingzhouzhen/CenterNet，fork自https://github.com/xingyizhou/CenterNet， 2019/10/8 0.4.1```123456#### 训练模型按照&lt;https://github.com/qingzhouzhen/CenterNet/blob/master/experiments/ctdet_coco_dla_1x.sh&gt;，默认使用pose_dla_dcn, 使用修dlav0模型：```cd path/CenterNet/src 1python main.py ctdet --arch dlav0_34 --exp_id coco_dla_1x --batch_size 64 --master_batch 16 --lr 5e-4 --gpus 0,1,2,3 --num_workers 16 --resume 之所以用dlav0_34是因为dla中含有Convolutional Networks)```， TensorRT不支持这个层，为了方便使用dlav0123456训练完后在```path/CenterNet/exp/ctdet/coco_dla_1x```路径下生成```model_best.pth```和```model_last.pth```两个模型#### 测试使用作者提供的demo.py脚本进行测试 ctdet –arch dlav0_34 –demo /data0/hhq/project/CenterNet/images/ –load_model ../models/model_best.pth –gpus 3123#### 转onnx使用脚本 ctdet –arch dlav0trans_34 –demo /data0/hhq/project/CenterNet/images/ –load_model ../models/model_best.pth –gpu 312修改```dlav0.py DLASeg```类的```forward```函数，修改成，这样返回的就不是3个heatmap，而是4个，最后一个是第一个maxpool之后的结果 def forward(self, x): x = self.base(x) x = self.dla_up(x[self.first_level:]) # x = self.fc(x) # y = self.softmax(self.up(x)) ret = [] for head in self.heads: ret.append(self.__getattr__(head)(x)) sigmoid = ret[0].sigmoid_() hmax = nn.functional.max_pool2d( sigmoid, (3, 3), stride=1, padding=1) ret[0] = hmax ret.append(sigmoid) return [ret] 1234把返回的字典修改成列表，因为TensorRT不支持字典操作；将heatmap做sigmoid和maxpool,为了方便做nms。转换代码参考: import _init_paths import osimport cv2 from opts import optsfrom detectors.detector_factory import detector_factoryfrom torch.autograd import Variableimport torch image_ext = [‘jpg’, ‘jpeg’, ‘png’, ‘webp’]video_ext = [‘mp4’, ‘mov’, ‘avi’, ‘mkv’]time_stats = [‘tot’, ‘load’, ‘pre’, ‘net’, ‘dec’, ‘post’, ‘merge’] def demo(opt): os.environ[‘CUDA_VISIBLE_DEVICES’] = opt.gpus_str opt.debug = max(opt.debug, 1) Detector = detector_factory[opt.task] detector = Detector(opt) torch_model = detector.model c, h, w = (3, 512, 512) dummy_input = Variable(torch.randn(1, c, h, w, device=’cuda’)) torch.onnx.export(torch_model, dummy_input, “/data0/hhq/project/CenterNet/models/dlav0_opt_80.onnx”, verbose=True, export_params=True) #, operator_export_type=OperatorExportTypes.ONNX_ATEN) if name == ‘main‘: opt = opts().init() demo(opt)` 转完后生成.onnx结尾的文件 onnx转TensorRTTensorRT-5.1.2 参考：http://code-cbu.huawei.com/EI-VisonComputing/AlgorithmGroup/ServiceSDK/ConvertTensorRT/alphapose_convert_tensorrt 加载RT模型并推理自己写的ctdecode]]></content>
  </entry>
  <entry>
    <title><![CDATA[无人机下道路语义分割(FCN)]]></title>
    <url>%2F2020%2F08%2F22%2F%E6%97%A0%E4%BA%BA%E6%9C%BA%E4%B8%8B%E9%81%93%E8%B7%AF%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2(FCN)%2F</url>
    <content type="text"><![CDATA[环境 ubuntu16.4 CUDA 9.0 Anaconda 3 （numpy、os、datetime、matplotlib） pytorch == 0.4.1 or 1.0 torchvision == 0.2.1 OpenCV-Python == 3.4.1 源码https://github.com/qingzhouzhen/road_seg 原理 下采样之后不断上采样，每次上采样后和下采样后相同形状的张量相加，最终得到输入和输出张量的shape相同，每个像素都被预测类别。 数据原图放在一个文件夹，标签图片放在另外一个文件夹，标签图片为黑白图片，但是是3通道。原图生成标签图片的脚本：https://github.com/qingzhouzhen/data_process/blob/master/generate_road_mask.py]]></content>
  </entry>
  <entry>
    <title><![CDATA[mxnet如何查看中间结果]]></title>
    <url>%2F2020%2F08%2F22%2Fmxnet%E5%A6%82%E4%BD%95%E6%9F%A5%E7%9C%8B%E4%B8%AD%E9%97%B4%E7%BB%93%E6%9E%9C%2F</url>
    <content type="text"><![CDATA[mxnet如何查看权重，中间输出结果 参考：https://blog.csdn.net/u010414386/article/details/55668880 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364import mxnet as mximport cv2ctx = mx.cpu()sym, arg_params, aux_params = mx.model.load_checkpoint(&apos;model/light_classify&apos;, 299)args = sym.get_internals().list_outputs() #获得所有中间输出print(&quot;args:&quot;, args)internals = sym.get_internals()conv1 = internals[&apos;conv_1_conv2d_output&apos;]conv1_bn = internals[&apos;conv_1_batchnorm_output&apos;]group = mx.symbol.Group([sym, conv1, conv1_bn])mod = mx.mod.Module(symbol=group, context=ctx, label_names=None)mod.bind(for_training=False, data_shapes=[(&apos;data&apos;, (1,3,16,16))], label_shapes=mod._label_shapes)mod.set_params(arg_params, aux_params, allow_missing=True)with open(&apos;synset.txt&apos;, &apos;r&apos;) as f: labels = [l.rstrip() for l in f]import matplotlib.pyplot as pltimport numpy as np# define a simple data batchfrom collections import namedtupleBatch = namedtuple(&apos;Batch&apos;, [&apos;data&apos;])def get_image(url, show=False): # download and show the image. Remove query string from the file name. # fname = mx.test_utils.download(url, fname=url.split(&apos;/&apos;)[-1].split(&apos;?&apos;)[0]) img = mx.image.imread(url) if img is None: return None if show: plt.imshow(img.asnumpy()) plt.axis(&apos;off&apos;) # convert into format (batch, RGB, width, height) img = mx.image.imresize(img, 16, 16) # resize img = img.transpose((2, 0, 1)) # Channel first # print(&quot;+++++++++++&quot;, img.asnumpy().__repr__()) img = img.expand_dims(axis=0) # batchify img = img.astype(&apos;float32&apos;) # for gpu context return imgdef predict(): # path = &apos;pic/B1000_1.jpg&apos; # path = &apos;pic/black/190.121.11.119_20180101_080207_00124_1.jpg&apos; # black path = &apos;pic/green/aa2600000005_1.jpg&apos; # green # path = &apos;pic/yellow/wuhe_zhangheng_S_34_9-00_9-30-033_0.jpg&apos; # path = &quot;pic/red/aa1700000017_0.jpg&quot; path = &apos;/data0/hhq/data/light/test/yellow/190.201.11.101_20180101_075246_00231_1_0.jpg&apos; img = get_image(path, show=False) # compute the predict probabilities mod.forward(Batch([img])) prob = mod.get_outputs()[0].asnumpy() conv1_out = mod.get_outputs()[1] conv1_bn_out = mod.get_outputs()[2] # print the top-5 prob = np.squeeze(prob) a = np.argsort(prob)[::-1] for i in a[0:5]: print(&apos;probability=%f, class=%s&apos; %(prob[i], labels[i]))predict() caffe查看中间结果123456789101112131415161718192021222324252627282930313233343536373839404142434445464748caffe_root=&apos;/data0/hhq/project/caffe-master/&apos;import syssys.path.insert(0, caffe_root+&apos;python&apos;)import caffeimport numpy as npimport structimport matplotlib.pyplot as pltimport cv2# caffe.set_mode_gpu()# caffe.set_device(1)deploy=&apos;model_caffe/light_classify.prototxt&apos;caffe_model= &apos;model_caffe/light_classify.caffemodel&apos;# image_path =&apos;pic/red/aa1700000017_0.jpg&apos; # red# image_path = &apos;pic/yellow/wuhe_zhangheng_S_34_9-00_9-30-033_0.jpg&apos; # yellow# image_path =&apos;pic/black/190.121.11.119_20180101_080207_00134_0.jpg&apos; # blackimage_path = &apos;pic/green/aa2600000006_0.jpg&apos;net = caffe.Net(deploy, caffe_model, caffe.TEST)transformer = caffe.io.Transformer(&#123;&apos;data&apos;: net.blobs[&apos;data&apos;].data.shape&#125;)transformer.set_transpose(&apos;data&apos;, (2, 0, 1))transformer.set_raw_scale(&apos;data&apos;, 225)# transformer.set_channel_swap(&apos;data&apos;, (2, 1, 0)) # should not exchangenet.blobs[&apos;data&apos;].reshape(1, 3, 16, 16)im = caffe.io.load_image(image_path)image_pro = transformer.preprocess(&apos;data&apos;, im)net.blobs[&apos;data&apos;].data[...] = image_proout = net.forward()fc = net.blobs[&apos;fc&apos;].data[0].flatten()conv1 = net.blobs[&apos;conv_1_conv2d&apos;].data[0]conv1_bn = net.blobs[&apos;conv_1_batchnorm&apos;].data[0]prob = net.blobs[&apos;fc&apos;].data[0].flatten()print(fc)print(prob)order = prob.argsort()[3]print(&apos;the class is:&apos;, order)]]></content>
  </entry>
  <entry>
    <title><![CDATA[ubuntu16.04安装cuda]]></title>
    <url>%2F2020%2F08%2F22%2Fubuntu16.04%E5%AE%89%E8%A3%85cuda%2F</url>
    <content type="text"><![CDATA[环境ubuntu16.04.01桌面 kernel 4.4.0 RTX270 禁用nouveaunouveau是默认的显卡驱动，一般linux出厂都使用该驱动，要禁止它 创建文件: vilink12添加内容 blacklist nouveauoptions nouveau modeset=012更新```initramfs 1sudo update-initramfs -u 重启 | grep nouveau```12345678没有输出则禁用成功#### 关闭X Server`CTRL+ALT+F1`进入tty1```sudo service lightdm stop 安装cuda习惯采用run文件安装，连驱动一起安装，之前都是ubuntu server上，很容易就安装上了，但是换了桌面，驱动死活安装不上，总是报错 1The driver installation is unable to locate the kernel source. Please make sure that the kernel source packages are installed and set up correctly. If you know that the kernel source packages are installed and set up correctly, you may pass the location of the kernel source with the &apos;--kernel-source-path&apos; flag 所以单独安装，先安装驱动 下载驱动：https://www.nvidia.com/Download/index.aspx?lang=en-us，选择适合自己的系统的选项，页面提示推荐的驱动版本，下载 安装驱动 1sudo sh NVIDIA-Linux-x86_64-430.40.run 提示distribution-provided pre-install script failded! Are you sure you want to continue?```，不用理会，选择```continue installation```123456789101112```warming```不理会，继续安装```Would you like to run the nvidia-xconfig utility…```选择是，安装完毕，输入```nvidia-smi```可以显示驱动，显卡名称等信息再安装cuda下载：&lt;https://developer.nvidia.com/cuda-92-download-archive?target_os=Linux&amp;target_arch=x86_64&amp;target_distro=Ubuntu&amp;target_version=1604&amp;target_type=runfilelocal&gt;安装：``sudo sh cuda_9.2.148_396.37_linux.run``,选择的时候驱动不要安装设置环境变量 export PATH=/usr/local/cuda-9.2/bin:$PATHexport LD_LIBRARY_PATH=/usr/local/cuda-9.2/lib64:$LD_LIBRARY_PATH12打开x-server sudo service lightdm start1234#### 测试cuda安装 sudo make -j4cd path/NVIDAI_CUDA-9.2_Samples/bin/x86_64/linux/releases./deviceQuery12以上过程必要是安装如下更新，我执行过下面这些步骤，但是不知道是否起作用 sudo apt-get updatesudo apt-get upgradesudo apt dist-upgrade123456#### cudnn安装下载对应的cudnn版本，解压后执行： sudo cp cuda/include/cudnn.h /usr/local/cuda/include/sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64/12查看cudnn版本： cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2` 参考 https://developer.download.nvidia.cn/compute/cuda/9.2/Prod2/docs/sidebar/CUDA_Installation_Guide_Linux.pdf https://zhuanlan.zhihu.com/p/36979657 https://zhuanlan.zhihu.com/p/68069328]]></content>
  </entry>
  <entry>
    <title><![CDATA[ExtremeNet训练自己的数据]]></title>
    <url>%2F2020%2F08%2F22%2FExtremeNet_custom%2F</url>
    <content type="text"><![CDATA[环境labelme数据转换coco格式用python2，labelme, Ubuntu16.04, python3.7 数据准备 下载转labelme到coco格式的脚本：https://github.com/wucng/TensorExpand.git 将1中下载的12labelme2COCO.pyTensorExpand/TensorExpand/Object detection/Data_interface/MSCOCO/labelme data 拷贝到labelme的文件夹中 在labelme标注文件中执行 1python labelme2COCO.py 3如果正确，将生产new.json，coco格式的标签文件 1mv new.json instances_train2017.json 下载ExtremeNet, 1https://github.com/xingyizhou/ExtremeNet.git 将图片数据放在1ExtremeNet/data/coco/images 中， 将coco格式的标签文件放在1ExtremeNet/data/coco/annotations 中 123cd ExtremeNet/tools``` 执行 python python gen_coco_extreme_points.py1生成 instances_extreme_train2017.json123456789101112131415文件### 安装ExtremeNet&lt;https://github.com/xingyizhou/ExtremeNet#installation&gt;### 测试EXtremeNet&lt;https://github.com/xingyizhou/ExtremeNet#demo&gt;### 训练1. 修改```/db/coco_extreme.py 56行，我这里模型有3类，改为 self._cat_ids = [1, 2, 3] utils/debugger.py 103行 num_classes改为类别数，我改为3 demo.py改class_name, 修改为自己的类别名称，推理的时候用 db/detection.py8行改类别数，我改为self._configs[&quot;categories&quot;]= 3 models/py_utils/_cpools/exkp.py修改330的num_class为3 ExtremeNet.json修改categories为3， 根据机器情况修改batch_size, max_iter, stepsize, snapshot, chunk_size 修改model/py_utils/ExtremeNet.py19行out_dim,我修改为3 执行训练python train.py ExtremeNet]]></content>
  </entry>
  <entry>
    <title><![CDATA[Extremenet]]></title>
    <url>%2F2020%2F08%2F22%2FExtremeNet%2F</url>
    <content type="text"><![CDATA[ExtremeNet论文地址： 摘要提出一种目标检测新思路，首先通过标准的关键点检测检测四个极值点和一个中心点，然后通过几何关系对提取到的关键点进行分组，5个极值点对应一个检测结果 该方法使目标检测转化为一个基于外观信息的关键点估计问题，从而避开区域分类和隐含特征学习 原理 在O(hw)的空间预测5个heatmap，代表该位置是否是某个类别的极左点，极右点，极顶点，极底点，预测的是概率值，代表该位置是否有极值点以及它所属的类别；ExtremeNet通过暴力组合所有的极点来判断哪些点来自于同一个物体，通过四个极值点计算该四个点的物理中心，到center heatmap中找到该点的值，如果该值超过阈值则这是一个有效的组合 edge aggregation当物体边缘处于水平或者竖直状态，极值点会不唯一，这样会导致两个问题，一个是极值点的极值响应会低于阈值，导致极值消失，漏检；另一个问题，同一个物体，水平状态下极值点响应会很低，但是旋转一个比较小的角度，该物体的极值点的响应会很大。 解决该问题的办法是左右极值点在竖直方向上聚合，上下极值点上水平聚合，聚合直到遇到单调递减的最低值 nms对相应的heatmap做nms，heatmap上是一些分数值，通过一次max_pool，然后判断max_pool后和原来一样的点的位置，这些点保留，其他的删除，即选出3x3滑动窗口上最大的那个位置 去置信度最大的40个点，记录他们的位置和置信度 lossExtremeNet角点和offset的loss采用了CornerNet的loss，没有组点的过程所有没有那部分的损失，但是角点的损失由2个增加到5个]]></content>
  </entry>
  <entry>
    <title><![CDATA[github+hexo+mac建博客]]></title>
    <url>%2F2020%2F08%2F22%2Fgithub%2Bhexo%2Bmac%E5%BB%BA%E5%8D%9A%E5%AE%A2%2F</url>
    <content type="text"><![CDATA[由于板瓦工升级，之前的wordpress弃坑，转hexo+github方案 一 安装node.js 安装node 123brew link nodebrew uninstall nodebrew install node 验证node 12npm -vnode -v 卸载node 1brew uninstall node 二 配置ssh免密登陆 配置免密登陆 1github-&gt;setting-&gt;SSH And GPG keys-&gt;New SSH Key 验证配置成功 1ssh -T git@github.com 输入yes 显示： 12Warning: Permanently added &apos;github.com,192.30.252.130&apos; (RSA) to the list of known hosts.You&apos;ve successfully authenticated, but GitHub does not provide shell access. 三 安装hexo,部署项目 安装hexo 1npm install -g hexo-cli 创建项目 123hexo init &lt;folder&gt;cd &lt;folder&gt;npm install 本地部署 123hexo generate本地部署：hexo server 部署到github 1vim _config.yml 进入文件最后部分 1234deploy: type: git repository: https://github.com/qingzhouzhen/qingzhouzhen.github.io branch: master 部署： 12hexo ghexo d 若执行hexo g出错则执行npm install hexo --save，若执行hexo d出错则执行npm install hexo-deployer-git --save。错误修正后再次执行hexo g和hexo d上传到服务器。 更换next主题，进入到项目根目录 1git clone https://github.com/iissnan/hexo-theme-next themes/next 四 绑定域名 github端 在项目主题中source文件夹中创建CNAME文件，没有后缀名，然后将个人域名huanghanqing.com添加进CNAME文件即可，然后通过hexo g hexo d重新部署网站。 域名解析 登陆域名解析商：https://www.dnspod.cn/console/dns/huanghanqing.com 记录类型：CNAME 主机记录：@ 解析线路：默认 记录值：qingzhouzhen.github.io 重新建一个记录，因为我之前指向了板瓦工端ip，这里指向了CNAME,把之前存在的cname记录删掉重新建一个 next 主题设置文章只显示预览，themes-&gt;next-&gt;_config.yml 12345# Automatically Excerpt. Not recommand.# Please use &lt;!-- more --&gt; in the post to control excerpt accurately.auto_excerpt: enable: false length: 150 false改为true 参考： https://zhuanlan.zhihu.com/p/34654952 https://www.jianshu.com/p/5031134c374e]]></content>
  </entry>
  <entry>
    <title><![CDATA[优化算法]]></title>
    <url>%2F2020%2F08%2F22%2F%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[随机梯度$$x_t = x{t-1}+\eta \nabla f(x)$$ 小批量随机梯度下降$$x_t=x_{t-1}+\frac{1}{m}\sum_i^m\nabla f(x^i)$$ 动量法$$v_t=v_{t-1}+g_t$$ $$x_t=x_{t-1}+v_t$$ 如果当前某个方向梯度和上一个方向不一致，则会衰减，如果一致则会得到加强 Adagrad自动调整学习率，通过不断累积梯度，梯度累积多少用以调整后期的学习率 $$s_t = s_{t-1}+g_t \bigodot g_t$$ $$x_t = x_{t-1}- \sqrt{\frac{\eta}{s_t+\epsilon}}g_t$$ RMSProp在Adagrad基础上，元素带参数指数加权平均，具体的就是在梯度累积s前面加$\gamma$，梯度累积的时候有方向 $$s_t = \gamma s_{t-1}+(1-\gamma)g_t \bigodot g_t$$ $$x_t = x_{t-1}+\sqrt \frac{\eta}{s_t+\epsilon}g_t$$ Adadelta$$s_t=s_{t-1}+ (1-\gamma )g_t \bigodot g_t$$ $$g_t`=\sqrt \frac{\nabla x+\epsilon}{s_t + \epsilon}\bigodot g_t$$ $$x_t = x_{t-1}+g_t`$$ $$\nabla x​$$按元素加权平均 $$\nabla x = \rho \nabla x +(1-\rho)g_t\bigodot g_t$$ Adam结合动量法和自动调整学习率 $$v_t = (\beta_1)v_{t-1} + 1-\beta_1)g_t$$ $$s_t = \beta_2s_{t-1}+(1-\beta_2)g_t \bigodot g_t$$ $$x_t = x_{t-1} + \sqrt{\frac{\eta}{s_t+\epsilon}}v_t$$ 以上推倒中x代表参数，t代表一步迭代，t-1代表前一时刻迭代，v代表梯度的惯量，g代表梯度]]></content>
  </entry>
  <entry>
    <title><![CDATA[Mask-rcnn]]></title>
    <url>%2F2020%2F08%2F22%2FMask-rcnn%2F</url>
    <content type="text"><![CDATA[mask-rcnn在faster-rcnn基础上修改而来，网络结构图如下 roi-align roi-pooling: roi-pooling有两个地方发生精度损失，一个找出感兴趣区域后经过32倍下采样取整的精度损失，如上图的rpn阶段找出一个665*665的框，映射到下采样过的特征图上变成20*20，另一个地方是对roi区域变成7*7对时候采取的一个取整，先将roi变成49个小区域，每个小区域都是取整过的，有精度损失 roi-align: roi-align有针对上述两处取整做了改进，第一次rpn出来的框不取整，带上小数，直接对带小数的roi区域划分成7*7的网格，这时候这个7*7会把特征图不完整的分割，解决办法是采用插值的方式 mask分支 假如mask分支输出K*m*n大小的特征图，其中K是物体类别可能的数量，m和n是特征的宽和高，一共有K个通道，每个通道都是sigmoid后的二值mask，代表是该类别的概率，如果是概率会接近1 公共特征出来后进入mask分支，对整图做上采样，采样到和输入一样大大小的宽和高，一共K个通道，然后对每个像素对每个类别做二值推理，判断其是否是某个类别]]></content>
  </entry>
  <entry>
    <title><![CDATA[pytorch模型可视化]]></title>
    <url>%2F2020%2F08%2F22%2Fpytorch%E5%8F%AF%E8%A7%86%E5%8C%96%2F</url>
    <content type="text"><![CDATA[环境安装graphviz, pytorchviz 12pip install graphvizpip install git+https://github.com/szagoruyko/pytorchviz 示例： 123456789101112import torchfrom torch import nnfrom torchviz import make_dotfrom torchvision.models import AlexNetmodel = AlexNet()x = torch.randn(1, 3, 227, 227).requires_grad_(True)y = model(x)vis_graph = make_dot(y, params=dict(list(model.named_parameters()) + [(&apos;x&apos;, x)]))vis_graph.view() 使用模型查看工具]]></content>
  </entry>
  <entry>
    <title><![CDATA[TensorRT-CenterNet-姿态估计]]></title>
    <url>%2F2020%2F08%2F22%2FTensorRT-CenterNet-MultiPose%2F</url>
    <content type="text"><![CDATA[源码https://github.com/qingzhouzhen/CenterNet，fork自https://github.com/xingyizhou/CenterNet， 2019/10/10 0.4.1```1234#### 训练模型按照&lt;https://github.com/qingzhouzhen/CenterNet/blob/master/experiments/ctdet_coco_dla_1x.sh&gt;，默认使用pose_dla_dcn, 使用修dlav0模型： python main.py multi_pose –arch dlav0_34 –exp_id dla_1x –dataset coco_hp –batch_size 64 –master_batch 16 –lr 5e-4 –load_model ../models/ctdet_coco_dla_2x.pth –gpus 0,1,2,3 –num_workers 16 python main.py multi_pose –arch dlav0_34 –exp_id dla_1x –dataset coco_hp –batch_size 48 –master_batch 16 –lr 5e-4 –load_model ../models/ctdet_coco_dla_2x.pth –gpus 0,1,2 –num_workers 16 –resume12345678之所以用dlav0_34是因为dla中含有```DCN(Deformable Convolutional Networks)```， TensorRT不支持这个层，为了方便使用dlav0训练完后在```path/CenterNet/exp/multi_pose/dla_1x```路径下生成```model_best.pth```和```model_last.pth```两个模型#### 测试使用作者提供的demo.py脚本进行测试 multi_pose –arch dlav0_34 –demo /data0/hhq/project/CenterNet/images/ –load_model ../exp/multi_pose/dla_1x/model_best_10_10.pth –gpus 312#### 转onnx multi_pose –arch dlav0trans_34 –demo /data0/hhq/project/CenterNet/images/ –load_model ../exp/multi_pose/dla_1x/model_best_10_10.pth –gpu 312修改```dlav0.py DLASeg```类的```forward```函数，修改成，这样返回的就不是3个heatmap，而是4个，最后一个是第一个maxpool之后的结果 def forward(self, x): x = self.base(x) x = self.dla_up(x[self.first_level:]) # x = self.fc(x) # y = self.softmax(self.up(x)) ret = [] for head in self.heads: ret.append(self.__getattr__(head)(x)) sigmoid = ret[0].sigmoid_() hmax = nn.functional.max_pool2d( sigmoid, (3, 3), stride=1, padding=1) ret[0] = hmax ret.append(sigmoid) return [ret] 123456把返回的字典修改成列表，因为TensorRT不支持字典操作；将heatmap做sigmoid和maxpool,为了方便做nms。转换代码参考:dla2onnx.py import _init_paths import osimport cv2 from opts import optsfrom detectors.detector_factory import detector_factoryfrom torch.autograd import Variableimport torch image_ext = [‘jpg’, ‘jpeg’, ‘png’, ‘webp’]video_ext = [‘mp4’, ‘mov’, ‘avi’, ‘mkv’]time_stats = [‘tot’, ‘load’, ‘pre’, ‘net’, ‘dec’, ‘post’, ‘merge’] def demo(opt): os.environ[‘CUDA_VISIBLE_DEVICES’] = opt.gpus_str opt.debug = max(opt.debug, 1) Detector = detector_factory[opt.task] detector = Detector(opt) torch_model = detector.model c, h, w = (3, 512, 512) dummy_input = Variable(torch.randn(1, c, h, w, device=’cuda’)) torch.onnx.export(torch_model, dummy_input, “/data0/hhq/project/CenterNet/models/dlav0_opt_80.onnx”, verbose=True, export_params=True) #, operator_export_type=OperatorExportTypes.ONNX_ATEN) if name == ‘main‘: opt = opts().init() demo(opt)` 转完后生成.onnx结尾的文件 onnx转TensorRTTensorRT-5.1.2 参考：http://code-cbu.huawei.com/EI-VisonComputing/AlgorithmGroup/ServiceSDK/ConvertTensorRT/alphapose_convert_tensorrt 加载RT模型并推理自己写的multipose_decode]]></content>
  </entry>
  <entry>
    <title><![CDATA[线性svm推理inference]]></title>
    <url>%2F2020%2F08%2F22%2F%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E7%90%86%E8%A7%A3svm%2F</url>
    <content type="text"><![CDATA[svm.SVC()svm.SVC()使用ovo，每一个类别和其他类别分别构造一个svm，一共n*(n-1)/2svm分类器，当判断一个未知类别的样本，每个分类器对这个点进行分类，最后投票，得票最多的类别为最终类别 构造四个类别： 1234cls1 = np.array([[0, 1, 0],[1, 1, 0], [2, 1, 0], [3, 1, 0], [4, 1, 0]])cls2 = np.array([[0, 3, 1],[1, 3, 1], [2, 3, 1], [3, 3, 1], [4, 3, 1]])cls3 = np.array([[0, 5, 2],[1, 5, 2], [2, 5, 2], [3, 5, 2], [4, 5, 2]])cls4 = np.array([[0, 7, 3],[1, 7, 3], [2, 7, 3], [3, 7, 3], [4, 7, 3]]) 分别为A,B,C,D类，AB构造一个分类器y=2x, AC构造一个分类器y=3x, AD构造一个分类器y=4x, BC构造一个分类器y=4x, BD构造一个分类器y=5x, CD构造一个分类器y=6x 当来一个点时，计算它到每个分类平面的距离，如果大于0则分为正类，否则负类，如点(0,1)到这两个平面距离都大于0，得到[Ture,True,True,Ture,True,True],AB分类，A=1；AC分类，A=2；AD分类，A=3；BC分类，B=1；BD分类，B=2；CD分类，C=1；最后A类得分最多，该点被分为A类 sklearn ovo源码： 123456789# multiclass.pyk = 0for i in range(n_classes): for j in range(i + 1, n_classes): sum_of_confidences[:, i] -= confidences[:, k] sum_of_confidences[:, j] += confidences[:, k] votes[predictions[:, k] == 0, i] += 1 votes[predictions[:, k] == 1, j] += 1 k += 1 svm.SVC()有参数 decision_function_shape=’ovr’不过没用，还是采用的ovo的方式 svm.LinearSVC()svm.LinearSVC 使用ovr进行多分类，每个类别和剩下的类别训练得到一个svm分类器，分类的时候，对于一个未知的点，计算该点到所有超平面的距离(没有除模)，最后改样本被分为距离超平面最远的那一个类，之所以不除模，是因为这个超平面可以将绝对距离远的分为正类，而将绝对距离近的超平面分为负类，下面看一个例子，还是上面那四个点， 训练得到的参数w&amp;b: 12345[[ 0.07355282 -0.82129448] [-0.00218351 -0.10126131] [-0.01921047 0.08401843] [-0.09088882 0.39134251]][ 1.42011892 -0.08837931 -0.77819071 -2.07359502] 测试点(0,1)到四个超平面的距离0.59882445 -0.18964062 -0.69417228 -1.68225251]]```12最大的位置在0，所以该点被分为0类；测试点(0,2)到四个超平面的距离```[[-1.04376451 -0.39216323 -0.52613543 -0.89956749]] 之所以该点到第二个超平面还更‘近’，是因为它的距离没有除以模 sklearn源码： 1234# path/linear_model/base.pyscores = safe_sparse_dot(X, self.coef_.T, dense_output=True) + self.intercept_return scores.ravel() if scores.shape[1] == 1 else scores 测试样例代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import numpy as npfrom matplotlib import pyplot as pltfrom sklearn.svm import LinearSVCfrom sklearn import svmif __name__ == &apos;__main__&apos;: cls1 = np.array([[0, 1, 0],[1, 1, 0], [2, 1, 0], [3, 1, 0], [4, 1, 0]]) cls2 = np.array([[0, 3, 1],[1, 3, 1], [2, 3, 1], [3, 3, 1], [4, 3, 1]]) cls3 = np.array([[0, 5, 2],[1, 5, 2], [2, 5, 2], [3, 5, 2], [4, 5, 2]]) cls4 = np.array([[0, 7, 3],[1, 7, 3], [2, 7, 3], [3, 7, 3], [4, 7, 3]]) data = np.vstack((cls1, cls2, cls3, cls4)) X = data[:,0:2] Y = data[:, -1] clf = LinearSVC(max_iter=10000) #clf = svm.SVC(kernel=&apos;linear&apos;,decision_function_shape=&apos;ovr&apos;) clf.fit(X, Y) w = clf.coef_ b = clf.intercept_ print(w) print(b) ax = plt.subplot() ax.scatter(cls1[:, 0], cls1[:, 1], label=cls1[:, 2]) ax.scatter(cls2[:, 0], cls2[:, 1], label=cls2[:, 2]) ax.scatter(cls3[:, 0], cls3[:, 1], label=cls3[:, 2]) ax.scatter(cls4[:, 0], cls4[:, 1], label=cls4[:, 2]) point1 = np.array([[0,1]]) point2 = np.array([[0,3]]) point3 = np.array([[0,5]]) point4 = np.array([[0,7]]) scores1 = clf.decision_function(point1) scores2 = clf.decision_function(point2) scores3 = clf.decision_function(point3) scores4 = clf.decision_function(point4) clf.predict(point1) print(&apos;scores 1:&apos;, scores1, &quot;cls:&quot;, clf.predict(point1)) print(&apos;scores 2:&apos;, scores2, &quot;cls:&quot;, clf.predict(point2)) print(&apos;scores 3:&apos;, scores3, &quot;cls:&quot;, clf.predict(point3)) print(&apos;scores 4:&apos;, scores4, &quot;cls:&quot;, clf.predict(point4)) # 超平面 x = np.linspace(0,5) #x = np.meshgrid(x) y1 = (w[0, 0]*x+b[0])/(-w[0, 1]) y2 = (w[1, 0]*x+b[1])/(-w[1, 1]) y3 = (w[2, 0]*x+b[2])/(-w[2, 1]) y4 = (w[3, 0]*x+b[3])/(-w[3, 1]) ax.plot(x, y1) ax.plot(x, y2) ax.plot(x, y3) ax.plot(x, y4) plt.xlim([0,30]) plt.show()]]></content>
  </entry>
  <entry>
    <title><![CDATA[分类，检测，语义分割，实例分割的区别]]></title>
    <url>%2F2020%2F08%2F22%2F%E5%88%86%E7%B1%BB%EF%BC%8C%E6%A3%80%E6%B5%8B%EF%BC%8C%E8%AF%AD%E4%B9%89%E5%88%86%E5%89%B2%EF%BC%8C%E5%AE%9E%E4%BE%8B%E5%88%86%E5%89%B2%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[分类，检测，语义分割，实例分割的区别 语义分割(semantic segmentation)就是把图像中每一个像素属于什么类别(比如person)都识别出来，但是它没有告诉这个像素属于第几个人，而实例分割(instance segmentation)不光把一个像素属于那个类别识别出来，还识别出它属于该类别中的第几个实例 FCN和Alexnet的区别在于，后者对一张图片做一次分类，而前者对每一个像素点都做一次分类]]></content>
  </entry>
  <entry>
    <title><![CDATA[协同过滤]]></title>
    <url>%2F2020%2F08%2F22%2F%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%2F</url>
    <content type="text"><![CDATA[基于物品的协同过滤总结起来就是说如果用户A购买力物品1和物品2， 那么认为物品1和物品2是相似物品，在这个前提下，如果用户B购买了物品1，就把物品2推荐给B 基于用户的协同过滤总结起来就是如果用户AB都购买了物品xyz，那么认为用户A和用户B是同一类用户，如果A购买了w，也把w推荐给B 欧几里得距离：像几何一样计算两个变量之间的距离 皮尔逊相关系数：两个变量XY之间的相关度，定义为两个变量之间的协方差和标准差之间的尚 参考 https://blog.csdn.net/yimingsilence/article/details/54934302]]></content>
  </entry>
  <entry>
    <title><![CDATA[CenterNet训练自己的数据]]></title>
    <url>%2F2020%2F08%2F22%2FCenterNet%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E7%9A%84%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[准备数据 pascal voc格式数据转coco格式 参考： https://blog.csdn.net/weixin_41765699/article/details/100124689 在12343. 在```path/src/lib/datasets/datasets```下新建```gaodian.py```，内容参照```coco.py```改，改成自己需求的样子 1. 修改类名为```Gaodian 14行1234563. 改均值和方差，参考 https://blog.csdn.net/weixin_41765699/article/details/100118660 4. 修改数据和图片的路径，```data_dir```到数据集名称文件夹，```img_dir```放图片的位置5. 修改标签文件```json```文件，我改了如下，这样train和val都读2036的文件 else: self.annot_path = os.path.join( self.data_dir, &apos;annotations&apos;, &apos;2036.json&apos;) 126. 修改```path/src/lib/datasets/dataset_factory.py 1234567dataset_factory = &#123; &apos;coco&apos;: COCO, &apos;pascal&apos;: PascalVOC, &apos;kitti&apos;: KITTI, &apos;coco_hp&apos;: COCOHP, &apos;gaodian&apos;: Gaodian&#125; 修改12 将默认的数据集名称修改为```gaodian 12self.parser.add_argument(&apos;--dataset&apos;, default=&apos;gaodian&apos;, help=&apos;coco | kitti | coco_hp | pascal&apos;) 将默认数据集的均值方差修改 12345def init(self, args=&apos;&apos;): default_dataset_info = &#123; &apos;ctdet&apos;: &#123;&apos;default_resolution&apos;: [512, 512], &apos;num_classes&apos;: 80, &apos;mean&apos;: [0.408, 0.447, 0.470], &apos;std&apos;: [0.289, 0.274, 0.278], &apos;dataset&apos;: &apos;coco&apos;&#125;, 修改1 elif num_classes == 80 or dataset == &apos;coco&apos;: self.names = coco_class_name elif num_classes == 3 or dataset == &apos;gaodian&apos;: self.names = gaodian_class_name elif num_classes == 20 or dataset == &apos;pascal&apos;: 1 gaodian_class_name = [ &apos;person&apos;, &apos;vehicle&apos;, &apos;none-motor&apos;] 12 #### 训练 python main.py ctdet –arch dlav0_34 –exp_id coco_dla_1x –batch_size 64 –master_batch 16 –lr 5e-4 –gpus 0,1,2,3 –num_workers 16`]]></content>
  </entry>
  <entry>
    <title><![CDATA[矩阵分解]]></title>
    <url>%2F2020%2F08%2F22%2F%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[PCA(principal component analysis)主成分分析，原始矩阵中的每一个item可以用基向量的线性组合表示 SVD(singular value decomposition)奇异值矩阵分解，是一个矩阵及其转置矩阵的PCA， $R=M\sum U$ SVD能通过矩阵分解能同时拿到行向量与列向量的基 在电影推荐中，R的行代表某个用户，列代表某个电影，具体某个数值代表该用户对某个电影的评分，M的行代表某个用户对所有类型电影的喜好程度，U的列代表某个电影是否属于某种类型的电影，两者点乘为某个用户对某个电影的评分 该算法的目标是：基于稀疏的矩阵R，填充那些缺失的值 rui = pu * qi 我们要最小化sum(rui - pu * pi)^2,意思就是使用那些已有的矩阵R中的值去估计pu和pi，虽然不是无偏低，但是也是可以用的 具体参考：http://www.infoq.com/cn/articles/matrix-decomposition-of-recommend-system]]></content>
  </entry>
  <entry>
    <title><![CDATA[darknet框架使用]]></title>
    <url>%2F2020%2F08%2F22%2Fdarknet%E8%AE%AD%E7%BB%83%E8%87%AA%E5%B7%B1%E7%9A%84yolov3%2F</url>
    <content type="text"><![CDATA[编译修改Makefile 123GPU=1CUDNN=1OPENCV=1 1234567python3额外添加LINKFLAGS := -Wl,-rpath,/home/hhq/anaconda3/lib$(EXEC): $(EXECOBJ) $(ALIB) $(CC) $(COMMON) $(CFLAGS) $^ -o $@ $(LDFLAGS) $(LINKFLAGS) $(ALIB)$(SLIB): $(OBJS) $(CC) $(CFLAGS) -shared $^ -o $@ $(LDFLAGS) $(LINKFLAGS) make -j24 处理数据在 1/data0/hhq/data 下执行 1python voc_label.py 从xml标注文件生成txt标注文件 custom数据训练 修改voc.data 12345classes= 1train = /data0/hhq/project/darknet-master/mydata/2031_train.txtvalid = /data0/hhq/project/darknet-master/mydata/2031_trainval.txtnames = /data2/hhq/project/darknet-master/mydata/voc.namesbackup = /data2/hhq/project/darknet-master/mydata/model/ 修改voc.names light 修改yolov3-voc.cfg 修改classes，yolo上面一层卷积层的filter数量n=anchors*(1+4+1) 训练 1./darknet detector train ./mydata/voc.data ./mydata/yolov3-voc.cfg ./mydata/yolov3.weights -clear 0 -gpus 0,1 | tee train-0322-01.log 输出 batch 输出，如12705: 0.256951, 0.259272 avg, 0.000040 rate, 34.392978 seconds, 692480 images 2706迭代次数，0.256951总损失loss，0.259272 avg平均损失，0.000040当前学习率，34当前批次数据训练所花时间，692480=64*2705 详细输出，如 123Region 82 Avg IOU: -nan, Class: -nan, Obj: -nan, No Obj: 0.000002, .5R: -nan, .75R: -nan, count: 0Region 94 Avg IOU: -nan, Class: -nan, Obj: -nan, No Obj: 0.000010, .5R: -nan, .75R: -nan, count: 0Region 106 Avg IOU: 0.851382, Class: 0.999875, Obj: 0.997986, No Obj: 0.000242, .5R: 1.000000, .75R: 0.846154, count: 13 Region Avg IOU: 0.32657, 表示在当前 subdivision 内的图片的平均 IOU, 代表预测的矩形框和真实目标的交集与并集之比, 这里是 75.96%, 这个模型此时已经达到了很高的训练精度; Class: 0.809470, 标注物体分类的正确率, 期望该值趋近于1; Obj: 0.732717, 越接近 1 越好; No Obj: 0.002799, 期望该值越来越小, 但不为零; .5R: 1.000000, 是在 recall/count 中定义的, 是当前模型在所有 subdivision 图片中检测出的正样本与实际的正样本的比值。在本例中, 全部的正样本被正确的检测到。 count: 4, 所有当前 subdivision 图片（本例中一共 8 张）中包含正样本的图片的数量。 在输出 log 中的其他行中, 可以看到其他 subdivision 也有的只含有 6 或 1 个正样本, 说明在 subdivision 中含有不包含在检测对象 classes 中的图片。]]></content>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归]]></title>
    <url>%2F2020%2F08%2F22%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[逻辑回归先从线形回归引入，即通过一些数据去拟合一个函数$y=wx +b$,再来一个新的数据$x_i$，可以通过这个函数(模型)得到它对应的输出 通过均方误差求解函数参数 $E_{(w,b)}=min \sum_i^m(y_i-wx_i-b)^2$ 上式子分别对w,b求导并令导数为0解出 $w=\frac {\sum_i^my_i(x_i-\hat x)}{\sum_i^mx_i^2-\frac{1}{m}(\sum_i^mx_i)^2}$ $b=\frac{1}{m}\sum_1^m(y_i-wx_i)$ 将上述的线形模型整体输入sigmod函数就可以得到逻辑回归 即将$y=wx+b$整体输入进$y=\frac{1}{1+e^{-z}}$,得到一个0-1之间的概率值 对每个输入x，通过逻辑回归都会得到一个概率值，代表它是正类的概率或者是负类的概率是多少，得到 $p(y=1|x)=\frac{e^{w^Tx+b}}{1+e^{w^Tx+b}}$ $p(y=0|x)=\frac{1}{1+e^{w^Tx+b}}$ 逻辑回归的模型代表了什么样的输入就会得到什么样概率的输出，那如何得到模型的参数，有了输入，它所对应的分类—比如是0还是1，目标定为使得模型输出为label的概率最大 所以先定义个似然函数： $l(w,b)=\sum_i^mlnp(y_i|x_i;w,b)$—&gt;输入$x_i$，得到$y_i$的概率要尽可能的大 在二分类中 $$p(y_i|x_i;w,b)=y_ip_1(x_i;w,b)+(1-y_i)p_0(x_i;w,b)$$ 所以将上式代入loss函数中，得到最终的目标函数： $$l(w,b)=min \sum_i^m(-y_i(wx_i+b)+ln((1+e^{wx_i+b})))$$ 用梯度下降法等解决凸优化等可以解得w,b $z=w^Tx+b$ $\hat{y}=a=\sigma(z)$ $L(a,y)=-(ylog(a)+(1-y)log(1-a))$ 参考吴恩达：https://mooc.study.163.com/learn/2001281002?tid=2001392029#/learn/content?type=detail&amp;id=2001702009]]></content>
  </entry>
  <entry>
    <title><![CDATA[just my socks飞机场笔记]]></title>
    <url>%2F2020%2F08%2F22%2Fjust-my-socks%E9%A3%9E%E6%9C%BA%E5%9C%BA%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[购买飞机场购买链接：https://justmysocks2.net/members/cart.php?gid=1 我买了最便宜的一个月的，先试试能不能用。 购买完之后点my service可以看到5个可用的站点，用户名密码等等 下载客户端在官网support-&gt;Knowledgebase-&gt;Software-download-links https://justmysocks2.net/members/index.php?rp=/knowledgebase/5/Software-download-links.html 下载对应平台的客户端，我测试了mac和andorid的 配置客户端mac在网页上找到购买的飞机场-&gt;找到飞机场的ip,端口(每个可用ip都有一个二维码)-&gt;打开上一步下载的小飞机-&gt;扫面屏幕二维码-&gt;打开shadowsocks 校验： 小飞机-&gt;服务器-&gt;服务器设置，检查密码，ip(域名)是否正确 选择一个ip(域名)上网 android下载客户端，安装 打开客户端，手动设置，填写ip(域名)，端口，密码，加密方式等 遇到的问题 由于本机修改了hosts，导致无法上网，设置默认hosts文件 扫二维码出来的加密方式不对，需要手动修改，坑 参考：https://jmsyhw.com/246.html]]></content>
  </entry>
  <entry>
    <title><![CDATA[genymotion安装]]></title>
    <url>%2F2020%2F08%2F22%2Fgenymotion%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[环境Mac OS 10.14，vituralbox6.0 下载安装个人免费版，地址https://www.genymotion.com/get-full-version/ 一路next安装完毕，启动一个Android模拟器 配置x86转arm下载Genymotion-ARM-Translation-Librarities工具转换包:https://pan.baidu.com/s/1nsm-Tn0jkx8JCEJL7ItZpA 将下载好的转换包直接拖到Android模拟器界面上，点击ok然后重启模拟器，最后将需要安装的apk包拖到模拟器上安装]]></content>
  </entry>
  <entry>
    <title><![CDATA[yolov3解析]]></title>
    <url>%2F2020%2F08%2F22%2Fyolov3%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[yolo系列论文看过，源码包调过，抽点时间把论文理解和源码做个一一对应，加深理解，论文 12源码看的mxnet,gluon-cv,代码地址：```https://github.com/dmlc/gluon-cv yolov3 networkdarknet53一共53层卷积，除去最后一个FC总共52个卷积用于当做主体网络，主体网络被分成三个stage，结构类似FPN，1-26层卷积为stage1,27-43层卷积为stage2,44-52层卷积为stage3，低层卷积(26)感受野更小，负责检测小目标，深层卷积(52)感受野大，容易检测出大目标，整体网络的graph如下 边框预测$$b_x = \rho(t_x)+c_x \ b_y = \rho(t_y)+c_y\ b_w = p_we^t_w\ b_h = p_he^t_h$$ 预测的中心坐标是相对于左上角的一个0-1之间的一个值，$p_w, p_h$是anchor的高宽，在anchor的基础上乘以一个比例。 loss$\lambda {coord} \sum{i=0}^{s^2} \sum_{j=0}^{B} \pmb {1}_{ij}^{obj} [(x_i- \hat x_i)^2 + (y_i -\hat {y_i})^2]$+$\lambda {coord} \sum{i=0}^{s^2} \sum_{j=0}^{B} \pmb {1}_{ij}^{obj} [(\sqrt{w_i}- \sqrt{\hat w_i)^2} + (\sqrt {h_i} - \sqrt{\hat {h_i}})^2]$+$\sum_{i=0}^{s^2} \sum_{j=0}^{B} \pmb {1}_{ij}^{obj} (C_i- \hat C_i)^2 + \lambda_{coord}\sum_{i=0}^{s^2} \sum_{j=0}^{B} \pmb {1}_{ij}^{noobj} (C_i- \hat C_i)^2$+$\sum_i^{s2} \pmb {1}i^{obj} \sum{c} (p_i(c)-\hat{p}_i(c))^2$整体思路为：每个cell的每个anchor和label做loss，根据label会有一个mask，中心点，scale有物体的cell，anchor才有loss，其他位置被0mask值忽略，每个cell,anchor有没有物体的置信度都都被用来做loss，有物体的cell才会做分类loss，依次对应上面的数学公式；针对某个cell，某个类被预测，则为1，该cell如果有物体，那这个位置肯定为1 如果某个cell的某个anchor不负责检测某个ground truth，那只有置信度的loss，位置，高宽，分类loss全部没有 anchors1anchors = [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], 这个应该是相对于416的宽和高，相对于416，320，608训练的时候是等比例调整得，同样等比例调整了的应该还有label值 yolov3的anchor和faster-rcnn的 anchor有如下区别 yolov3的anchor是聚类生成的，faster-rcnn是三个尺度，单个大小固定生成的 预测的时候yolov3多坐标是相对于目标像素左上角的，faster-rcnn是相对于全图的 yolov3是三个尺度每个尺度分配3个anchor，faster-rcnn是每个像素分配9个anchor yolov3与yolov2的区别一个区别是v3采用fpn的多尺度，增加检测的准确性，另一个是分类网络没有使用soft而是针对每个类别做一个二分类，20类的pascal voc就有20个二分类器 yolov3 summary123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112layer filters size input output0 0 conv 32 3 x 3 / 1 416 x 416 x 3 -&gt; 416 x 416 x 32 0.299 BFLOPs1 1 conv 64 3 x 3 / 2 416 x 416 x 32 -&gt; 208 x 208 x 64 1.595 BFLOPs2 2 conv 32 1 x 1 / 1 208 x 208 x 64 -&gt; 208 x 208 x 32 0.177 BFLOPs3 3 conv 64 3 x 3 / 1 208 x 208 x 32 -&gt; 208 x 208 x 64 1.595 BFLOPs 4 res 1 208 x 208 x 64 -&gt; 208 x 208 x 644 5 conv 128 3 x 3 / 2 208 x 208 x 64 -&gt; 104 x 104 x 128 1.595 BFLOPs5 6 conv 64 1 x 1 / 1 104 x 104 x 128 -&gt; 104 x 104 x 64 0.177 BFLOPs6 7 conv 128 3 x 3 / 1 104 x 104 x 64 -&gt; 104 x 104 x 128 1.595 BFLOPs 8 res 5 104 x 104 x 128 -&gt; 104 x 104 x 1287 9 conv 64 1 x 1 / 1 104 x 104 x 128 -&gt; 104 x 104 x 64 0.177 BFLOPs8 10 conv 128 3 x 3 / 1 104 x 104 x 64 -&gt; 104 x 104 x 128 1.595 BFLOPs 11 res 8 104 x 104 x 128 -&gt; 104 x 104 x 1289 12 conv 256 3 x 3 / 2 104 x 104 x 128 -&gt; 52 x 52 x 256 1.595 BFLOPs10 13 conv 128 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BFLOPs11 14 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFLOPs 15 res 12 52 x 52 x 256 -&gt; 52 x 52 x 25612 16 conv 128 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BFLOPs13 17 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFLOPs 18 res 15 52 x 52 x 256 -&gt; 52 x 52 x 25614 19 conv 128 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BFLOPs15 20 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFLOPs 21 res 18 52 x 52 x 256 -&gt; 52 x 52 x 25616 22 conv 128 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BFLOPs17 23 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFLOPs 24 res 21 52 x 52 x 256 -&gt; 52 x 52 x 25618 25 conv 128 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BFLOPs19 26 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFLOPs 27 res 24 52 x 52 x 256 -&gt; 52 x 52 x 25620 28 conv 128 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BFLOPs21 29 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFLOPs 30 res 27 52 x 52 x 256 -&gt; 52 x 52 x 25622 31 conv 128 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BFLOPs23 32 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFLOPs 33 res 30 52 x 52 x 256 -&gt; 52 x 52 x 25624 34 conv 128 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BFLOPs25 35 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFLOPs 36 res 33 52 x 52 x 256 -&gt; 52 x 52 x 256fpn1---------------------------------------------------------26 37 conv 512 3 x 3 / 2 52 x 52 x 256 -&gt; 26 x 26 x 512 1.595 BFLOPs27 38 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 0.177 BFLOPs28 39 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFLOPs 40 res 37 26 x 26 x 512 -&gt; 26 x 26 x 51229 41 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 0.177 BFLOPs30 42 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFLOPs 43 res 40 26 x 26 x 512 -&gt; 26 x 26 x 51231 44 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 0.177 BFLOPs32 45 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFLOPs 46 res 43 26 x 26 x 512 -&gt; 26 x 26 x 51233 47 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 0.177 BFLOPs34 48 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFLOPs 49 res 46 26 x 26 x 512 -&gt; 26 x 26 x 51235 50 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 0.177 BFLOPs36 51 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFLOPs 52 res 49 26 x 26 x 512 -&gt; 26 x 26 x 51237 53 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 0.177 BFLOPs38 54 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFLOPs 55 res 52 26 x 26 x 512 -&gt; 26 x 26 x 51239 56 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 0.177 BFLOPs40 57 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFLOPs 58 res 55 26 x 26 x 512 -&gt; 26 x 26 x 51241 59 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 0.177 BFLOPs42 60 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFLOPs 61 res 58 26 x 26 x 512 -&gt; 26 x 26 x 512fpn2------------------------------------------------------------43 62 conv 1024 3 x 3 / 2 26 x 26 x 512 -&gt; 13 x 13 x1024 1.595 BFLOPs44 63 conv 512 1 x 1 / 1 13 x 13 x1024 -&gt; 13 x 13 x 512 0.177 BFLOPs45 64 conv 1024 3 x 3 / 1 13 x 13 x 512 -&gt; 13 x 13 x1024 1.595 BFLOPs 65 res 62 13 x 13 x1024 -&gt; 13 x 13 x102446 66 conv 512 1 x 1 / 1 13 x 13 x1024 -&gt; 13 x 13 x 512 0.177 BFLOPs47 67 conv 1024 3 x 3 / 1 13 x 13 x 512 -&gt; 13 x 13 x1024 1.595 BFLOPs 68 res 65 13 x 13 x1024 -&gt; 13 x 13 x102448 69 conv 512 1 x 1 / 1 13 x 13 x1024 -&gt; 13 x 13 x 512 0.177 BFLOPs49 70 conv 1024 3 x 3 / 1 13 x 13 x 512 -&gt; 13 x 13 x1024 1.595 BFLOPs 71 res 68 13 x 13 x1024 -&gt; 13 x 13 x102450 72 conv 512 1 x 1 / 1 13 x 13 x1024 -&gt; 13 x 13 x 512 0.177 BFLOPs51 73 conv 1024 3 x 3 / 1 13 x 13 x 512 -&gt; 13 x 13 x1024 1.595 BFLOPs 74 res 71 13 x 13 x1024 -&gt; 13 x 13 x1024fpn3--------------------------------------------------------------- 0 75 conv 512 1 x 1 / 1 13 x 13 x1024 -&gt; 13 x 13 x 512 0.177 BFLOPs1 76 conv 1024 3 x 3 / 1 13 x 13 x 512 -&gt; 13 x 13 x1024 1.595 BFLOPs2 77 conv 512 1 x 1 / 1 13 x 13 x1024 -&gt; 13 x 13 x 512 0.177 BFLOPs3 78 conv 1024 3 x 3 / 1 13 x 13 x 512 -&gt; 13 x 13 x1024 1.595 BFLOPs4 79 conv 512 1 x 1 / 1 13 x 13 x1024 -&gt; 13 x 13 x 512 0.177 BFLOPs5 80 conv 1024 3 x 3 / 1 13 x 13 x 512 -&gt; 13 x 13 x1024 1.595 BFLOPs58 81 conv 75 1 x 1 / 1 13 x 13 x1024 -&gt; 13 x 13 x 75 0.026 BFLOPs 82 yolo 83 route 7959 84 conv 256 1 x 1 / 1 13 x 13 x 512 -&gt; 13 x 13 x 256 0.044 BFLOPs 85 upsample 2x 13 x 13 x 256 -&gt; 26 x 26 x 256 86 route 85 6160 87 conv 256 1 x 1 / 1 26 x 26 x 768 -&gt; 26 x 26 x 256 0.266 BFLOPs88 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFLOPs89 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 0.177 BFLOPs90 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFLOPs91 conv 256 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 256 0.177 BFLOPs92 conv 512 3 x 3 / 1 26 x 26 x 256 -&gt; 26 x 26 x 512 1.595 BFLOPs93 conv 75 1 x 1 / 1 26 x 26 x 512 -&gt; 26 x 26 x 75 0.052 BFLOPs94 yolo95 route 9196 conv 128 1 x 1 / 1 26 x 26 x 256 -&gt; 26 x 26 x 128 0.044 BFLOPs97 upsample 2x 26 x 26 x 128 -&gt; 52 x 52 x 12898 route 97 3699 conv 128 1 x 1 / 1 52 x 52 x 384 -&gt; 52 x 52 x 128 0.266 BFLOPs100 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFLOPs101 conv 128 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BFLOPs102 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFLOPs103 conv 128 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 128 0.177 BFLOPs104 conv 256 3 x 3 / 1 52 x 52 x 128 -&gt; 52 x 52 x 256 1.595 BFLOPs105 conv 75 1 x 1 / 1 52 x 52 x 256 -&gt; 52 x 52 x 75 0.104 BFLOPs106 yolo--------------------- YOLOOutputV3从tip卷积套件relu输出开始，到推理的reshape成detection结束。 针对最后一个尺度：卷积输出24=（1+4+3）3,3是类别，4代表一个box，1代表是否有物体，最后的3=6/2，anchor两个一组，一组分别代表高和宽。yolo3.py-&gt;YOLOOutputV3-&gt;123行-&gt;pred-&gt;24x169 (1+4+3)x3x13x13(置信度+坐标+类别)xanchor数量x高x宽pred-&gt;169x3x8 列代表特征位置，横代表anchor的index，通道分别是置信度，位置，类别raw_box_centers-&gt;169x3x2 每个格子，每个anchor相对的中心点raw_box_scales-&gt;169x3x2 每个格子，每个anchor的伸缩比例objness-&gt;169x3x1 每个格子，每个anchor的置信度class_pred-&gt;169x3x3 每个格子，每个anchor的类别概率box_centers-&gt;169x3x2 每个格子，每个anchor对应box相对原图的中心点,加了offsetbox_scales-&gt;169x3x2 每个格子，每个anchor对应box相对原图高宽,它由raw_box_scales先按元素计算以 e(2.71)为底的幂，再和anchor相乘class_score-&gt;169x3x3 每个格子，每个anchor每个类别的得分乘以置信度，分类与置信度联合做lossbbox-&gt;169x3x4 每个格子，每个anchor对应box的坐标，左上角，右下角offsets-&gt;169x1x2 ,每个网格相对偏移，x(0-&gt;12),y(0-&gt;12),每个网格中心点加上其左上角的相对位置偏移，再乘以stride(32)，坐标中心从相对变为绝对anchor-&gt;[[116,90],[156,198],[373,326]],每个anchor的比例，最后一个尺度(1313)的三个anchor，相对于定义，anchor被颠倒，高纬用于检测大物体，yolo3定义的三组anchors:1如果是训练，返回```bbox(1,507,4),raw_box_centers(1x169x3x2),raw_box_scales(1x169x3x2),bojness(1x169x3x1),clas_pred(1x169x3),anchor(1x1x3x2),offset(1x169x1x2) 针对其他两个尺度针对其他两个尺度分别返回1```bbox(1,8112,4),raw_box_centers(1x2704x3x2),raw_box_scales(1x2704x3x2),bojness(1x2704x3x1),clas_pred(1x2704x3),anchor(1x1x3x2),offset(1x2704x1x2) 训练的时候前向计算的(1x507x1) con (1x2028x1) con (1x8112x1)->(1x10647x1)```1```all_box_centers,(1x507x2) con (1x2028x2) con (1x8112x1)-&gt;(1x10647x2) con (1x2028x2) con (1x8112x2)->(1x10647x2)```1```all_class_pred (1x507x3) con (1x2028x3) con (1x8112x3)-&gt;(1x10647x3) 与构造好的label做loss更新参数，所有的cell长宽以及anchor数量糅合到一维 YOLODetectionBlockV3接在特征提取后面，介于特征提取和输出pred之间，用作特征转换，降维等,源码在yolo3.py，类名YOLODetectionBlockV3,每一个stage之后都接一个YOLODetectionBlock,channel设置为[512,256,128],所以每个YOLODetectionBlock最后输出的通道数依次减少，[512,1024,512,1024,512,1024],[256,512,256,512,256,512], [128,256,128,256,128,256]，每一组一个6个卷积，最后一个卷积的输出(tip)进入output用于检测，第5个卷积的输出进入transitions层后和对应的stage concate后进入下一个YOLODetectionBlockV3。&lt;img src=”/Users/alpha/Documents/pic/yolodetectionblock.png”, width=”400px”, height=”200px”/&gt;YOLODetectionBlockV3之间transition,就一个卷积，卷积后分别在特征图高和宽的维度各做一次repeat使得上采样，然后做一次slice_like使得YOLODetectionBlockV3的输出和route的一模一样以便concate 参考https://www.jianshu.com/p/d13ae1055302]]></content>
  </entry>
  <entry>
    <title><![CDATA[rapidjson使用]]></title>
    <url>%2F2020%2F08%2F22%2Frapidjson%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[官方文档：http://rapidjson.org/zh-cn/index.html 纯头文件，不需要安装，拷贝rapidjson到项目，然后include它 读取include 1234#include &quot;lib/rapidjson/document.h&quot;#include &quot;lib/rapidjson/writer.h&quot;#include &quot;lib/rapidjson/stringbuffer.h&quot;#include &quot;lib/rapidjson/filereadstream.h&quot; 读取文件并解析 1234567891011FILE* fp = fopen(config_file.c_str(), &quot;r&quot;); char readBuffer[65536]; rapidjson::FileReadStream is(fp, readBuffer, sizeof(readBuffer)); rapidjson::Document d; d.ParseStream(is); rapidjson::Value&amp; s = d[cameraID.c_str()]; list&lt;std::array&lt;int, 4&gt;&gt; boxesList; for (auto&amp; v : s.GetArray()) &#123; std::array&lt;int, 4&gt; boxArray; string box = v.GetString(); &#125; 写入123456789101112131415161718192021rapidjson::StringBuffer s;rapidjson::Writer&lt;rapidjson::StringBuffer&gt; writer(s);writer.StartObject();writer.Key(&quot;detID&quot;);writer.String(m_videoInfo.stream_id.c_str());writer.Key(&quot;vehicleInfor&quot;);writer.StartArray();for (auto box:boxes)&#123; writer.StartObject(); writer.Key(&quot;x1&quot;); writer.Double(box.x); writer.Key(&quot;y1&quot;); writer.Double(box.y); writer.Key(&quot;x2&quot;); writer.Double(box.x + box.w); writer.Key(&quot;y2&quot;); writer.Double(box.y + box.h); writer.EndObject();&#125;writer.EndArray();writer.EndObject();]]></content>
  </entry>
  <entry>
    <title><![CDATA[win10安装ubuntu16.04]]></title>
    <url>%2F2020%2F08%2F22%2Fwin10%E5%AE%89%E8%A3%85ubuntu16.04%2F</url>
    <content type="text"><![CDATA[环境win+r 输入msinfo32, 华硕B360-plus Win10 查看bios引导程序是UEFI还是老式Legacßy, UEFI对应的磁盘分区为GPT,Legacy对应的MBR 方法一，win+r 输入msinfo32能直接看到BIOS的模式 方法二，进入磁盘管理，右键磁盘0，点击卷能看到磁盘分区模式是UEFI还是MBR 本文针对新的UEFI模式 下载ubuntu16.04从网易下载：http://mirrors.163.com/ubuntu-releases/16.04/，选择16.04-desktop 默认16.04.6，该版本kernel过高，可以从http://old-releases.ubuntu.com/releases/16.04.1/下载过往版本 从国内下载，https://man.linuxde.net/download/Ubuntu_16_04 制作U盘启动盘 在windows上下载UltralSO，文件-&gt;打开，打开下载好的ios文件 选择u盘 启动-&gt;写入硬盘映像 参考：https://jingyan.baidu.com/article/5225f26b0bb45fe6fa0908bc.html 创建分区我的电脑–管理–磁盘管理–右键–压缩卷–填写压缩大小 关闭win10快速启动和安全启动我只关闭了快速启动，安全启动不知道在哪 安装按F8进入bios，next 安装类型选第三个，其他选项，这样进入系统第一个是ubuntu 选择空闲分区，创建swap和/两个分区，swap和网上说的有点不一样，不是在创建的时候选出来的，而是在类型处选择出来的 安装好之后重启就行了 卸载ubuntu安装easyUEFI,删除ubuntu引导 删除ubuntu的磁盘 参考 https://www.jianshu.com/p/38e6be8efecf https://blog.csdn.net/u013052326/article/details/81545449 https://zhuanlan.zhihu.com/p/64961724 https://blog.csdn.net/tjuyanming/article/details/64929901]]></content>
  </entry>
  <entry>
    <title><![CDATA[MXNet制作rec文件]]></title>
    <url>%2F2020%2F08%2F22%2FMXNet%E5%88%B6%E4%BD%9Crec%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[参考https://www.jianshu.com/p/1c85e4127cea 准备10个文件夹，一个文件夹放对应的类别 cd incubator-mxnet/tools 制作lst文件， python im2rec.py train /data0/hhq/data/light/train --list --recursive 从tran.lst生成rec，python im2rec.py train /data0/hhq/data/light/train --num-thread=4 以上步骤会将所有文件生成train.rec，如果要按照一定比例分成train-val,在im2rec.py加参数 对于val，注意当前目录下不要有val文件夹, im2rec.py val--list --recursive```12- ```python im2rec.py val /data0/hhq/data/light/test --num-thread=4]]></content>
  </entry>
  <entry>
    <title><![CDATA[pascal voc数据转coco]]></title>
    <url>%2F2020%2F08%2F22%2Fpascalvoc%E8%BD%ACcoco%2F</url>
    <content type="text"><![CDATA[准备pascal voc数据 123AnnotationsImageSetsJPEGImages 转的过程中没用到ImagesSets文件夹 转换后输出json文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178import xml.etree.ElementTree as ETimport osimport json coco = dict()coco[&apos;images&apos;] = []coco[&apos;type&apos;] = &apos;instances&apos;coco[&apos;annotations&apos;] = []coco[&apos;categories&apos;] = [] category_set = dict()image_set = set() category_item_id = -1image_id = 20180000000annotation_id = 0 def addCatItem(name): global category_item_id category_item = dict() category_item[&apos;supercategory&apos;] = &apos;none&apos; category_item_id += 1 category_item[&apos;id&apos;] = category_item_id category_item[&apos;name&apos;] = name coco[&apos;categories&apos;].append(category_item) category_set[name] = category_item_id return category_item_id def addImgItem(file_name, size): global image_id if file_name is None: raise Exception(&apos;Could not find filename tag in xml file.&apos;) if size[&apos;width&apos;] is None: raise Exception(&apos;Could not find width tag in xml file.&apos;) if size[&apos;height&apos;] is None: raise Exception(&apos;Could not find height tag in xml file.&apos;) image_id += 1 image_item = dict() image_item[&apos;id&apos;] = image_id image_item[&apos;file_name&apos;] = file_name image_item[&apos;width&apos;] = size[&apos;width&apos;] image_item[&apos;height&apos;] = size[&apos;height&apos;] coco[&apos;images&apos;].append(image_item) image_set.add(file_name) return image_id def addAnnoItem(object_name, image_id, category_id, bbox): global annotation_id annotation_item = dict() annotation_item[&apos;segmentation&apos;] = [] seg = [] # bbox[] is x,y,w,h # left_top seg.append(bbox[0]) seg.append(bbox[1]) # left_bottom seg.append(bbox[0]) seg.append(bbox[1] + bbox[3]) # right_bottom seg.append(bbox[0] + bbox[2]) seg.append(bbox[1] + bbox[3]) # right_top seg.append(bbox[0] + bbox[2]) seg.append(bbox[1]) annotation_item[&apos;segmentation&apos;].append(seg) annotation_item[&apos;area&apos;] = bbox[2] * bbox[3] annotation_item[&apos;iscrowd&apos;] = 0 annotation_item[&apos;ignore&apos;] = 0 annotation_item[&apos;image_id&apos;] = image_id annotation_item[&apos;bbox&apos;] = bbox annotation_item[&apos;category_id&apos;] = category_id annotation_id += 1 annotation_item[&apos;id&apos;] = annotation_id coco[&apos;annotations&apos;].append(annotation_item) def parseXmlFiles(xml_path): for f in os.listdir(xml_path): if not f.endswith(&apos;.xml&apos;): continue bndbox = dict() size = dict() current_image_id = None current_category_id = None file_name = None size[&apos;width&apos;] = None size[&apos;height&apos;] = None size[&apos;depth&apos;] = None xml_file = os.path.join(xml_path, f) print(xml_file) tree = ET.parse(xml_file) root = tree.getroot() if root.tag != &apos;annotation&apos;: raise Exception(&apos;pascal voc xml root element should be annotation, rather than &#123;&#125;&apos;.format(root.tag)) # elem is &lt;folder&gt;, &lt;filename&gt;, &lt;size&gt;, &lt;object&gt; for elem in root: current_parent = elem.tag current_sub = None object_name = None if elem.tag == &apos;folder&apos;: continue if elem.tag == &apos;filename&apos;: file_name = elem.text if file_name in category_set: raise Exception(&apos;file_name duplicated&apos;) # add img item only after parse &lt;size&gt; tag elif current_image_id is None and file_name is not None and size[&apos;width&apos;] is not None: if file_name not in image_set: current_image_id = addImgItem(file_name, size) print(&apos;add image with &#123;&#125; and &#123;&#125;&apos;.format(file_name, size)) else: raise Exception(&apos;duplicated image: &#123;&#125;&apos;.format(file_name)) # subelem is &lt;width&gt;, &lt;height&gt;, &lt;depth&gt;, &lt;name&gt;, &lt;bndbox&gt; for subelem in elem: bndbox[&apos;xmin&apos;] = None bndbox[&apos;xmax&apos;] = None bndbox[&apos;ymin&apos;] = None bndbox[&apos;ymax&apos;] = None current_sub = subelem.tag if current_parent == &apos;object&apos; and subelem.tag == &apos;name&apos;: object_name = subelem.text if object_name not in category_set: current_category_id = addCatItem(object_name) else: current_category_id = category_set[object_name] elif current_parent == &apos;size&apos;: if size[subelem.tag] is not None: raise Exception(&apos;xml structure broken at size tag.&apos;) size[subelem.tag] = int(subelem.text) # option is &lt;xmin&gt;, &lt;ymin&gt;, &lt;xmax&gt;, &lt;ymax&gt;, when subelem is &lt;bndbox&gt; for option in subelem: if current_sub == &apos;bndbox&apos;: if bndbox[option.tag] is not None: raise Exception(&apos;xml structure corrupted at bndbox tag.&apos;) bndbox[option.tag] = int(option.text) # only after parse the &lt;object&gt; tag if bndbox[&apos;xmin&apos;] is not None: if object_name is None: raise Exception(&apos;xml structure broken at bndbox tag&apos;) if current_image_id is None: raise Exception(&apos;xml structure broken at bndbox tag&apos;) if current_category_id is None: raise Exception(&apos;xml structure broken at bndbox tag&apos;) bbox = [] # x bbox.append(bndbox[&apos;xmin&apos;]) # y bbox.append(bndbox[&apos;ymin&apos;]) # w bbox.append(bndbox[&apos;xmax&apos;] - bndbox[&apos;xmin&apos;]) # h bbox.append(bndbox[&apos;ymax&apos;] - bndbox[&apos;ymin&apos;]) print(&apos;add annotation with &#123;&#125;,&#123;&#125;,&#123;&#125;,&#123;&#125;&apos;.format(object_name, current_image_id, current_category_id, bbox)) addAnnoItem(object_name, current_image_id, current_category_id, bbox) if __name__ == &apos;__main__&apos;: xml_path = &apos;Z:\pycharm_projects\ssd\VOCtest60\Annotations&apos; # 这是xml文件所在的地址 json_file = &apos;./test.json&apos; # 这是你要生成的json文件 parseXmlFiles(xml_path) # 只需要改动这两个参数就行了 json.dump(coco, open(json_file, &apos;w&apos;))]]></content>
  </entry>
  <entry>
    <title><![CDATA[anchor-free论文阅读笔记]]></title>
    <url>%2F2020%2F08%2F22%2Fanchor-free%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%2F</url>
    <content type="text"><![CDATA[12345678flowst=&gt;start: Startop1=&gt;operation: DenseBox(2015)op2=&gt;operation: YOLOV1op3=&gt;operation: CornerNetop4=&gt;operation: ExtremeNete=&gt;endop1(right)-&gt;op2(right)-&gt;op3(right)-&gt;op4(right) 12345678flowst=&gt;start: Startop5=&gt;operation: FSAFop6=&gt;operation: FCOSop7=&gt;operation: FoveaBoxop8=&gt;operation: CenterNete=&gt;endop5(right)-&gt;op6(right)-&gt;op7(right)-&gt;op8 DenseBox论文地址：https://arxiv.org/pdf/1509.04874.pdf 系统将整个图片作为输入，每个像素输出5维的feature map， 一个代表该像素是人脸的置信分，另外四个代表该像素到四条边的距离 YOLOV1论文地址：https://arxiv.org/abs/1506.02640 YOLOV1将图片分为SxS的网格，一次前向网络得到SxSx(B*5+C)的张量，每个bbox预测x,y,w,h,c的数值，x,y为相对栅格左上角偏移量，w,h为相对于整张图的相对值 CornerNet论文地址：https://arxiv.org/pdf/1808.01244.pdf 摘要CornerNet是无anchor，one-stage的目标检测算法，通过检测物体的左上角和右下角角点从而得到物体的bbox，CornetNet通过省略anchor等步骤是的算法更简单，但是缺点是更慢 原理CornetNet通过一次前向传播，分别通过backbone, CornerPooling得到Heatmap, Embeddings, Offsets三个组件，通过这三个组件得到目标所在的位置，总体网络结构图如下图,前面是一个二次级联的沙漏网络，进入左下角和右上角的角点预测模块，每个角点预测模块包括CornerPooling, 预测角点位置的Heatmap，角点组bbox的Embeddings, 这两个张量决定哪两个角点是属于同一个bbox的，Offsets用于框的微调，由于第一次下采样，预测值映射回原图量化而出现的精度损失 沙漏网络 输入从128下采样后又上采样到128，这个操作和语义分割几乎一样，CornerNet堆叠了两个HourglassNetwork CornerPooling从backbone出来，针对某个角点张量分成三个分支，计算当前位置值时向右方与下方max的距离取最大值并求和作为该点的最终结果 HeatmapHeatmap的形状为NCHW，C为类别数，每个点上的位置为0-1之间的值，代表这个点是否是该类别的置信度 Embeddings上一步得到左上角和右下角的角点，还需要把属于同一个物体的角点组合起来，本文通过两个Embeddings将他们结合起来。论文通过L1范数距离判断两个角点是否是同一个角点，具体的为通过计算相同位置的向量值的L1，如果两个值的L1距离很小则这两个角点来自同一个物体 Offset$$o_k=(\frac{x_k}{n}-\lfloor\frac{x_k}{n}\rfloor, \frac{y_k}{n}-\lfloor\frac{y_k}{n}\rfloor)$$ 下采样的时候由于取整导致heatmap对应位置映射回原图的时候不是真实位置，这时通过offset来微调 Lossheatmap采用修改过的focal loss 通过$(1-p_{cij})^{\alpha}$控制难易样本的loss权重。这里和标准的focal loss的一个却别是负样本的处理，在ground true的地方$y_{cij}=1$，非gt点理论上$y_{cij}$应该是0，但是这里给了一个高斯处理的非0值，通过$\beta$控制权重 offset采用smoothl1 loss，$L_{offset}=\frac{1}{N}\sum{SmoothL1Loss(o_k, \hat{o}_k)}$，$o_k$为计算出的偏差，$\hat{o}_k$为真实的下采样后精度偏差 embedding loss, 4式使同一个目标的embedding vector距离很小，5式是的不属于同一个目标的距离很大 ExtremeNet论文地址： 摘要提出一种目标检测新思路，首先通过标准的关键点检测检测四个极值点和一个中心点，然后通过几何关系对提取到的关键点进行分组，5个极值点对应一个检测结果 该方法使目标检测转化为一个基于外观信息的关键点估计问题，从而避开区域分类和隐含特征学习 原理 在O(hw)的空间预测5个heatmap，代表该位置是否是某个类别的极左点，极右点，极顶点，极底点，预测的是概率值，代表该位置是否有极值点以及它所属的类别；ExtremeNet通过暴力组合所有的极点来判断哪些点来自于同一个物体，通过四个极值点计算该四个点的物理中心，到center heatmap中找到该点的值，如果该值超过阈值则这是一个有效的组合 lossExtremeNet角点和offset的loss采用了CornerNet的loss，没有组点的过程所有没有那部分的损失，但是角点的损失由2个增加到5个 CenterNet(object as point)论文地址：https://arxiv.org/pdf/1904.07850.pdf 原理原图下采样4倍，针对每一个热点图的像素预测CHW,2HW,2HW三个张量，分别代表该点有某个类别物体中心点的概率，长宽，中心点的偏置 loss中心点损失策略和cornernet处理角点一样，通过GT高斯处理中心点的像素 长宽损失采用L1损失 偏移损失 decode bbox decode heatmap取前100的位置，得分，类别-&gt;找出取前100点对应的reg的回归值-&gt;取出前100对应的wh的值-&gt;中心点的位置加上回归位置加上高宽得到bbox pos decode CenterNet:Keypoint Triplets原文： https://arxiv.org/abs/1904.08189 #### 摘要 本篇文章基于CornerNet，CornerNet基于检测边界值确定角点从而确定bbox，但是作者认为这样系统感受不到框内部的信息，不像两阶段网络那样做ROIPooling，作者提出通过改进pooling的方式来提升检测的效果 原理通过backbone网络卷积出两个分支的featuremap，一个分支代表两个角点，这个和corner一样，另一个代表是中心点。 角点配对通过embedding实现，左上角角点对应的embedding向量和右下角角点对应的embedding向量距离小于某个阈值则判断为同一个框，这个和cornernet一样。 区别cornernet的地方在于，centernet先通过左上角右下角确定一个bbox的中新区域，然后去中新点heatmap的该区域找是否有和左上角右下角构成bbox同一个类别的中心点，如果存在同类别中心点置信度超过阈值，则判断为一个bbox。 center pooling从主干网络出来后的CHW,对某个点做centerpooling如下：从水平(x)和垂直(y)轴分别找到一个最大值，然后把他们加起来。这个和cornernet的cornerpooling的区别在于后者从起始点朝右，下方一个max的距离找最值，而centerpooling在整个轴上找 cascade pooling使用该pooling的目的是为了解决cornerpooling不能感知到bbox内部的信息，所以cascade pooling除了像cornerpooling一样在边界取最大值，还在bbox内部找最大值加起来，覆盖了bbox整一块区域，像瀑布一样 中心区域的选取判断一个bbox是否要保留还是删除，要看该框的中心点在中心点区域的center heatmap是否有同类别且高置信度的特征张量点，计算方式为 ctlx,ctly,cbrx,cbry为计算出来的中心点，tlx,tly,brx,bry为bbox的点，n取3或5 性能对比 network backbone FPS AP GPU Centernet:triplet Hourglass-104 3.3 44.9/47.0 V100 Centernet:keypoint Hourglass-104 7.8 42.1/45.1 Titan-V Extremenet Hourglass-104 3.1 40.2/43.7 Titan-V Cornernet Hourglass-104 4.1 40.5/42.1 Titan-V YOLOv3 DarkNet-53 20 33.0 Titan-V 不明白的地方YOLO、SSD等在空间中设置anchor 参考https://blog.csdn.net/u014380165/article/details/77019084]]></content>
  </entry>
  <entry>
    <title><![CDATA[花生壳内网穿透ssh远程登录]]></title>
    <url>%2F2020%2F08%2F22%2F%E8%8A%B1%E7%94%9F%E5%A3%B3%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8Fssh%E8%BF%9C%E7%A8%8B%E7%99%BB%E5%BD%95%2F</url>
    <content type="text"><![CDATA[注册账号https://console.oray.com/passport/register.html?fromurl=http%3A%2F%2Fhsk.oray.com%2F 注册成功可以买5元一个的壳域名，我没有买 下载客户端https://hsk.oray.com/download/ 选择linux 安装客户端，得到sn码 添加设备https://hsk.oray.com/console/manage/添加设备 映射https://b.oray.com/点击+号图片添加一个新的映射，对于新建ssh服务映射，映射类型选通用应用，外网端口选动态端口号，内网主机选内网主机号(通过ifconfig查看或者查看路由器管理页)，端口号填22 测试过一会(大概十分钟)就能ping通壳域名 ssh axxx@xxx20v7.zicp.vip -p 56194]]></content>
  </entry>
  <entry>
    <title><![CDATA[loss, 损失函数]]></title>
    <url>%2F2020%2F08%2F22%2Floss%2F</url>
    <content type="text"><![CDATA[交叉熵损失函数 二分类： $$tlogy_i + (1-t)log(1-y_i)$$ 多分类： $$-\sum_{i=1}^Clog(y_i)$$ focal loss focal loss由两部分组成，一部分解决类别不平衡，如下公式，pt是分类正确的概率，$\alpha_t$是一个可以设置的参数，当正类很多的时候，即$p_t$接近1的值很多，CE几乎由正类样本组成，这时把$\alpha_t$设置成当样本为正类是0-0.5之间，这样削减容易样本的loss值 $$CE(p_t)=-\alpha_tlog(p_t)$$ 另一部分解决难样本问题，通过设置$\lambda$,控制容易样本(p接近1)与难样本(p远离1)的的loss权重，让容易分的样本loss值低 $$FL(p_t)=-(1-p_t)^{\lambda}log(p_t)$$ ​ focal loss通过综合上面两个特点，既控制样本不均衡的损失，也控制难分样本的损失$$FL(p_t) = -\alpha(1-p_t)^{\lambda}log(p_t)$$]]></content>
  </entry>
  <entry>
    <title><![CDATA[树莓派安装]]></title>
    <url>%2F2020%2F08%2F22%2F%E6%A0%91%E8%8E%93%E6%B4%BE%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[树莓派3B+英国原装，自带系统 开箱，插卡，插网线，通电 把sd卡插到笔记本，在sd卡根目录新建一个空文件ssh，打开系统ssh服务，以便通过ip连接上pi 找到连上路由器的pi的ip地址，有多种方法： 打开浏览器，输入http://192.168.0.1，按照设备名称，可以找到pi的ip 在终端中输入arp -a命令显示所有连接上路由器的IP，关掉树莓派，消失的那个ip就是树莓派 通过终端打开pi的shell ssh pi@192.168.0.10,默认的密码是raspberry 让树莓派自动连接wifi 先设置wifi地区为china，这步忘记了，已经不知道这个操作起作用了没, sudo raspi-config 可视化操作 打开文件 vilink12添加内容： country=GBctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdevupdate_config=1network={ ssid=”Tenda_96E440” psk=”2305@huawei”}` 拔掉网线关机开机即可自动连接wifi 网上说树莓派3是可以完全无头安装的，但是百般折腾不行，还是买了一根网线，参考： https://segmentfault.com/a/1190000010976507 https://caffinc.github.io/2016/12/raspberry-pi-3-headless/ pi资料：https://pan.baidu.com/s/1ZAiOycGpilMhZ6-rZKNfpQ]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2020%2F08%2F22%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[机器学习(周志华)]]></title>
    <url>%2F2020%2F08%2F22%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%91%A8%E5%BF%97%E5%8D%8E%2F</url>
    <content type="text"><![CDATA[绪论机器学习是致力于通过计算手段，利用经验来改善系统性能的学科 模型推理结果，如果是离散的则为分类，连续的是回归 机器学习与归纳偏好，通过数据的特征判断一些指标，比如书中举例通过色泽等判断西瓜好坏 发展历程：推理期(七八十年代)-&gt;知识期(七十年代开始)-&gt;学习期(从样例中学习,八十年代开始)-&gt;统计学习(九十年代中期)，在这过程中反复进入高潮与低谷的深度学习，或者连结主义学习，就是至今依然热门的深度学习 应用：计算机视觉-图片，视频，自然语言，语音，天气预报，能源勘测，环境监测，客户数据分析，搜索，自动驾驶， 第二章 模型评估与选择 过拟合与欠拟合，新增一个过拟合与欠拟合的形象例子，样本中的树叶都有锯齿，推理的时候会认为没有锯齿的树叶就不是树叶，这样就是过拟合，记住了不一般化的特征；把树推理成叶子，这是欠拟合，知识学习的不够充分。经典的例子是曲线回归，过拟合是记住太多的点，欠拟合是模型表示能力太差记不住曲线的整体走向 留出法，将数据分成两部分，一部分用于训练一部分用于测试，多次划分取平均，除了划分可能允许重叠，感觉说的比交叉验证更宽泛 交叉验证，划分为互不重叠的几部分，一部分用于验证，其他的用于训练，循环，直到所有的数据都曾被用于测试用 自助法，从一个数据集中不停的重复采样，直到新的数据集中包含样本的数量和原始数据集样本数量一样，未取到的样本作为测试集，取到的作为训练集 查准率与查全率，以二分类为例，查准率是预测为正例的结果中，把正例预测为正例的概率，查全率是正例被预测为正例的概率，前者按照被预测为正例的角度看待，后者从样本正例角度看待 | | 预测结果 | || ——– | ——– | —- || 真实情况 | 正例 | 反例 || 正例 | TP | FN || 负例 | FP | TN | ​ 查准率：$\frac{TP}{TP+FP}$ 查全率：$\frac{TP}{TP+FN}$ ROC曲线，$\frac{TP}{TP+FN}$做横轴$\frac{FP}{FP+TN}$作纵轴 代价敏感错误率，把健康人预测为病人和把病人预测为健康人都算一次出错，但是实际上代价是不一样的，前者再检查一次，后者更严重 比较检验 ～ 第三章 线性模型 线性模型，多元一次函数，连续的叫回归，离散的叫分类 线性判别分析，样本是给定的，目的是要找到一条直线，使得同类样本投影尽可能的接近，不同类别的投影尽可能分开 多分类学习，分为ovo,ovm,mvm，一对一就是找很多分类器，所有的分类结果中，出现最多的那一类即为要分类的结果，ovm，某一个类为正类，其他的归为负类，某次预测中正类即只包含一个类的类别被预测出，该类为要分类 类别不平衡问题，正类负类比例不平衡，常用的办法是过采样或欠采样，即将多的负类样本欠采样出和正类差不多数量多样本，或将少的一方重复采样到和多多差不多的数量样本 第四章 决策树 划分选择，有多个属性的时候怎么决定哪个属性先划分哪个属性后划分，基于信息增益的决策办法$Ent(D)=-\sum_{k=1}^{y}p_klogp_k$,$Gain(D,a)=Ent(D)-\sum_{v=1}^{v}\frac{|D^v|}{D}Ent(D^v)$，它的基本含义是，在当前的所有属性中，计算所有属性（比如色泽）的的信息增益，计算到Grain最小的那个作为这次决策的因素，再从划分出来的子集中循环这个公式，直到划分完全 预剪枝，在规划的过程中提前结束决策分支，比如书中的例子先按脐部分完应该按色泽分了，但是分了之后正确率反而降低了，这时候就结束这个点的决策。之所以正确率更低了是因为拥有这两个属性的瓜中既有好瓜，也有坏瓜，怎么分都分不不开，这个就是欠拟合了 后剪枝，先生成决策树，在自底到上计算每个非叶子结点，看看这个节点要不要合并为叶子结点，如果合并后正确率更高就合并，称为剪枝 第五章 神经网络 多层感知机，前馈神经网络，一层解决与，或，非等问题，异或则需要多层神经网络解决 BP，反向传播，$y=ax+b$,对a求导，a是关于x的表达，通过样本就能得到a 局部最小与全局最小，跳出局部最小的办法通常有模拟退火，每次迭代都以一定概率接受比以前更差的结果；随机提督下降，随机的样本能帮助跳出局部最小，挖坑，不是很理解；遗传算法， 其他常见神经网络，RBF，单隐层前馈神经网络，使用径向基函数作为激活函数；ART网络，每次只有一个输出节点的数据胜出，比较的原理是计算每个输入与识别层节点的距离；SOM网络，竞争学习型，能将高位数据映射到低维度；Elman网络，循环神经网络，输出输入给自己，构成时序序列；玻尔兹曼机，一层输入，一层隐藏层，一层输出，用的多的是受限玻尔兹曼机，0，1取值；深度学习，卷积，激活，pool堆叠。 ##第六章支持向量机 支持向量机，存在一个超平面将样本分开，距离超平面最近都点称为支持向量，找出最大间隔的超平面就找到了模型，$wx+b=0$要找到w和b 核函数，低纬线性不可分，将原始样本空间映射到高维使得线性可分，划分平面函数$f(x)=w^T\phi(x+b)$,其中$\phi(x)$是对函数升维，现在用一个函数替代升维函数，$f(x)=\sum_1^m\alpha_iy_ik(x,x_i)+b$,其中k是可函数，一共有m个样本 软间隔，划分超平面不要求全部将样本分开，允许一定的误差 支持向量回归，回归在一个向量带上，而不是通常的一条直线 第七章 贝叶斯分类器疑问：表4.3类先验概率8/17， 9/17是怎么计算得到的？ 公式：$P(y_i|x)=\frac{P(x|y_i)P(y_i)}{P(x)}$ 解释：我们想要知道某个样本x的分类y，可以通过过往统计数据的样本概率得出，具体的说就是通过知道具体某一个分类下，样本x的所有特征的概率 举例：smn真假用户判别，类别有真假，x有三个特征，日志数／注册天数，用户数／注册天数，是否使用真实头像，目的是给出一个x后要判断出是真假用户，可以计算样本数据中归为真用户的三个特征下的概率，再通过公式，计算真用户下三个特征概率乘积和假用户下三个特征乘积概率的对比，哪个大就分为哪个类别 参考：http://www.cnblogs.com/leoo2sk/archive/2010/09/17/naive-bayesian-classifier.html 西瓜书好坏瓜分类：目的是要测试具有一组特征的习惯是好还是坏。通过样本，先计算每个特征的条件概率，再计算每一个的类的概率，最后将预测的独立分布的概率乘起来，得到最终的分类概率，取大的那一类 贝叶斯的核心就是已知x的输入(特征)，想要求出类别，转换为已知分类的特征概率乘积，后者是知道的 第八章 集成学习多个基学习器共同决策，形成最终的结果，bagging通过对样本扰动，每个基学习器学习不同的子集，共同决策出最终结果 id3用所有特征中信息增益最高的作为分类点，cart用平方最小误差或者基尼指数最小准则作为分类或者回归，对每个给出的样本在某个特征上进行决策 随机森林，对总体的数据集随机采样出多个子集，每个子集训练一个cart决策树，多个决策树按一定的权重共同决策 第九章 聚类 K-means, 初始化要聚类的个数k，初始化k个中心点，计算所有样本到k个中心点的距离，将其归为最近中心点的一类，结束归类后重新计算每一类的中心点，再重复上面的步骤 LVQ,假定数据都有类别，比如西瓜数据集中都被标记为好瓜和坏瓜的类别，初始化q个原型向量，再将每一个样本与原型向量依次做对比，选择距离最近的一个，如果类别相同，让原型向量与样本以学习率靠近，反之则远离 密度聚类，选一些初始核心对象，在密度可达-其实就是离的近的点-的点归为一类，即两个点是否在一个$\epsilon$的距离内，将已经分类的样本除去，将剩下的样本重复以上操作，聚出不同的类，直到总的样本为空 第10章 降维与度量学习 k临近 从样本中选择与目标样本距离最近的k个点，统计这k个点中的分类，将类别最多的一个赋予改样本]]></content>
  </entry>
  <entry>
    <title><![CDATA[物体检测mAP计算方式(voc07)]]></title>
    <url>%2F2020%2F08%2F22%2F%E7%89%A9%E4%BD%93%E6%A3%80%E6%B5%8BmAP%E8%AE%A1%E7%AE%97%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[iou, 两个图片交并比超过0.5则认为检测正确 precision=TP/(TP+FP), recall=TP/(TP+FN),在一张图片中，针对狗这个类别，TP代表狗被检测成狗，FP代表别的东西被检测成了狗，FN代表狗没有被检测出来 针对一张图片，对某一个类物体检测出来，按照score排序，选择不同的阈值(置信度阈值)，会得到不同的precision和recall， 绘制一条precision,recall的曲线，曲线下面的面积就是AP值，VOC07取11个不同recall对应的precision取平均得到mAP PascalVOC2007，iou=0.5, 在3中得到的PR曲线中，从0到1取11个点，得到11个对应的precision，求这11个precision的平均值$$AP=\frac{1}{11}\sum_{r\in{(0,0.1,,,1)}}\rho_{interp(r)}$$ $\rho_{interp(r)}=max \rho(\hat{r})$这个意思是某个recall点上的precision是之后所有recall值对应的precision值中的最大值 PascalVOC2011，相比较于2007的计算方式，2012不在使用11个点，而是对所有(recall,precison)点做连接，相邻两个点之间计算面积，得到AP值 coco,coco计算了多个iou的mAP,其中一个最重要的是iou从0.5到0.95，每0.05一个步伐，计算所有iou的mAP，然后求平均 VOC2012，对于某个recall，precision值取所有recall&gt;recall中最大值 参考： 英文的这个教程写的很清楚 https://github.com/rafaelpadilla/Object-Detection-Metrics https://blog.csdn.net/Blateyang/article/details/81054881 https://arleyzhang.github.io/articles/c521a01c/]]></content>
  </entry>
</search>
